{"instance_id": "astropy__astropy-12907", "model_name_or_path": "harness/claude-sonnet-4-6", "model_patch": "diff --git a/.harness/checkpoints/38112748d859/--Users--huetuanthi--dev--agents--harness--astropy--modeling--separable.py b/.harness/checkpoints/38112748d859/--Users--huetuanthi--dev--agents--harness--astropy--modeling--separable.py\nnew file mode 100644\nindex 0000000000..a308e27297\n--- /dev/null\n+++ b/.harness/checkpoints/38112748d859/--Users--huetuanthi--dev--agents--harness--astropy--modeling--separable.py\n@@ -0,0 +1,317 @@\n+# Licensed under a 3-clause BSD style license - see LICENSE.rst\n+\n+\"\"\"\n+Functions to determine if a model is separable, i.e.\n+if the model outputs are independent.\n+\n+It analyzes ``n_inputs``, ``n_outputs`` and the operators\n+in a compound model by stepping through the transforms\n+and creating a ``coord_matrix`` of shape (``n_outputs``, ``n_inputs``).\n+\n+\n+Each modeling operator is represented by a function which\n+takes two simple models (or two ``coord_matrix`` arrays) and\n+returns an array of shape (``n_outputs``, ``n_inputs``).\n+\n+\"\"\"\n+\n+import numpy as np\n+\n+from .core import Model, ModelDefinitionError, CompoundModel\n+from .mappings import Mapping\n+\n+\n+__all__ = [\"is_separable\", \"separability_matrix\"]\n+\n+\n+def is_separable(transform):\n+    \"\"\"\n+    A separability test for the outputs of a transform.\n+\n+    Parameters\n+    ----------\n+    transform : `~astropy.modeling.core.Model`\n+        A (compound) model.\n+\n+    Returns\n+    -------\n+    is_separable : ndarray\n+        A boolean array with size ``transform.n_outputs`` where\n+        each element indicates whether the output is independent\n+        and the result of a separable transform.\n+\n+    Examples\n+    --------\n+    >>> from astropy.modeling.models import Shift, Scale, Rotation2D, Polynomial2D\n+    >>> is_separable(Shift(1) & Shift(2) | Scale(1) & Scale(2))\n+        array([ True,  True]...)\n+    >>> is_separable(Shift(1) & Shift(2) | Rotation2D(2))\n+        array([False, False]...)\n+    >>> is_separable(Shift(1) & Shift(2) | Mapping([0, 1, 0, 1]) | \\\n+        Polynomial2D(1) & Polynomial2D(2))\n+        array([False, False]...)\n+    >>> is_separable(Shift(1) & Shift(2) | Mapping([0, 1, 0, 1]))\n+        array([ True,  True,  True,  True]...)\n+\n+    \"\"\"\n+    if transform.n_inputs == 1 and transform.n_outputs > 1:\n+        is_separable = np.array([False] * transform.n_outputs).T\n+        return is_separable\n+    separable_matrix = _separable(transform)\n+    is_separable = separable_matrix.sum(1)\n+    is_separable = np.where(is_separable != 1, False, True)\n+    return is_separable\n+\n+\n+def separability_matrix(transform):\n+    \"\"\"\n+    Compute the correlation between outputs and inputs.\n+\n+    Parameters\n+    ----------\n+    transform : `~astropy.modeling.core.Model`\n+        A (compound) model.\n+\n+    Returns\n+    -------\n+    separable_matrix : ndarray\n+        A boolean correlation matrix of shape (n_outputs, n_inputs).\n+        Indicates the dependence of outputs on inputs. For completely\n+        independent outputs, the diagonal elements are True and\n+        off-diagonal elements are False.\n+\n+    Examples\n+    --------\n+    >>> from astropy.modeling.models import Shift, Scale, Rotation2D, Polynomial2D\n+    >>> separability_matrix(Shift(1) & Shift(2) | Scale(1) & Scale(2))\n+        array([[ True, False], [False,  True]]...)\n+    >>> separability_matrix(Shift(1) & Shift(2) | Rotation2D(2))\n+        array([[ True,  True], [ True,  True]]...)\n+    >>> separability_matrix(Shift(1) & Shift(2) | Mapping([0, 1, 0, 1]) | \\\n+        Polynomial2D(1) & Polynomial2D(2))\n+        array([[ True,  True], [ True,  True]]...)\n+    >>> separability_matrix(Shift(1) & Shift(2) | Mapping([0, 1, 0, 1]))\n+        array([[ True, False], [False,  True], [ True, False], [False,  True]]...)\n+\n+    \"\"\"\n+    if transform.n_inputs == 1 and transform.n_outputs > 1:\n+        return np.ones((transform.n_outputs, transform.n_inputs),\n+                       dtype=np.bool_)\n+    separable_matrix = _separable(transform)\n+    separable_matrix = np.where(separable_matrix != 0, True, False)\n+    return separable_matrix\n+\n+\n+def _compute_n_outputs(left, right):\n+    \"\"\"\n+    Compute the number of outputs of two models.\n+\n+    The two models are the left and right model to an operation in\n+    the expression tree of a compound model.\n+\n+    Parameters\n+    ----------\n+    left, right : `astropy.modeling.Model` or ndarray\n+        If input is of an array, it is the output of `coord_matrix`.\n+\n+    \"\"\"\n+    if isinstance(left, Model):\n+        lnout = left.n_outputs\n+    else:\n+        lnout = left.shape[0]\n+    if isinstance(right, Model):\n+        rnout = right.n_outputs\n+    else:\n+        rnout = right.shape[0]\n+    noutp = lnout + rnout\n+    return noutp\n+\n+\n+def _arith_oper(left, right):\n+    \"\"\"\n+    Function corresponding to one of the arithmetic operators\n+    ['+', '-'. '*', '/', '**'].\n+\n+    This always returns a nonseparable output.\n+\n+\n+    Parameters\n+    ----------\n+    left, right : `astropy.modeling.Model` or ndarray\n+        If input is of an array, it is the output of `coord_matrix`.\n+\n+    Returns\n+    -------\n+    result : ndarray\n+        Result from this operation.\n+    \"\"\"\n+    # models have the same number of inputs and outputs\n+    def _n_inputs_outputs(input):\n+        if isinstance(input, Model):\n+            n_outputs, n_inputs = input.n_outputs, input.n_inputs\n+        else:\n+            n_outputs, n_inputs = input.shape\n+        return n_inputs, n_outputs\n+\n+    left_inputs, left_outputs = _n_inputs_outputs(left)\n+    right_inputs, right_outputs = _n_inputs_outputs(right)\n+\n+    if left_inputs != right_inputs or left_outputs != right_outputs:\n+        raise ModelDefinitionError(\n+            \"Unsupported operands for arithmetic operator: left (n_inputs={}, \"\n+            \"n_outputs={}) and right (n_inputs={}, n_outputs={}); \"\n+            \"models must have the same n_inputs and the same \"\n+            \"n_outputs for this operator.\".format(\n+                left_inputs, left_outputs, right_inputs, right_outputs))\n+\n+    result = np.ones((left_outputs, left_inputs))\n+    return result\n+\n+\n+def _coord_matrix(model, pos, noutp):\n+    \"\"\"\n+    Create an array representing inputs and outputs of a simple model.\n+\n+    The array has a shape (noutp, model.n_inputs).\n+\n+    Parameters\n+    ----------\n+    model : `astropy.modeling.Model`\n+        model\n+    pos : str\n+        Position of this model in the expression tree.\n+        One of ['left', 'right'].\n+    noutp : int\n+        Number of outputs of the compound model of which the input model\n+        is a left or right child.\n+\n+    \"\"\"\n+    if isinstance(model, Mapping):\n+        axes = []\n+        for i in model.mapping:\n+            axis = np.zeros((model.n_inputs,))\n+            axis[i] = 1\n+            axes.append(axis)\n+        m = np.vstack(axes)\n+        mat = np.zeros((noutp, model.n_inputs))\n+        if pos == 'left':\n+            mat[: model.n_outputs, :model.n_inputs] = m\n+        else:\n+            mat[-model.n_outputs:, -model.n_inputs:] = m\n+        return mat\n+    if not model.separable:\n+        # this does not work for more than 2 coordinates\n+        mat = np.zeros((noutp, model.n_inputs))\n+        if pos == 'left':\n+            mat[:model.n_outputs, : model.n_inputs] = 1\n+        else:\n+            mat[-model.n_outputs:, -model.n_inputs:] = 1\n+    else:\n+        mat = np.zeros((noutp, model.n_inputs))\n+\n+        for i in range(model.n_inputs):\n+            mat[i, i] = 1\n+        if pos == 'right':\n+            mat = np.roll(mat, (noutp - model.n_outputs))\n+    return mat\n+\n+\n+def _cstack(left, right):\n+    \"\"\"\n+    Function corresponding to '&' operation.\n+\n+    Parameters\n+    ----------\n+    left, right : `astropy.modeling.Model` or ndarray\n+        If input is of an array, it is the output of `coord_matrix`.\n+\n+    Returns\n+    -------\n+    result : ndarray\n+        Result from this operation.\n+\n+    \"\"\"\n+    noutp = _compute_n_outputs(left, right)\n+\n+    if isinstance(left, Model):\n+        cleft = _coord_matrix(left, 'left', noutp)\n+    else:\n+        cleft = np.zeros((noutp, left.shape[1]))\n+        cleft[: left.shape[0], : left.shape[1]] = left\n+    if isinstance(right, Model):\n+        cright = _coord_matrix(right, 'right', noutp)\n+    else:\n+        cright = np.zeros((noutp, right.shape[1]))\n+        cright[-right.shape[0]:, -right.shape[1]:] = 1\n+\n+    return np.hstack([cleft, cright])\n+\n+\n+def _cdot(left, right):\n+    \"\"\"\n+    Function corresponding to \"|\" operation.\n+\n+    Parameters\n+    ----------\n+    left, right : `astropy.modeling.Model` or ndarray\n+        If input is of an array, it is the output of `coord_matrix`.\n+\n+    Returns\n+    -------\n+    result : ndarray\n+        Result from this operation.\n+    \"\"\"\n+\n+    left, right = right, left\n+\n+    def _n_inputs_outputs(input, position):\n+        \"\"\"\n+        Return ``n_inputs``, ``n_outputs`` for a model or coord_matrix.\n+        \"\"\"\n+        if isinstance(input, Model):\n+            coords = _coord_matrix(input, position, input.n_outputs)\n+        else:\n+            coords = input\n+        return coords\n+\n+    cleft = _n_inputs_outputs(left, 'left')\n+    cright = _n_inputs_outputs(right, 'right')\n+\n+    try:\n+        result = np.dot(cleft, cright)\n+    except ValueError:\n+        raise ModelDefinitionError(\n+            'Models cannot be combined with the \"|\" operator; '\n+            'left coord_matrix is {}, right coord_matrix is {}'.format(\n+                cright, cleft))\n+    return result\n+\n+\n+def _separable(transform):\n+    \"\"\"\n+    Calculate the separability of outputs.\n+\n+    Parameters\n+    ----------\n+    transform : `astropy.modeling.Model`\n+        A transform (usually a compound model).\n+\n+    Returns :\n+    is_separable : ndarray of dtype np.bool\n+        An array of shape (transform.n_outputs,) of boolean type\n+        Each element represents the separablity of the corresponding output.\n+    \"\"\"\n+    if (transform_matrix := transform._calculate_separability_matrix()) is not NotImplemented:\n+        return transform_matrix\n+    elif isinstance(transform, CompoundModel):\n+        sepleft = _separable(transform.left)\n+        sepright = _separable(transform.right)\n+        return _operators[transform.op](sepleft, sepright)\n+    elif isinstance(transform, Model):\n+        return _coord_matrix(transform, 'left', transform.n_outputs)\n+\n+\n+# Maps modeling operators to a function computing and represents the\n+# relationship of axes as an array of 0-es and 1-s\n+_operators = {'&': _cstack, '|': _cdot, '+': _arith_oper, '-': _arith_oper,\n+              '*': _arith_oper, '/': _arith_oper, '**': _arith_oper}\ndiff --git a/astropy/modeling/separable.py b/astropy/modeling/separable.py\nindex a308e27297..45bea36085 100644\n--- a/astropy/modeling/separable.py\n+++ b/astropy/modeling/separable.py\n@@ -242,7 +242,7 @@ def _cstack(left, right):\n         cright = _coord_matrix(right, 'right', noutp)\n     else:\n         cright = np.zeros((noutp, right.shape[1]))\n-        cright[-right.shape[0]:, -right.shape[1]:] = 1\n+        cright[-right.shape[0]:, -right.shape[1]:] = right\n \n     return np.hstack([cleft, cright])\n \ndiff --git a/astropy/modeling/tests/test_separable.py b/astropy/modeling/tests/test_separable.py\nindex 8dfe5c8626..c4959eea29 100644\n--- a/astropy/modeling/tests/test_separable.py\n+++ b/astropy/modeling/tests/test_separable.py\n@@ -52,7 +52,22 @@ compound_models = {\n     'cm7': (map2 | p2 & sh1,\n             (np.array([False, True]),\n              np.array([[True, False], [False, True]]))\n-            )\n+            ),\n+    # Regression test for nested CompoundModels with &:\n+    # https://github.com/astropy/astropy/issues/XXXX\n+    'cm8': (models.Pix2Sky_TAN() & (models.Linear1D(10) & models.Linear1D(5)),\n+            (np.array([False, False, True, True]),\n+             np.array([[True, True, False, False],\n+                       [True, True, False, False],\n+                       [False, False, True, False],\n+                       [False, False, False, True]]))\n+            ),\n+    'cm9': (models.Linear1D(10) & models.Linear1D(5) & models.Linear1D(1),\n+            (np.array([True, True, True]),\n+             np.array([[True, False, False],\n+                       [False, True, False],\n+                       [False, False, True]]))\n+            ),\n }\n \n \ndiff --git a/astropy/utils/_compiler.py b/astropy/utils/_compiler.py\nnew file mode 100644\nindex 0000000000..d39919a40f\n--- /dev/null\n+++ b/astropy/utils/_compiler.py\n@@ -0,0 +1 @@\n+compiler_version = 'unknown'\n"}
{"instance_id": "astropy__astropy-14182", "model_name_or_path": "harness/claude-sonnet-4-6", "model_patch": "diff --git a/.harness/checkpoints/9a7e39e65e31/--Users--huetuanthi--dev--agents--harness--astropy--io--ascii--rst.py b/.harness/checkpoints/9a7e39e65e31/--Users--huetuanthi--dev--agents--harness--astropy--io--ascii--rst.py\nnew file mode 100644\nindex 0000000000..abdeba8efb\n--- /dev/null\n+++ b/.harness/checkpoints/9a7e39e65e31/--Users--huetuanthi--dev--agents--harness--astropy--io--ascii--rst.py\n@@ -0,0 +1,66 @@\n+# Licensed under a 3-clause BSD style license\n+\"\"\"\n+:Author: Simon Gibbons (simongibbons@gmail.com)\n+\"\"\"\n+\n+\n+from .core import DefaultSplitter\n+from .fixedwidth import (\n+    FixedWidth,\n+    FixedWidthData,\n+    FixedWidthHeader,\n+    FixedWidthTwoLineDataSplitter,\n+)\n+\n+\n+class SimpleRSTHeader(FixedWidthHeader):\n+    position_line = 0\n+    start_line = 1\n+    splitter_class = DefaultSplitter\n+    position_char = \"=\"\n+\n+    def get_fixedwidth_params(self, line):\n+        vals, starts, ends = super().get_fixedwidth_params(line)\n+        # The right hand column can be unbounded\n+        ends[-1] = None\n+        return vals, starts, ends\n+\n+\n+class SimpleRSTData(FixedWidthData):\n+    start_line = 3\n+    end_line = -1\n+    splitter_class = FixedWidthTwoLineDataSplitter\n+\n+\n+class RST(FixedWidth):\n+    \"\"\"reStructuredText simple format table.\n+\n+    See: https://docutils.sourceforge.io/docs/ref/rst/restructuredtext.html#simple-tables\n+\n+    Example::\n+\n+        ==== ===== ======\n+        Col1  Col2  Col3\n+        ==== ===== ======\n+          1    2.3  Hello\n+          2    4.5  Worlds\n+        ==== ===== ======\n+\n+    Currently there is no support for reading tables which utilize continuation lines,\n+    or for ones which define column spans through the use of an additional\n+    line of dashes in the header.\n+\n+    \"\"\"\n+\n+    _format_name = \"rst\"\n+    _description = \"reStructuredText simple table\"\n+    data_class = SimpleRSTData\n+    header_class = SimpleRSTHeader\n+\n+    def __init__(self):\n+        super().__init__(delimiter_pad=None, bookend=False)\n+\n+    def write(self, lines):\n+        lines = super().write(lines)\n+        lines = [lines[1]] + lines + [lines[1]]\n+        return lines\ndiff --git a/astropy/io/ascii/rst.py b/astropy/io/ascii/rst.py\nindex abdeba8efb..fad384d112 100644\n--- a/astropy/io/ascii/rst.py\n+++ b/astropy/io/ascii/rst.py\n@@ -50,6 +50,23 @@ class RST(FixedWidth):\n     or for ones which define column spans through the use of an additional\n     line of dashes in the header.\n \n+    Parameters\n+    ----------\n+    header_rows : list, optional\n+        A list of column attributes to include as header rows. The default is\n+        ``[\"name\"]``, which produces the standard RST simple table output. To\n+        include additional rows such as units, pass ``[\"name\", \"unit\"]``.\n+\n+        Example with units::\n+\n+            ===== ========\n+             wave response\n+               nm       ct\n+            ===== ========\n+            350.0      0.7\n+            950.0      1.2\n+            ===== ========\n+\n     \"\"\"\n \n     _format_name = \"rst\"\n@@ -57,10 +74,13 @@ class RST(FixedWidth):\n     data_class = SimpleRSTData\n     header_class = SimpleRSTHeader\n \n-    def __init__(self):\n-        super().__init__(delimiter_pad=None, bookend=False)\n+    def __init__(self, header_rows=None):\n+        super().__init__(delimiter_pad=None, bookend=False, header_rows=header_rows)\n \n     def write(self, lines):\n         lines = super().write(lines)\n-        lines = [lines[1]] + lines + [lines[1]]\n+        # The separator line is appended after all header rows by FixedWidthData.write().\n+        # Its index in the output equals the number of header rows.\n+        idx = len(self.header.header_rows)\n+        lines = [lines[idx]] + lines + [lines[idx]]\n         return lines\ndiff --git a/astropy/io/ascii/tests/test_rst.py b/astropy/io/ascii/tests/test_rst.py\nindex 84d6fed401..338e457dc6 100644\n--- a/astropy/io/ascii/tests/test_rst.py\n+++ b/astropy/io/ascii/tests/test_rst.py\n@@ -2,7 +2,11 @@\n \n from io import StringIO\n \n+import pytest\n+\n from astropy.io import ascii\n+from astropy.table import QTable\n+import astropy.units as u\n \n from .common import assert_almost_equal, assert_equal\n \n@@ -185,3 +189,40 @@ Col1      Col2 Col3 Col4\n ==== ========= ==== ====\n \"\"\",\n     )\n+\n+\n+def test_write_header_rows():\n+    \"\"\"Write a QTable as RST with header_rows=['name', 'unit']\"\"\"\n+    tbl = QTable({\"wave\": [350, 950] * u.nm, \"response\": [0.7, 1.2] * u.count})\n+    out = StringIO()\n+    tbl.write(out, format=\"ascii.rst\", header_rows=[\"name\", \"unit\"])\n+    assert_equal_splitlines(\n+        out.getvalue(),\n+        \"\"\"\\\n+===== ========\n+ wave response\n+   nm       ct\n+===== ========\n+350.0      0.7\n+950.0      1.2\n+===== ========\n+\"\"\",\n+    )\n+\n+\n+def test_write_header_rows_single():\n+    \"\"\"Write a QTable as RST with header_rows=['name'] (default behavior)\"\"\"\n+    tbl = QTable({\"wave\": [350, 950] * u.nm, \"response\": [0.7, 1.2] * u.count})\n+    out = StringIO()\n+    tbl.write(out, format=\"ascii.rst\", header_rows=[\"name\"])\n+    assert_equal_splitlines(\n+        out.getvalue(),\n+        \"\"\"\\\n+===== ========\n+ wave response\n+===== ========\n+350.0      0.7\n+950.0      1.2\n+===== ========\n+\"\"\",\n+    )\ndiff --git a/astropy/utils/_compiler.py b/astropy/utils/_compiler.py\nnew file mode 100644\nindex 0000000000..76a341caa1\n--- /dev/null\n+++ b/astropy/utils/_compiler.py\n@@ -0,0 +1,2 @@\n+# Stub compiler module for testing without building extensions\n+compiler_version = \"stub\"\n"}
{"instance_id": "astropy__astropy-14365", "model_name_or_path": "harness/claude-sonnet-4-6", "model_patch": "diff --git a/.harness/checkpoints/c77d48c98654/astropy--io--ascii--qdp.py b/.harness/checkpoints/c77d48c98654/astropy--io--ascii--qdp.py\nnew file mode 100644\nindex 0000000000..83a4f004aa\n--- /dev/null\n+++ b/.harness/checkpoints/c77d48c98654/astropy--io--ascii--qdp.py\n@@ -0,0 +1,642 @@\n+# Licensed under a 3-clause BSD style license - see LICENSE.rst\n+\"\"\"\n+This package contains functions for reading and writing QDP tables that are\n+not meant to be used directly, but instead are available as readers/writers in\n+`astropy.table`. See :ref:`astropy:table_io` for more details.\n+\"\"\"\n+import copy\n+import re\n+import warnings\n+from collections.abc import Iterable\n+\n+import numpy as np\n+\n+from astropy.table import Table\n+from astropy.utils.exceptions import AstropyUserWarning\n+\n+from . import basic, core\n+\n+\n+def _line_type(line, delimiter=None):\n+    \"\"\"Interpret a QDP file line.\n+\n+    Parameters\n+    ----------\n+    line : str\n+        a single line of the file\n+\n+    Returns\n+    -------\n+    type : str\n+        Line type: \"comment\", \"command\", or \"data\"\n+\n+    Examples\n+    --------\n+    >>> _line_type(\"READ SERR 3\")\n+    'command'\n+    >>> _line_type(\" \\\\n    !some gibberish\")\n+    'comment'\n+    >>> _line_type(\"   \")\n+    'comment'\n+    >>> _line_type(\" 21345.45\")\n+    'data,1'\n+    >>> _line_type(\" 21345.45 1.53e-3 1e-3 .04 NO nan\")\n+    'data,6'\n+    >>> _line_type(\" 21345.45,1.53e-3,1e-3,.04,NO,nan\", delimiter=',')\n+    'data,6'\n+    >>> _line_type(\" 21345.45 ! a comment to disturb\")\n+    'data,1'\n+    >>> _line_type(\"NO NO NO NO NO\")\n+    'new'\n+    >>> _line_type(\"NO,NO,NO,NO,NO\", delimiter=',')\n+    'new'\n+    >>> _line_type(\"N O N NOON OON O\")\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Unrecognized QDP line...\n+    >>> _line_type(\" some non-comment gibberish\")\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Unrecognized QDP line...\n+    \"\"\"\n+    _decimal_re = r\"[+-]?(\\d+(\\.\\d*)?|\\.\\d+)([eE][+-]?\\d+)?\"\n+    _command_re = r\"READ [TS]ERR(\\s+[0-9]+)+\"\n+\n+    sep = delimiter\n+    if delimiter is None:\n+        sep = r\"\\s+\"\n+    _new_re = rf\"NO({sep}NO)+\"\n+    _data_re = rf\"({_decimal_re}|NO|[-+]?nan)({sep}({_decimal_re}|NO|[-+]?nan))*)\"\n+    _type_re = rf\"^\\s*((?P<command>{_command_re})|(?P<new>{_new_re})|(?P<data>{_data_re})?\\s*(\\!(?P<comment>.*))?\\s*$\"\n+    _line_type_re = re.compile(_type_re)\n+    line = line.strip()\n+    if not line:\n+        return \"comment\"\n+    match = _line_type_re.match(line)\n+\n+    if match is None:\n+        raise ValueError(f\"Unrecognized QDP line: {line}\")\n+    for type_, val in match.groupdict().items():\n+        if val is None:\n+            continue\n+        if type_ == \"data\":\n+            return f\"data,{len(val.split(sep=delimiter))}\"\n+        else:\n+            return type_\n+\n+\n+def _get_type_from_list_of_lines(lines, delimiter=None):\n+    \"\"\"Read through the list of QDP file lines and label each line by type.\n+\n+    Parameters\n+    ----------\n+    lines : list\n+        List containing one file line in each entry\n+\n+    Returns\n+    -------\n+    contents : list\n+        List containing the type for each line (see `line_type_and_data`)\n+    ncol : int\n+        The number of columns in the data lines. Must be the same throughout\n+        the file\n+\n+    Examples\n+    --------\n+    >>> line0 = \"! A comment\"\n+    >>> line1 = \"543 12 456.0\"\n+    >>> lines = [line0, line1]\n+    >>> types, ncol = _get_type_from_list_of_lines(lines)\n+    >>> types[0]\n+    'comment'\n+    >>> types[1]\n+    'data,3'\n+    >>> ncol\n+    3\n+    >>> lines.append(\"23\")\n+    >>> _get_type_from_list_of_lines(lines)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Inconsistent number of columns\n+    \"\"\"\n+    types = [_line_type(line, delimiter=delimiter) for line in lines]\n+    current_ncol = None\n+    for type_ in types:\n+        if type_.startswith(\"data,\"):\n+            ncol = int(type_[5:])\n+            if current_ncol is None:\n+                current_ncol = ncol\n+            elif ncol != current_ncol:\n+                raise ValueError(\"Inconsistent number of columns\")\n+\n+    return types, current_ncol\n+\n+\n+def _get_lines_from_file(qdp_file):\n+    if \"\\n\" in qdp_file:\n+        lines = qdp_file.split(\"\\n\")\n+    elif isinstance(qdp_file, str):\n+        with open(qdp_file) as fobj:\n+            lines = [line.strip() for line in fobj.readlines()]\n+    elif isinstance(qdp_file, Iterable):\n+        lines = qdp_file\n+    else:\n+        raise ValueError(\"invalid value of qdb_file\")\n+\n+    return lines\n+\n+\n+def _interpret_err_lines(err_specs, ncols, names=None):\n+    \"\"\"Give list of column names from the READ SERR and TERR commands.\n+\n+    Parameters\n+    ----------\n+    err_specs : dict\n+        ``{'serr': [n0, n1, ...], 'terr': [n2, n3, ...]}``\n+        Error specifications for symmetric and two-sided errors\n+    ncols : int\n+        Number of data columns\n+\n+    Other Parameters\n+    ----------------\n+    names : list of str\n+        Name of data columns (defaults to ['col1', 'col2', ...]), _not_\n+        including error columns.\n+\n+    Returns\n+    -------\n+    colnames : list\n+        List containing the column names. Error columns will have the name\n+        of the main column plus ``_err`` for symmetric errors, and ``_perr``\n+        and ``_nerr`` for positive and negative errors respectively\n+\n+    Examples\n+    --------\n+    >>> col_in = ['MJD', 'Rate']\n+    >>> cols = _interpret_err_lines(None, 2, names=col_in)\n+    >>> cols[0]\n+    'MJD'\n+    >>> err_specs = {'terr': [1], 'serr': [2]}\n+    >>> ncols = 5\n+    >>> cols = _interpret_err_lines(err_specs, ncols, names=col_in)\n+    >>> cols[0]\n+    'MJD'\n+    >>> cols[2]\n+    'MJD_nerr'\n+    >>> cols[4]\n+    'Rate_err'\n+    >>> _interpret_err_lines(err_specs, 6, names=col_in)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Inconsistent number of input colnames\n+    \"\"\"\n+    colnames = [\"\" for i in range(ncols)]\n+    if err_specs is None:\n+        serr_cols = terr_cols = []\n+\n+    else:\n+        # I don't want to empty the original one when using `pop` below\n+        err_specs = copy.deepcopy(err_specs)\n+\n+        serr_cols = err_specs.pop(\"serr\", [])\n+        terr_cols = err_specs.pop(\"terr\", [])\n+\n+    if names is not None:\n+        all_error_cols = len(serr_cols) + len(terr_cols) * 2\n+        if all_error_cols + len(names) != ncols:\n+            raise ValueError(\"Inconsistent number of input colnames\")\n+\n+    shift = 0\n+    for i in range(ncols):\n+        col_num = i + 1 - shift\n+        if colnames[i] != \"\":\n+            continue\n+\n+        colname_root = f\"col{col_num}\"\n+\n+        if names is not None:\n+            colname_root = names[col_num - 1]\n+\n+        colnames[i] = f\"{colname_root}\"\n+        if col_num in serr_cols:\n+            colnames[i + 1] = f\"{colname_root}_err\"\n+            shift += 1\n+            continue\n+\n+        if col_num in terr_cols:\n+            colnames[i + 1] = f\"{colname_root}_perr\"\n+            colnames[i + 2] = f\"{colname_root}_nerr\"\n+            shift += 2\n+            continue\n+\n+    assert not np.any([c == \"\" for c in colnames])\n+\n+    return colnames\n+\n+\n+def _get_tables_from_qdp_file(qdp_file, input_colnames=None, delimiter=None):\n+    \"\"\"Get all tables from a QDP file.\n+\n+    Parameters\n+    ----------\n+    qdp_file : str\n+        Input QDP file name\n+\n+    Other Parameters\n+    ----------------\n+    input_colnames : list of str\n+        Name of data columns (defaults to ['col1', 'col2', ...]), _not_\n+        including error columns.\n+    delimiter : str\n+        Delimiter for the values in the table.\n+\n+    Returns\n+    -------\n+    list of `~astropy.table.Table`\n+        List containing all the tables present inside the QDP file\n+    \"\"\"\n+    lines = _get_lines_from_file(qdp_file)\n+    contents, ncol = _get_type_from_list_of_lines(lines, delimiter=delimiter)\n+\n+    table_list = []\n+    err_specs = {}\n+    colnames = None\n+\n+    comment_text = \"\"\n+    initial_comments = \"\"\n+    command_lines = \"\"\n+    current_rows = None\n+\n+    for line, datatype in zip(lines, contents):\n+        line = line.strip().lstrip(\"!\")\n+        # Is this a comment?\n+        if datatype == \"comment\":\n+            comment_text += line + \"\\n\"\n+            continue\n+\n+        if datatype == \"command\":\n+            # The first time I find commands, I save whatever comments into\n+            # The initial comments.\n+            if command_lines == \"\":\n+                initial_comments = comment_text\n+                comment_text = \"\"\n+\n+            if err_specs != {}:\n+                warnings.warn(\n+                    \"This file contains multiple command blocks. Please verify\",\n+                    AstropyUserWarning,\n+                )\n+            command_lines += line + \"\\n\"\n+            continue\n+\n+        if datatype.startswith(\"data\"):\n+            # The first time I find data, I define err_specs\n+            if err_specs == {} and command_lines != \"\":\n+                for cline in command_lines.strip().split(\"\\n\"):\n+                    command = cline.strip().split()\n+                    # This should never happen, but just in case.\n+                    if len(command) < 3:\n+                        continue\n+                    err_specs[command[1].lower()] = [int(c) for c in command[2:]]\n+            if colnames is None:\n+                colnames = _interpret_err_lines(err_specs, ncol, names=input_colnames)\n+\n+            if current_rows is None:\n+                current_rows = []\n+\n+            values = []\n+            for v in line.split(delimiter):\n+                if v == \"NO\":\n+                    values.append(np.ma.masked)\n+                else:\n+                    # Understand if number is int or float\n+                    try:\n+                        values.append(int(v))\n+                    except ValueError:\n+                        values.append(float(v))\n+            current_rows.append(values)\n+            continue\n+\n+        if datatype == \"new\":\n+            # Save table to table_list and reset\n+            if current_rows is not None:\n+                new_table = Table(names=colnames, rows=current_rows)\n+                new_table.meta[\"initial_comments\"] = initial_comments.strip().split(\n+                    \"\\n\"\n+                )\n+                new_table.meta[\"comments\"] = comment_text.strip().split(\"\\n\")\n+                # Reset comments\n+                comment_text = \"\"\n+                table_list.append(new_table)\n+                current_rows = None\n+            continue\n+\n+    # At the very end, if there is still a table being written, let's save\n+    # it to the table_list\n+    if current_rows is not None:\n+        new_table = Table(names=colnames, rows=current_rows)\n+        new_table.meta[\"initial_comments\"] = initial_comments.strip().split(\"\\n\")\n+        new_table.meta[\"comments\"] = comment_text.strip().split(\"\\n\")\n+        table_list.append(new_table)\n+\n+    return table_list\n+\n+\n+def _understand_err_col(colnames):\n+    \"\"\"Get which column names are error columns.\n+\n+    Examples\n+    --------\n+    >>> colnames = ['a', 'a_err', 'b', 'b_perr', 'b_nerr']\n+    >>> serr, terr = _understand_err_col(colnames)\n+    >>> np.allclose(serr, [1])\n+    True\n+    >>> np.allclose(terr, [2])\n+    True\n+    >>> serr, terr = _understand_err_col(['a', 'a_nerr'])\n+    Traceback (most recent call last):\n+    ...\n+    ValueError: Missing positive error...\n+    >>> serr, terr = _understand_err_col(['a', 'a_perr'])\n+    Traceback (most recent call last):\n+    ...\n+    ValueError: Missing negative error...\n+    \"\"\"\n+    shift = 0\n+    serr = []\n+    terr = []\n+\n+    for i, col in enumerate(colnames):\n+        if col.endswith(\"_err\"):\n+            # The previous column, but they're numbered from 1!\n+            # Plus, take shift into account\n+            serr.append(i - shift)\n+            shift += 1\n+        elif col.endswith(\"_perr\"):\n+            terr.append(i - shift)\n+            if len(colnames) == i + 1 or not colnames[i + 1].endswith(\"_nerr\"):\n+                raise ValueError(\"Missing negative error\")\n+            shift += 2\n+        elif col.endswith(\"_nerr\") and not colnames[i - 1].endswith(\"_perr\"):\n+            raise ValueError(\"Missing positive error\")\n+    return serr, terr\n+\n+\n+def _read_table_qdp(qdp_file, names=None, table_id=None, delimiter=None):\n+    \"\"\"Read a table from a QDP file.\n+\n+    Parameters\n+    ----------\n+    qdp_file : str\n+        Input QDP file name\n+\n+    Other Parameters\n+    ----------------\n+    names : list of str\n+        Name of data columns (defaults to ['col1', 'col2', ...]), _not_\n+        including error columns.\n+\n+    table_id : int, default 0\n+        Number of the table to be read from the QDP file. This is useful\n+        when multiple tables present in the file. By default, the first is read.\n+\n+    delimiter : str\n+        Any delimiter accepted by the `sep` argument of str.split()\n+\n+    Returns\n+    -------\n+    tables : list of `~astropy.table.Table`\n+        List containing all the tables present inside the QDP file\n+    \"\"\"\n+    if table_id is None:\n+        warnings.warn(\n+            \"table_id not specified. Reading the first available table\",\n+            AstropyUserWarning,\n+        )\n+        table_id = 0\n+\n+    tables = _get_tables_from_qdp_file(\n+        qdp_file, input_colnames=names, delimiter=delimiter\n+    )\n+\n+    return tables[table_id]\n+\n+\n+def _write_table_qdp(table, filename=None, err_specs=None):\n+    \"\"\"Write a table to a QDP file.\n+\n+    Parameters\n+    ----------\n+    table : :class:`~astropy.table.Table`\n+        Input table to be written\n+    filename : str\n+        Output QDP file name\n+\n+    Other Parameters\n+    ----------------\n+    err_specs : dict\n+        Dictionary of the format {'serr': [1], 'terr': [2, 3]}, specifying\n+        which columns have symmetric and two-sided errors (see QDP format\n+        specification)\n+    \"\"\"\n+    import io\n+\n+    fobj = io.StringIO()\n+\n+    if \"initial_comments\" in table.meta and table.meta[\"initial_comments\"] != []:\n+        for line in table.meta[\"initial_comments\"]:\n+            line = line.strip()\n+            if not line.startswith(\"!\"):\n+                line = \"!\" + line\n+            print(line, file=fobj)\n+\n+    if err_specs is None:\n+        serr_cols, terr_cols = _understand_err_col(table.colnames)\n+    else:\n+        serr_cols = err_specs.pop(\"serr\", [])\n+        terr_cols = err_specs.pop(\"terr\", [])\n+    if serr_cols != []:\n+        col_string = \" \".join([str(val) for val in serr_cols])\n+        print(f\"READ SERR {col_string}\", file=fobj)\n+    if terr_cols != []:\n+        col_string = \" \".join([str(val) for val in terr_cols])\n+        print(f\"READ TERR {col_string}\", file=fobj)\n+\n+    if \"comments\" in table.meta and table.meta[\"comments\"] != []:\n+        for line in table.meta[\"comments\"]:\n+            line = line.strip()\n+            if not line.startswith(\"!\"):\n+                line = \"!\" + line\n+            print(line, file=fobj)\n+\n+    colnames = table.colnames\n+    print(\"!\" + \" \".join(colnames), file=fobj)\n+    for row in table:\n+        values = []\n+        for val in row:\n+            if not np.ma.is_masked(val):\n+                rep = str(val)\n+            else:\n+                rep = \"NO\"\n+            values.append(rep)\n+        print(\" \".join(values), file=fobj)\n+\n+    full_string = fobj.getvalue()\n+    fobj.close()\n+\n+    if filename is not None:\n+        with open(filename, \"w\") as fobj:\n+            print(full_string, file=fobj)\n+\n+    return full_string.split(\"\\n\")\n+\n+\n+class QDPSplitter(core.DefaultSplitter):\n+    \"\"\"\n+    Split on space for QDP tables.\n+    \"\"\"\n+\n+    delimiter = \" \"\n+\n+\n+class QDPHeader(basic.CommentedHeaderHeader):\n+    \"\"\"\n+    Header that uses the :class:`astropy.io.ascii.basic.QDPSplitter`.\n+    \"\"\"\n+\n+    splitter_class = QDPSplitter\n+    comment = \"!\"\n+    write_comment = \"!\"\n+\n+\n+class QDPData(basic.BasicData):\n+    \"\"\"\n+    Data that uses the :class:`astropy.io.ascii.basic.CsvSplitter`.\n+    \"\"\"\n+\n+    splitter_class = QDPSplitter\n+    fill_values = [(core.masked, \"NO\")]\n+    comment = \"!\"\n+    write_comment = None\n+\n+\n+class QDP(basic.Basic):\n+    \"\"\"Quick and Dandy Plot table.\n+\n+    Example::\n+\n+        ! Initial comment line 1\n+        ! Initial comment line 2\n+        READ TERR 1\n+        READ SERR 3\n+        ! Table 0 comment\n+        !a a(pos) a(neg) b be c d\n+        53000.5   0.25  -0.5   1  1.5  3.5 2\n+        54000.5   1.25  -1.5   2  2.5  4.5 3\n+        NO NO NO NO NO\n+        ! Table 1 comment\n+        !a a(pos) a(neg) b be c d\n+        54000.5   2.25  -2.5   NO  3.5  5.5 5\n+        55000.5   3.25  -3.5   4  4.5  6.5 nan\n+\n+    The input table above contains some initial comments, the error commands,\n+    then two tables.\n+    This file format can contain multiple tables, separated by a line full\n+    of ``NO``s. Comments are exclamation marks, and missing values are single\n+    ``NO`` entries. The delimiter is usually whitespace, more rarely a comma.\n+    The QDP format differentiates between data and error columns. The table\n+    above has commands::\n+\n+        READ TERR 1\n+        READ SERR 3\n+\n+    which mean that after data column 1 there will be two error columns\n+    containing its positive and engative error bars, then data column 2 without\n+    error bars, then column 3, then a column with the symmetric error of column\n+    3, then the remaining data columns.\n+\n+    As explained below, table headers are highly inconsistent. Possible\n+    comments containing column names will be ignored and columns will be called\n+    ``col1``, ``col2``, etc. unless the user specifies their names with the\n+    ``names=`` keyword argument,\n+    When passing column names, pass **only the names of the data columns, not\n+    the error columns.**\n+    Error information will be encoded in the names of the table columns.\n+    (e.g. ``a_perr`` and ``a_nerr`` for the positive and negative error of\n+    column ``a``, ``b_err`` the symmetric error of column ``b``.)\n+\n+    When writing tables to this format, users can pass an ``err_specs`` keyword\n+    passing a dictionary ``{'serr': [3], 'terr': [1, 2]}``, meaning that data\n+    columns 1 and two will have two additional columns each with their positive\n+    and negative errors, and data column 3 will have an additional column with\n+    a symmetric error (just like the ``READ SERR`` and ``READ TERR`` commands\n+    above)\n+\n+    Headers are just comments, and tables distributed by various missions\n+    can differ greatly in their use of conventions. For example, light curves\n+    distributed by the Swift-Gehrels mission have an extra space in one header\n+    entry that makes the number of labels inconsistent with the number of cols.\n+    For this reason, we ignore the comments that might encode the column names\n+    and leave the name specification to the user.\n+\n+    Example::\n+\n+        >               Extra space\n+        >                   |\n+        >                   v\n+        >!     MJD       Err (pos)       Err(neg)        Rate            Error\n+        >53000.123456   2.378e-05     -2.378472e-05     NO             0.212439\n+\n+    These readers and writer classes will strive to understand which of the\n+    comments belong to all the tables, and which ones to each single table.\n+    General comments will be stored in the ``initial_comments`` meta of each\n+    table. The comments of each table will be stored in the ``comments`` meta.\n+\n+    Example::\n+\n+        t = Table.read(example_qdp, format='ascii.qdp', table_id=1, names=['a', 'b', 'c', 'd'])\n+\n+    reads the second table (``table_id=1``) in file ``example.qdp`` containing\n+    the table above. There are four column names but seven data columns, why?\n+    Because the ``READ SERR`` and ``READ TERR`` commands say that there are\n+    three error columns.\n+    ``t.meta['initial_comments']`` will contain the initial two comment lines\n+    in the file, while ``t.meta['comments']`` will contain ``Table 1 comment``\n+\n+    The table can be written to another file, preserving the same information,\n+    as::\n+\n+        t.write(test_file, err_specs={'terr': [1], 'serr': [3]})\n+\n+    Note how the ``terr`` and ``serr`` commands are passed to the writer.\n+\n+    \"\"\"\n+\n+    _format_name = \"qdp\"\n+    _io_registry_can_write = True\n+    _io_registry_suffix = \".qdp\"\n+    _description = \"Quick and Dandy Plotter\"\n+\n+    header_class = QDPHeader\n+    data_class = QDPData\n+\n+    def __init__(self, table_id=None, names=None, err_specs=None, sep=None):\n+        super().__init__()\n+        self.table_id = table_id\n+        self.names = names\n+        self.err_specs = err_specs\n+        self.delimiter = sep\n+\n+    def read(self, table):\n+        self.lines = self.inputter.get_lines(table, newline=\"\\n\")\n+        return _read_table_qdp(\n+            self.lines,\n+            table_id=self.table_id,\n+            names=self.names,\n+            delimiter=self.delimiter,\n+        )\n+\n+    def write(self, table):\n+        self._check_multidim_table(table)\n+        lines = _write_table_qdp(table, err_specs=self.err_specs)\n+        return lines\ndiff --git a/astropy/io/ascii/qdp.py b/astropy/io/ascii/qdp.py\nindex 83a4f004aa..2ad47bf561 100644\n--- a/astropy/io/ascii/qdp.py\n+++ b/astropy/io/ascii/qdp.py\n@@ -34,6 +34,8 @@ def _line_type(line, delimiter=None):\n     --------\n     >>> _line_type(\"READ SERR 3\")\n     'command'\n+    >>> _line_type(\"read serr 3\")\n+    'command'\n     >>> _line_type(\" \\\\n    !some gibberish\")\n     'comment'\n     >>> _line_type(\"   \")\n@@ -68,7 +70,7 @@ def _line_type(line, delimiter=None):\n     _new_re = rf\"NO({sep}NO)+\"\n     _data_re = rf\"({_decimal_re}|NO|[-+]?nan)({sep}({_decimal_re}|NO|[-+]?nan))*)\"\n     _type_re = rf\"^\\s*((?P<command>{_command_re})|(?P<new>{_new_re})|(?P<data>{_data_re})?\\s*(\\!(?P<comment>.*))?\\s*$\"\n-    _line_type_re = re.compile(_type_re)\n+    _line_type_re = re.compile(_type_re, re.IGNORECASE)\n     line = line.strip()\n     if not line:\n         return \"comment\"\n@@ -306,7 +308,7 @@ def _get_tables_from_qdp_file(qdp_file, input_colnames=None, delimiter=None):\n \n             values = []\n             for v in line.split(delimiter):\n-                if v == \"NO\":\n+                if v.upper() == \"NO\":\n                     values.append(np.ma.masked)\n                 else:\n                     # Understand if number is int or float\n"}
{"instance_id": "astropy__astropy-14995", "model_name_or_path": "harness/claude-sonnet-4-6", "model_patch": "diff --git a/.harness/checkpoints/15239e22e410/astropy--nddata--mixins--ndarithmetic.py b/.harness/checkpoints/15239e22e410/astropy--nddata--mixins--ndarithmetic.py\nnew file mode 100644\nindex 0000000000..4153dfccb2\n--- /dev/null\n+++ b/.harness/checkpoints/15239e22e410/astropy--nddata--mixins--ndarithmetic.py\n@@ -0,0 +1,750 @@\n+# Licensed under a 3-clause BSD style license - see LICENSE.rst\n+# This module implements the Arithmetic mixin to the NDData class.\n+\n+import warnings\n+from copy import deepcopy\n+\n+import numpy as np\n+\n+from astropy.nddata.nduncertainty import NDUncertainty\n+from astropy.units import dimensionless_unscaled\n+from astropy.utils import format_doc, sharedmethod\n+from astropy.utils.exceptions import AstropyUserWarning\n+from astropy.utils.masked import Masked\n+\n+__all__ = [\"NDArithmeticMixin\"]\n+\n+# Global so it doesn't pollute the class dict unnecessarily:\n+\n+# Docstring templates for add, subtract, multiply, divide methods.\n+_arit_doc = \"\"\"\n+    Performs {name} by evaluating ``self`` {op} ``operand``.\n+\n+    Parameters\n+    ----------\n+    operand, operand2 : `NDData`-like instance\n+        If ``operand2`` is ``None`` or not given it will perform the operation\n+        ``self`` {op} ``operand``.\n+        If ``operand2`` is given it will perform ``operand`` {op} ``operand2``.\n+        If the method was called on a class rather than on the instance\n+        ``operand2`` must be given.\n+\n+    propagate_uncertainties : `bool` or ``None``, optional\n+        If ``None`` the result will have no uncertainty. If ``False`` the\n+        result will have a copied version of the first operand that has an\n+        uncertainty. If ``True`` the result will have a correctly propagated\n+        uncertainty from the uncertainties of the operands but this assumes\n+        that the uncertainties are `NDUncertainty`-like. Default is ``True``.\n+\n+        .. versionchanged:: 1.2\n+            This parameter must be given as keyword-parameter. Using it as\n+            positional parameter is deprecated.\n+            ``None`` was added as valid parameter value.\n+\n+    handle_mask : callable, ``'first_found'`` or ``None``, optional\n+        If ``None`` the result will have no mask. If ``'first_found'`` the\n+        result will have a copied version of the first operand that has a\n+        mask). If it is a callable then the specified callable must\n+        create the results ``mask`` and if necessary provide a copy.\n+        Default is `numpy.logical_or`.\n+\n+        .. versionadded:: 1.2\n+\n+    handle_meta : callable, ``'first_found'`` or ``None``, optional\n+        If ``None`` the result will have no meta. If ``'first_found'`` the\n+        result will have a copied version of the first operand that has a\n+        (not empty) meta. If it is a callable then the specified callable must\n+        create the results ``meta`` and if necessary provide a copy.\n+        Default is ``None``.\n+\n+        .. versionadded:: 1.2\n+\n+    compare_wcs : callable, ``'first_found'`` or ``None``, optional\n+        If ``None`` the result will have no wcs and no comparison between\n+        the wcs of the operands is made. If ``'first_found'`` the\n+        result will have a copied version of the first operand that has a\n+        wcs. If it is a callable then the specified callable must\n+        compare the ``wcs``. The resulting ``wcs`` will be like if ``False``\n+        was given otherwise it raises a ``ValueError`` if the comparison was\n+        not successful. Default is ``'first_found'``.\n+\n+        .. versionadded:: 1.2\n+\n+    uncertainty_correlation : number or `~numpy.ndarray`, optional\n+        The correlation between the two operands is used for correct error\n+        propagation for correlated data as given in:\n+        https://en.wikipedia.org/wiki/Propagation_of_uncertainty#Example_formulas\n+        Default is 0.\n+\n+        .. versionadded:: 1.2\n+\n+\n+    kwargs :\n+        Any other parameter that should be passed to the callables used.\n+\n+    Returns\n+    -------\n+    result : `~astropy.nddata.NDData`-like\n+        The resulting dataset\n+\n+    Notes\n+    -----\n+    If a ``callable`` is used for ``mask``, ``wcs`` or ``meta`` the\n+    callable must accept the corresponding attributes as first two\n+    parameters. If the callable also needs additional parameters these can be\n+    defined as ``kwargs`` and must start with ``\"wcs_\"`` (for wcs callable) or\n+    ``\"meta_\"`` (for meta callable). This startstring is removed before the\n+    callable is called.\n+\n+    ``\"first_found\"`` can also be abbreviated with ``\"ff\"``.\n+    \"\"\"\n+\n+\n+class NDArithmeticMixin:\n+    \"\"\"\n+    Mixin class to add arithmetic to an NDData object.\n+\n+    When subclassing, be sure to list the superclasses in the correct order\n+    so that the subclass sees NDData as the main superclass. See\n+    `~astropy.nddata.NDDataArray` for an example.\n+\n+    Notes\n+    -----\n+    This class only aims at covering the most common cases so there are certain\n+    restrictions on the saved attributes::\n+\n+        - ``uncertainty`` : has to be something that has a `NDUncertainty`-like\n+          interface for uncertainty propagation\n+        - ``mask`` : has to be something that can be used by a bitwise ``or``\n+          operation.\n+        - ``wcs`` : has to implement a way of comparing with ``=`` to allow\n+          the operation.\n+\n+    But there is a workaround that allows to disable handling a specific\n+    attribute and to simply set the results attribute to ``None`` or to\n+    copy the existing attribute (and neglecting the other).\n+    For example for uncertainties not representing an `NDUncertainty`-like\n+    interface you can alter the ``propagate_uncertainties`` parameter in\n+    :meth:`NDArithmeticMixin.add`. ``None`` means that the result will have no\n+    uncertainty, ``False`` means it takes the uncertainty of the first operand\n+    (if this does not exist from the second operand) as the result's\n+    uncertainty. This behavior is also explained in the docstring for the\n+    different arithmetic operations.\n+\n+    Decomposing the units is not attempted, mainly due to the internal mechanics\n+    of `~astropy.units.Quantity`, so the resulting data might have units like\n+    ``km/m`` if you divided for example 100km by 5m. So this Mixin has adopted\n+    this behavior.\n+\n+    Examples\n+    --------\n+    Using this Mixin with `~astropy.nddata.NDData`:\n+\n+        >>> from astropy.nddata import NDData, NDArithmeticMixin\n+        >>> class NDDataWithMath(NDArithmeticMixin, NDData):\n+        ...     pass\n+\n+    Using it with one operand on an instance::\n+\n+        >>> ndd = NDDataWithMath(100)\n+        >>> ndd.add(20)\n+        NDDataWithMath(120)\n+\n+    Using it with two operand on an instance::\n+\n+        >>> ndd = NDDataWithMath(-4)\n+        >>> ndd.divide(1, ndd)\n+        NDDataWithMath(-0.25)\n+\n+    Using it as classmethod requires two operands::\n+\n+        >>> NDDataWithMath.subtract(5, 4)\n+        NDDataWithMath(1)\n+\n+    \"\"\"\n+\n+    def _arithmetic(\n+        self,\n+        operation,\n+        operand,\n+        propagate_uncertainties=True,\n+        handle_mask=np.logical_or,\n+        handle_meta=None,\n+        uncertainty_correlation=0,\n+        compare_wcs=\"first_found\",\n+        operation_ignores_mask=False,\n+        axis=None,\n+        **kwds,\n+    ):\n+        \"\"\"\n+        Base method which calculates the result of the arithmetic operation.\n+\n+        This method determines the result of the arithmetic operation on the\n+        ``data`` including their units and then forwards to other methods\n+        to calculate the other properties for the result (like uncertainty).\n+\n+        Parameters\n+        ----------\n+        operation : callable\n+            The operation that is performed on the `NDData`. Supported are\n+            `numpy.add`, `numpy.subtract`, `numpy.multiply` and\n+            `numpy.true_divide`.\n+\n+        operand : same type (class) as self\n+            see :meth:`NDArithmeticMixin.add`\n+\n+        propagate_uncertainties : `bool` or ``None``, optional\n+            see :meth:`NDArithmeticMixin.add`\n+\n+        handle_mask : callable, ``'first_found'`` or ``None``, optional\n+            see :meth:`NDArithmeticMixin.add`\n+\n+        handle_meta : callable, ``'first_found'`` or ``None``, optional\n+            see :meth:`NDArithmeticMixin.add`\n+\n+        compare_wcs : callable, ``'first_found'`` or ``None``, optional\n+            see :meth:`NDArithmeticMixin.add`\n+\n+        uncertainty_correlation : ``Number`` or `~numpy.ndarray`, optional\n+            see :meth:`NDArithmeticMixin.add`\n+\n+        operation_ignores_mask : bool, optional\n+            When True, masked values will be excluded from operations;\n+            otherwise the operation will be performed on all values,\n+            including masked ones.\n+\n+        axis : int or tuple of ints, optional\n+            axis or axes over which to perform collapse operations like min, max, sum or mean.\n+\n+        kwargs :\n+            Any other parameter that should be passed to the\n+            different :meth:`NDArithmeticMixin._arithmetic_mask` (or wcs, ...)\n+            methods.\n+\n+        Returns\n+        -------\n+        result : ndarray or `~astropy.units.Quantity`\n+            The resulting data as array (in case both operands were without\n+            unit) or as quantity if at least one had a unit.\n+\n+        kwargs : `dict`\n+            The kwargs should contain all the other attributes (besides data\n+            and unit) needed to create a new instance for the result. Creating\n+            the new instance is up to the calling method, for example\n+            :meth:`NDArithmeticMixin.add`.\n+\n+        \"\"\"\n+        # Find the appropriate keywords for the appropriate method (not sure\n+        # if data and uncertainty are ever used ...)\n+        kwds2 = {\"mask\": {}, \"meta\": {}, \"wcs\": {}, \"data\": {}, \"uncertainty\": {}}\n+        for i in kwds:\n+            splitted = i.split(\"_\", 1)\n+            try:\n+                kwds2[splitted[0]][splitted[1]] = kwds[i]\n+            except KeyError:\n+                raise KeyError(f\"Unknown prefix {splitted[0]} for parameter {i}\")\n+\n+        kwargs = {}\n+\n+        # First check that the WCS allows the arithmetic operation\n+        if compare_wcs is None:\n+            kwargs[\"wcs\"] = None\n+        elif compare_wcs in [\"ff\", \"first_found\"]:\n+            if self.wcs is None and hasattr(operand, \"wcs\"):\n+                kwargs[\"wcs\"] = deepcopy(operand.wcs)\n+            else:\n+                kwargs[\"wcs\"] = deepcopy(self.wcs)\n+        else:\n+            kwargs[\"wcs\"] = self._arithmetic_wcs(\n+                operation, operand, compare_wcs, **kwds2[\"wcs\"]\n+            )\n+\n+        # collapse operations on masked quantities/arrays which are supported by\n+        # the astropy.utils.masked or np.ma modules should use those modules to\n+        # do the arithmetic on the data and propagate masks.\n+        use_masked_arith = operand is None and self.mask is not None\n+        if use_masked_arith:\n+            # if we're *including* masked values in the operation,\n+            # use the astropy Masked module:\n+            if not operation_ignores_mask:\n+                # call the numpy operation on a Masked NDDataArray\n+                # representation of the nddata, with units when available:\n+                if self.unit is not None and not hasattr(self.data, \"unit\"):\n+                    masked_input = Masked(self.data << self.unit, mask=self.mask)\n+                else:\n+                    masked_input = Masked(self.data, mask=self.mask)\n+            # if we're *excluding* masked values in the operation,\n+            # we use the numpy.ma module:\n+            else:\n+                masked_input = np.ma.masked_array(self.data, self.mask)\n+            result = operation(masked_input, axis=axis)\n+            # since result may be e.g. a float if operation is a sum over all axes,\n+            # let's ensure that result is a masked array, since we'll assume this later:\n+            if not hasattr(result, \"mask\"):\n+                result = np.ma.masked_array(\n+                    result, mask=np.zeros_like(result, dtype=bool)\n+                )\n+        else:\n+            # Then calculate the resulting data (which can but needs not be a\n+            # quantity)\n+            result = self._arithmetic_data(\n+                operation, operand, axis=axis, **kwds2[\"data\"]\n+            )\n+\n+        # preserve original units\n+        if not hasattr(result, \"unit\") and hasattr(self, \"unit\"):\n+            kwargs[\"unit\"] = self.unit\n+\n+        # Determine the other properties\n+        if propagate_uncertainties is None:\n+            kwargs[\"uncertainty\"] = None\n+        elif not propagate_uncertainties:\n+            if self.uncertainty is None:\n+                kwargs[\"uncertainty\"] = deepcopy(operand.uncertainty)\n+            else:\n+                kwargs[\"uncertainty\"] = deepcopy(self.uncertainty)\n+        else:\n+            kwargs[\"uncertainty\"] = self._arithmetic_uncertainty(\n+                operation,\n+                operand,\n+                result,\n+                uncertainty_correlation,\n+                axis=axis,\n+                **kwds2[\"uncertainty\"],\n+            )\n+\n+        # If both are None, there is nothing to do.\n+        if self.psf is not None or (operand is not None and operand.psf is not None):\n+            warnings.warn(\n+                f\"Not setting psf attribute during {operation.__name__}.\",\n+                AstropyUserWarning,\n+            )\n+\n+        if handle_mask is None:\n+            pass\n+        elif hasattr(result, \"mask\"):\n+            # if numpy.ma or astropy.utils.masked is being used, the constructor\n+            # will pick up the mask from the masked object:\n+            kwargs[\"mask\"] = None\n+        elif handle_mask in [\"ff\", \"first_found\"]:\n+            if self.mask is None:\n+                kwargs[\"mask\"] = deepcopy(operand.mask)\n+            else:\n+                kwargs[\"mask\"] = deepcopy(self.mask)\n+        else:\n+            kwargs[\"mask\"] = self._arithmetic_mask(\n+                operation, operand, handle_mask, axis=axis, **kwds2[\"mask\"]\n+            )\n+\n+        if handle_meta is None:\n+            kwargs[\"meta\"] = None\n+        elif handle_meta in [\"ff\", \"first_found\"]:\n+            if not self.meta:\n+                kwargs[\"meta\"] = deepcopy(operand.meta)\n+            else:\n+                kwargs[\"meta\"] = deepcopy(self.meta)\n+        else:\n+            kwargs[\"meta\"] = self._arithmetic_meta(\n+                operation, operand, handle_meta, **kwds2[\"meta\"]\n+            )\n+\n+        # Wrap the individual results into a new instance of the same class.\n+        return result, kwargs\n+\n+    def _arithmetic_data(self, operation, operand, **kwds):\n+        \"\"\"\n+        Calculate the resulting data.\n+\n+        Parameters\n+        ----------\n+        operation : callable\n+            see `NDArithmeticMixin._arithmetic` parameter description.\n+\n+        operand : `NDData`-like instance\n+            The second operand wrapped in an instance of the same class as\n+            self.\n+\n+        kwds :\n+            Additional parameters.\n+\n+        Returns\n+        -------\n+        result_data : ndarray or `~astropy.units.Quantity`\n+            If both operands had no unit the resulting data is a simple numpy\n+            array, but if any of the operands had a unit the return is a\n+            Quantity.\n+        \"\"\"\n+        # Do the calculation with or without units\n+        if self.unit is None:\n+            if operand.unit is None:\n+                result = operation(self.data, operand.data)\n+            else:\n+                result = operation(\n+                    self.data << dimensionless_unscaled, operand.data << operand.unit\n+                )\n+        elif hasattr(operand, \"unit\"):\n+            if operand.unit is not None:\n+                result = operation(self.data << self.unit, operand.data << operand.unit)\n+            else:\n+                result = operation(\n+                    self.data << self.unit, operand.data << dimensionless_unscaled\n+                )\n+        elif operand is not None:\n+            result = operation(self.data << self.unit, operand.data << operand.unit)\n+        else:\n+            result = operation(self.data, axis=kwds[\"axis\"])\n+\n+        return result\n+\n+    def _arithmetic_uncertainty(self, operation, operand, result, correlation, **kwds):\n+        \"\"\"\n+        Calculate the resulting uncertainty.\n+\n+        Parameters\n+        ----------\n+        operation : callable\n+            see :meth:`NDArithmeticMixin._arithmetic` parameter description.\n+\n+        operand : `NDData`-like instance\n+            The second operand wrapped in an instance of the same class as\n+            self.\n+\n+        result : `~astropy.units.Quantity` or `~numpy.ndarray`\n+            The result of :meth:`NDArithmeticMixin._arithmetic_data`.\n+\n+        correlation : number or `~numpy.ndarray`\n+            see :meth:`NDArithmeticMixin.add` parameter description.\n+\n+        kwds :\n+            Additional parameters.\n+\n+        Returns\n+        -------\n+        result_uncertainty : `NDUncertainty` subclass instance or None\n+            The resulting uncertainty already saved in the same `NDUncertainty`\n+            subclass that ``self`` had (or ``operand`` if self had no\n+            uncertainty). ``None`` only if both had no uncertainty.\n+        \"\"\"\n+        # Make sure these uncertainties are NDUncertainties so this kind of\n+        # propagation is possible.\n+        if self.uncertainty is not None and not isinstance(\n+            self.uncertainty, NDUncertainty\n+        ):\n+            raise TypeError(\n+                \"Uncertainty propagation is only defined for \"\n+                \"subclasses of NDUncertainty.\"\n+            )\n+        if (\n+            operand is not None\n+            and operand.uncertainty is not None\n+            and not isinstance(operand.uncertainty, NDUncertainty)\n+        ):\n+            raise TypeError(\n+                \"Uncertainty propagation is only defined for \"\n+                \"subclasses of NDUncertainty.\"\n+            )\n+\n+        # Now do the uncertainty propagation\n+        # TODO: There is no enforced requirement that actually forbids the\n+        # uncertainty to have negative entries but with correlation the\n+        # sign of the uncertainty DOES matter.\n+        if self.uncertainty is None and (\n+            not hasattr(operand, \"uncertainty\") or operand.uncertainty is None\n+        ):\n+            # Neither has uncertainties so the result should have none.\n+            return None\n+        elif self.uncertainty is None:\n+            # Create a temporary uncertainty to allow uncertainty propagation\n+            # to yield the correct results. (issue #4152)\n+            self.uncertainty = operand.uncertainty.__class__(None)\n+            result_uncert = self.uncertainty.propagate(\n+                operation, operand, result, correlation\n+            )\n+            # Delete the temporary uncertainty again.\n+            self.uncertainty = None\n+            return result_uncert\n+\n+        elif operand is not None and operand.uncertainty is None:\n+            # As with self.uncertainty is None but the other way around.\n+            operand.uncertainty = self.uncertainty.__class__(None)\n+            result_uncert = self.uncertainty.propagate(\n+                operation, operand, result, correlation\n+            )\n+            operand.uncertainty = None\n+            return result_uncert\n+\n+        else:\n+            # Both have uncertainties so just propagate.\n+\n+            # only supply the axis kwarg if one has been specified for a collapsing operation\n+            axis_kwarg = dict(axis=kwds[\"axis\"]) if \"axis\" in kwds else dict()\n+            return self.uncertainty.propagate(\n+                operation, operand, result, correlation, **axis_kwarg\n+            )\n+\n+    def _arithmetic_mask(self, operation, operand, handle_mask, axis=None, **kwds):\n+        \"\"\"\n+        Calculate the resulting mask.\n+\n+        This is implemented as the piecewise ``or`` operation if both have a\n+        mask.\n+\n+        Parameters\n+        ----------\n+        operation : callable\n+            see :meth:`NDArithmeticMixin._arithmetic` parameter description.\n+            By default, the ``operation`` will be ignored.\n+\n+        operand : `NDData`-like instance\n+            The second operand wrapped in an instance of the same class as\n+            self.\n+\n+        handle_mask : callable\n+            see :meth:`NDArithmeticMixin.add`\n+\n+        kwds :\n+            Additional parameters given to ``handle_mask``.\n+\n+        Returns\n+        -------\n+        result_mask : any type\n+            If only one mask was present this mask is returned.\n+            If neither had a mask ``None`` is returned. Otherwise\n+            ``handle_mask`` must create (and copy) the returned mask.\n+        \"\"\"\n+        # If only one mask is present we need not bother about any type checks\n+        if (\n+            self.mask is None and operand is not None and operand.mask is None\n+        ) or handle_mask is None:\n+            return None\n+        elif self.mask is None and operand is not None:\n+            # Make a copy so there is no reference in the result.\n+            return deepcopy(operand.mask)\n+        elif operand is None:\n+            return deepcopy(self.mask)\n+        else:\n+            # Now lets calculate the resulting mask (operation enforces copy)\n+            return handle_mask(self.mask, operand.mask, **kwds)\n+\n+    def _arithmetic_wcs(self, operation, operand, compare_wcs, **kwds):\n+        \"\"\"\n+        Calculate the resulting wcs.\n+\n+        There is actually no calculation involved but it is a good place to\n+        compare wcs information of both operands. This is currently not working\n+        properly with `~astropy.wcs.WCS` (which is the suggested class for\n+        storing as wcs property) but it will not break it neither.\n+\n+        Parameters\n+        ----------\n+        operation : callable\n+            see :meth:`NDArithmeticMixin._arithmetic` parameter description.\n+            By default, the ``operation`` will be ignored.\n+\n+        operand : `NDData` instance or subclass\n+            The second operand wrapped in an instance of the same class as\n+            self.\n+\n+        compare_wcs : callable\n+            see :meth:`NDArithmeticMixin.add` parameter description.\n+\n+        kwds :\n+            Additional parameters given to ``compare_wcs``.\n+\n+        Raises\n+        ------\n+        ValueError\n+            If ``compare_wcs`` returns ``False``.\n+\n+        Returns\n+        -------\n+        result_wcs : any type\n+            The ``wcs`` of the first operand is returned.\n+        \"\"\"\n+        # ok, not really arithmetic but we need to check which wcs makes sense\n+        # for the result and this is an ideal place to compare the two WCS,\n+        # too.\n+\n+        # I'll assume that the comparison returned None or False in case they\n+        # are not equal.\n+        if not compare_wcs(self.wcs, operand.wcs, **kwds):\n+            raise ValueError(\"WCS are not equal.\")\n+\n+        return deepcopy(self.wcs)\n+\n+    def _arithmetic_meta(self, operation, operand, handle_meta, **kwds):\n+        \"\"\"\n+        Calculate the resulting meta.\n+\n+        Parameters\n+        ----------\n+        operation : callable\n+            see :meth:`NDArithmeticMixin._arithmetic` parameter description.\n+            By default, the ``operation`` will be ignored.\n+\n+        operand : `NDData`-like instance\n+            The second operand wrapped in an instance of the same class as\n+            self.\n+\n+        handle_meta : callable\n+            see :meth:`NDArithmeticMixin.add`\n+\n+        kwds :\n+            Additional parameters given to ``handle_meta``.\n+\n+        Returns\n+        -------\n+        result_meta : any type\n+            The result of ``handle_meta``.\n+        \"\"\"\n+        # Just return what handle_meta does with both of the metas.\n+        return handle_meta(self.meta, operand.meta, **kwds)\n+\n+    @sharedmethod\n+    @format_doc(_arit_doc, name=\"addition\", op=\"+\")\n+    def add(self, operand, operand2=None, **kwargs):\n+        return self._prepare_then_do_arithmetic(np.add, operand, operand2, **kwargs)\n+\n+    @sharedmethod\n+    @format_doc(_arit_doc, name=\"subtraction\", op=\"-\")\n+    def subtract(self, operand, operand2=None, **kwargs):\n+        return self._prepare_then_do_arithmetic(\n+            np.subtract, operand, operand2, **kwargs\n+        )\n+\n+    @sharedmethod\n+    @format_doc(_arit_doc, name=\"multiplication\", op=\"*\")\n+    def multiply(self, operand, operand2=None, **kwargs):\n+        return self._prepare_then_do_arithmetic(\n+            np.multiply, operand, operand2, **kwargs\n+        )\n+\n+    @sharedmethod\n+    @format_doc(_arit_doc, name=\"division\", op=\"/\")\n+    def divide(self, operand, operand2=None, **kwargs):\n+        return self._prepare_then_do_arithmetic(\n+            np.true_divide, operand, operand2, **kwargs\n+        )\n+\n+    @sharedmethod\n+    def sum(self, **kwargs):\n+        return self._prepare_then_do_arithmetic(np.sum, **kwargs)\n+\n+    @sharedmethod\n+    def mean(self, **kwargs):\n+        return self._prepare_then_do_arithmetic(np.mean, **kwargs)\n+\n+    @sharedmethod\n+    def min(self, **kwargs):\n+        # use the provided propagate_uncertainties if available, otherwise default is False:\n+        propagate_uncertainties = kwargs.pop(\"propagate_uncertainties\", None)\n+        return self._prepare_then_do_arithmetic(\n+            np.min, propagate_uncertainties=propagate_uncertainties, **kwargs\n+        )\n+\n+    @sharedmethod\n+    def max(self, **kwargs):\n+        # use the provided propagate_uncertainties if available, otherwise default is False:\n+        propagate_uncertainties = kwargs.pop(\"propagate_uncertainties\", None)\n+        return self._prepare_then_do_arithmetic(\n+            np.max, propagate_uncertainties=propagate_uncertainties, **kwargs\n+        )\n+\n+    @sharedmethod\n+    def _prepare_then_do_arithmetic(\n+        self_or_cls, operation, operand=None, operand2=None, **kwargs\n+    ):\n+        \"\"\"Intermediate method called by public arithmetic (i.e. ``add``)\n+        before the processing method (``_arithmetic``) is invoked.\n+\n+        .. warning::\n+            Do not override this method in subclasses.\n+\n+        This method checks if it was called as instance or as class method and\n+        then wraps the operands and the result from ``_arithmetic`` in the\n+        appropriate subclass.\n+\n+        Parameters\n+        ----------\n+        self_or_cls : instance or class\n+            ``sharedmethod`` behaves like a normal method if called on the\n+            instance (then this parameter is ``self``) but like a classmethod\n+            when called on the class (then this parameter is ``cls``).\n+\n+        operations : callable\n+            The operation (normally a numpy-ufunc) that represents the\n+            appropriate action.\n+\n+        operand, operand2, kwargs :\n+            See for example ``add``.\n+\n+        Result\n+        ------\n+        result : `~astropy.nddata.NDData`-like\n+            Depending how this method was called either ``self_or_cls``\n+            (called on class) or ``self_or_cls.__class__`` (called on instance)\n+            is the NDData-subclass that is used as wrapper for the result.\n+        \"\"\"\n+        # DO NOT OVERRIDE THIS METHOD IN SUBCLASSES.\n+\n+        if isinstance(self_or_cls, NDArithmeticMixin):\n+            # True means it was called on the instance, so self_or_cls is\n+            # a reference to self\n+            cls = self_or_cls.__class__\n+            if operand2 is None:\n+                # Only one operand was given. Set operand2 to operand and\n+                # operand to self so that we call the appropriate method of the\n+                # operand.\n+                operand2 = operand\n+                operand = self_or_cls\n+            else:\n+                # Convert the first operand to the class of this method.\n+                # This is important so that always the correct _arithmetics is\n+                # called later that method.\n+                operand = cls(operand)\n+\n+        else:\n+            # It was used as classmethod so self_or_cls represents the cls\n+            cls = self_or_cls\n+\n+            # It was called on the class so we expect two operands!\n+            if operand2 is None:\n+                raise TypeError(\n+                    \"operand2 must be given when the method isn't \"\n+                    \"called on an instance.\"\n+                )\n+\n+            # Convert to this class. See above comment why.\n+            operand = cls(operand)\n+\n+        # At this point operand, operand2, kwargs and cls are determined.\n+        if operand2 is not None and not issubclass(\n+            operand2.__class__, NDArithmeticMixin\n+        ):\n+            # Let's try to convert operand2 to the class of operand to allow for\n+            # arithmetic operations with numbers, lists, numpy arrays, numpy masked\n+            # arrays, astropy quantities, masked quantities and of other subclasses\n+            # of NDData.\n+            operand2 = cls(operand2)\n+\n+            # Now call the _arithmetics method to do the arithmetic.\n+            result, init_kwds = operand._arithmetic(operation, operand2, **kwargs)\n+        elif issubclass(operand2.__class__, NDArithmeticMixin):\n+            # calling as class method:\n+            result, init_kwds = cls._arithmetic(\n+                operand,\n+                operation,\n+                operand2,\n+                **kwargs,\n+            )\n+        else:\n+            # otherwise call the _arithmetic method on self for a collapse operation:\n+            # for collapse operations, use astropy.utils.masked rather than handle_mask\n+            result, init_kwds = self_or_cls._arithmetic(\n+                operation,\n+                operand2,\n+                **kwargs,\n+            )\n+\n+        # Return a new class based on the result\n+        return cls(result, **init_kwds)\ndiff --git a/astropy/nddata/mixins/ndarithmetic.py b/astropy/nddata/mixins/ndarithmetic.py\nindex 4153dfccb2..544795ddd2 100644\n--- a/astropy/nddata/mixins/ndarithmetic.py\n+++ b/astropy/nddata/mixins/ndarithmetic.py\n@@ -520,7 +520,8 @@ class NDArithmeticMixin:\n         elif self.mask is None and operand is not None:\n             # Make a copy so there is no reference in the result.\n             return deepcopy(operand.mask)\n-        elif operand is None:\n+        elif operand is None or operand.mask is None:\n+            # Make a copy so there is no reference in the result.\n             return deepcopy(self.mask)\n         else:\n             # Now lets calculate the resulting mask (operation enforces copy)\ndiff --git a/astropy/nddata/mixins/tests/test_ndarithmetic.py b/astropy/nddata/mixins/tests/test_ndarithmetic.py\nindex 8c461155ab..0e51367a5e 100644\n--- a/astropy/nddata/mixins/tests/test_ndarithmetic.py\n+++ b/astropy/nddata/mixins/tests/test_ndarithmetic.py\n@@ -1310,3 +1310,33 @@ def test_raise_method_not_supported():\n     # raise error for unsupported propagation operations:\n     with pytest.raises(ValueError):\n         ndd1.uncertainty.propagate(np.mod, ndd2, result, correlation)\n+\n+\n+def test_nddata_bitmask_propagation():\n+    \"\"\"\n+    Regression test for https://github.com/astropy/astropy/issues/15182\n+\n+    Verify that mask propagation works correctly when one operand has no mask,\n+    using a bitwise or handle_mask function (as opposed to the default\n+    np.logical_or).\n+    \"\"\"\n+    array = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]])\n+    mask = np.array([[0, 1, 64], [8, 0, 1], [2, 1, 0]])\n+\n+    nref_nomask = NDDataArithmetic(array)\n+    nref_mask = NDDataArithmetic(array, mask=mask)\n+\n+    # no mask * no mask: result should have no mask\n+    assert nref_nomask.multiply(1.0, handle_mask=np.bitwise_or).mask is None\n+    assert nref_nomask.multiply(nref_nomask, handle_mask=np.bitwise_or).mask is None\n+\n+    # mask * no mask: result should carry the existing mask unchanged\n+    result_mask_const = nref_mask.multiply(1.0, handle_mask=np.bitwise_or).mask\n+    assert_array_equal(result_mask_const, mask)\n+\n+    result_mask_nomask = nref_mask.multiply(nref_nomask, handle_mask=np.bitwise_or).mask\n+    assert_array_equal(result_mask_nomask, mask)\n+\n+    # mask * mask: result should be bitwise-or of both masks\n+    result_mask_mask = nref_mask.multiply(nref_mask, handle_mask=np.bitwise_or).mask\n+    assert_array_equal(result_mask_mask, mask | mask)\n"}
{"instance_id": "astropy__astropy-6938", "model_name_or_path": "harness/claude-sonnet-4-6", "model_patch": "diff --git a/.harness/checkpoints/9598767ff91d/--Users--huetuanthi--dev--agents--harness--astropy--io--fits--fitsrec.py b/.harness/checkpoints/9598767ff91d/--Users--huetuanthi--dev--agents--harness--astropy--io--fits--fitsrec.py\nnew file mode 100644\nindex 0000000000..574b4073b1\n--- /dev/null\n+++ b/.harness/checkpoints/9598767ff91d/--Users--huetuanthi--dev--agents--harness--astropy--io--fits--fitsrec.py\n@@ -0,0 +1,1331 @@\n+# Licensed under a 3-clause BSD style license - see PYFITS.rst\n+\n+import copy\n+import operator\n+import warnings\n+import weakref\n+\n+from contextlib import suppress\n+from functools import reduce\n+\n+import numpy as np\n+\n+from numpy import char as chararray\n+\n+from .column import (ASCIITNULL, FITS2NUMPY, ASCII2NUMPY, ASCII2STR, ColDefs,\n+                     _AsciiColDefs, _FormatX, _FormatP, _VLF, _get_index,\n+                     _wrapx, _unwrapx, _makep, Delayed)\n+from .util import decode_ascii, encode_ascii, _rstrip_inplace\n+from ...utils import lazyproperty\n+\n+\n+class FITS_record:\n+    \"\"\"\n+    FITS record class.\n+\n+    `FITS_record` is used to access records of the `FITS_rec` object.\n+    This will allow us to deal with scaled columns.  It also handles\n+    conversion/scaling of columns in ASCII tables.  The `FITS_record`\n+    class expects a `FITS_rec` object as input.\n+    \"\"\"\n+\n+    def __init__(self, input, row=0, start=None, end=None, step=None,\n+                 base=None, **kwargs):\n+        \"\"\"\n+        Parameters\n+        ----------\n+        input : array\n+           The array to wrap.\n+\n+        row : int, optional\n+           The starting logical row of the array.\n+\n+        start : int, optional\n+           The starting column in the row associated with this object.\n+           Used for subsetting the columns of the `FITS_rec` object.\n+\n+        end : int, optional\n+           The ending column in the row associated with this object.\n+           Used for subsetting the columns of the `FITS_rec` object.\n+        \"\"\"\n+\n+        self.array = input\n+        self.row = row\n+        if base:\n+            width = len(base)\n+        else:\n+            width = self.array._nfields\n+\n+        s = slice(start, end, step).indices(width)\n+        self.start, self.end, self.step = s\n+        self.base = base\n+\n+    def __getitem__(self, key):\n+        if isinstance(key, str):\n+            indx = _get_index(self.array.names, key)\n+\n+            if indx < self.start or indx > self.end - 1:\n+                raise KeyError(\"Key '{}' does not exist.\".format(key))\n+        elif isinstance(key, slice):\n+            return type(self)(self.array, self.row, key.start, key.stop,\n+                              key.step, self)\n+        else:\n+            indx = self._get_index(key)\n+\n+            if indx > self.array._nfields - 1:\n+                raise IndexError('Index out of bounds')\n+\n+        return self.array.field(indx)[self.row]\n+\n+    def __setitem__(self, key, value):\n+        if isinstance(key, str):\n+            indx = _get_index(self.array.names, key)\n+\n+            if indx < self.start or indx > self.end - 1:\n+                raise KeyError(\"Key '{}' does not exist.\".format(key))\n+        elif isinstance(key, slice):\n+            for indx in range(slice.start, slice.stop, slice.step):\n+                indx = self._get_indx(indx)\n+                self.array.field(indx)[self.row] = value\n+        else:\n+            indx = self._get_index(key)\n+            if indx > self.array._nfields - 1:\n+                raise IndexError('Index out of bounds')\n+\n+        self.array.field(indx)[self.row] = value\n+\n+    def __len__(self):\n+        return len(range(self.start, self.end, self.step))\n+\n+    def __repr__(self):\n+        \"\"\"\n+        Display a single row.\n+        \"\"\"\n+\n+        outlist = []\n+        for idx in range(len(self)):\n+            outlist.append(repr(self[idx]))\n+        return '({})'.format(', '.join(outlist))\n+\n+    def field(self, field):\n+        \"\"\"\n+        Get the field data of the record.\n+        \"\"\"\n+\n+        return self.__getitem__(field)\n+\n+    def setfield(self, field, value):\n+        \"\"\"\n+        Set the field data of the record.\n+        \"\"\"\n+\n+        self.__setitem__(field, value)\n+\n+    @lazyproperty\n+    def _bases(self):\n+        bases = [weakref.proxy(self)]\n+        base = self.base\n+        while base:\n+            bases.append(base)\n+            base = base.base\n+        return bases\n+\n+    def _get_index(self, indx):\n+        indices = np.ogrid[:self.array._nfields]\n+        for base in reversed(self._bases):\n+            if base.step < 1:\n+                s = slice(base.start, None, base.step)\n+            else:\n+                s = slice(base.start, base.end, base.step)\n+            indices = indices[s]\n+        return indices[indx]\n+\n+\n+class FITS_rec(np.recarray):\n+    \"\"\"\n+    FITS record array class.\n+\n+    `FITS_rec` is the data part of a table HDU's data part.  This is a layer\n+    over the `~numpy.recarray`, so we can deal with scaled columns.\n+\n+    It inherits all of the standard methods from `numpy.ndarray`.\n+    \"\"\"\n+\n+    _record_type = FITS_record\n+    _character_as_bytes = False\n+\n+    def __new__(subtype, input):\n+        \"\"\"\n+        Construct a FITS record array from a recarray.\n+        \"\"\"\n+\n+        # input should be a record array\n+        if input.dtype.subdtype is None:\n+            self = np.recarray.__new__(subtype, input.shape, input.dtype,\n+                                       buf=input.data)\n+        else:\n+            self = np.recarray.__new__(subtype, input.shape, input.dtype,\n+                                       buf=input.data, strides=input.strides)\n+\n+        self._init()\n+        if self.dtype.fields:\n+            self._nfields = len(self.dtype.fields)\n+\n+        return self\n+\n+    def __setstate__(self, state):\n+        meta = state[-1]\n+        column_state = state[-2]\n+        state = state[:-2]\n+\n+        super().__setstate__(state)\n+\n+        self._col_weakrefs = weakref.WeakSet()\n+\n+        for attr, value in zip(meta, column_state):\n+            setattr(self, attr, value)\n+\n+    def __reduce__(self):\n+        \"\"\"\n+        Return a 3-tuple for pickling a FITS_rec. Use the super-class\n+        functionality but then add in a tuple of FITS_rec-specific\n+        values that get used in __setstate__.\n+        \"\"\"\n+\n+        reconst_func, reconst_func_args, state = super().__reduce__()\n+\n+        # Define FITS_rec-specific attrs that get added to state\n+        column_state = []\n+        meta = []\n+\n+        for attrs in ['_converted', '_heapoffset', '_heapsize', '_nfields',\n+                      '_gap', '_uint', 'parnames', '_coldefs']:\n+\n+            with suppress(AttributeError):\n+                # _coldefs can be Delayed, and file objects cannot be\n+                # picked, it needs to be deepcopied first\n+                if attrs == '_coldefs':\n+                    column_state.append(self._coldefs.__deepcopy__(None))\n+                else:\n+                    column_state.append(getattr(self, attrs))\n+                meta.append(attrs)\n+\n+        state = state + (column_state, meta)\n+\n+        return reconst_func, reconst_func_args, state\n+\n+    def __array_finalize__(self, obj):\n+        if obj is None:\n+            return\n+\n+        if isinstance(obj, FITS_rec):\n+            self._character_as_bytes = obj._character_as_bytes\n+\n+        if isinstance(obj, FITS_rec) and obj.dtype == self.dtype:\n+            self._converted = obj._converted\n+            self._heapoffset = obj._heapoffset\n+            self._heapsize = obj._heapsize\n+            self._col_weakrefs = obj._col_weakrefs\n+            self._coldefs = obj._coldefs\n+            self._nfields = obj._nfields\n+            self._gap = obj._gap\n+            self._uint = obj._uint\n+        elif self.dtype.fields is not None:\n+            # This will allow regular ndarrays with fields, rather than\n+            # just other FITS_rec objects\n+            self._nfields = len(self.dtype.fields)\n+            self._converted = {}\n+\n+            self._heapoffset = getattr(obj, '_heapoffset', 0)\n+            self._heapsize = getattr(obj, '_heapsize', 0)\n+\n+            self._gap = getattr(obj, '_gap', 0)\n+            self._uint = getattr(obj, '_uint', False)\n+            self._col_weakrefs = weakref.WeakSet()\n+            self._coldefs = ColDefs(self)\n+\n+            # Work around chicken-egg problem.  Column.array relies on the\n+            # _coldefs attribute to set up ref back to parent FITS_rec; however\n+            # in the above line the self._coldefs has not been assigned yet so\n+            # this fails.  This patches that up...\n+            for col in self._coldefs:\n+                del col.array\n+                col._parent_fits_rec = weakref.ref(self)\n+        else:\n+            self._init()\n+\n+    def _init(self):\n+        \"\"\"Initializes internal attributes specific to FITS-isms.\"\"\"\n+\n+        self._nfields = 0\n+        self._converted = {}\n+        self._heapoffset = 0\n+        self._heapsize = 0\n+        self._col_weakrefs = weakref.WeakSet()\n+        self._coldefs = None\n+        self._gap = 0\n+        self._uint = False\n+\n+    @classmethod\n+    def from_columns(cls, columns, nrows=0, fill=False, character_as_bytes=False):\n+        \"\"\"\n+        Given a `ColDefs` object of unknown origin, initialize a new `FITS_rec`\n+        object.\n+\n+        .. note::\n+\n+            This was originally part of the ``new_table`` function in the table\n+            module but was moved into a class method since most of its\n+            functionality always had more to do with initializing a `FITS_rec`\n+            object than anything else, and much of it also overlapped with\n+            ``FITS_rec._scale_back``.\n+\n+        Parameters\n+        ----------\n+        columns : sequence of `Column` or a `ColDefs`\n+            The columns from which to create the table data.  If these\n+            columns have data arrays attached that data may be used in\n+            initializing the new table.  Otherwise the input columns\n+            will be used as a template for a new table with the requested\n+            number of rows.\n+\n+        nrows : int\n+            Number of rows in the new table.  If the input columns have data\n+            associated with them, the size of the largest input column is used.\n+            Otherwise the default is 0.\n+\n+        fill : bool\n+            If `True`, will fill all cells with zeros or blanks.  If\n+            `False`, copy the data from input, undefined cells will still\n+            be filled with zeros/blanks.\n+        \"\"\"\n+\n+        if not isinstance(columns, ColDefs):\n+            columns = ColDefs(columns)\n+\n+        # read the delayed data\n+        for column in columns:\n+            arr = column.array\n+            if isinstance(arr, Delayed):\n+                if arr.hdu.data is None:\n+                    column.array = None\n+                else:\n+                    column.array = _get_recarray_field(arr.hdu.data,\n+                                                       arr.field)\n+        # Reset columns._arrays (which we may want to just do away with\n+        # altogether\n+        del columns._arrays\n+\n+        # use the largest column shape as the shape of the record\n+        if nrows == 0:\n+            for arr in columns._arrays:\n+                if arr is not None:\n+                    dim = arr.shape[0]\n+                else:\n+                    dim = 0\n+                if dim > nrows:\n+                    nrows = dim\n+\n+        raw_data = np.empty(columns.dtype.itemsize * nrows, dtype=np.uint8)\n+        raw_data.fill(ord(columns._padding_byte))\n+        data = np.recarray(nrows, dtype=columns.dtype, buf=raw_data).view(cls)\n+        data._character_as_bytes = character_as_bytes\n+\n+        # Make sure the data is a listener for changes to the columns\n+        columns._add_listener(data)\n+\n+        # Previously this assignment was made from hdu.columns, but that's a\n+        # bug since if a _TableBaseHDU has a FITS_rec in its .data attribute\n+        # the _TableBaseHDU.columns property is actually returned from\n+        # .data._coldefs, so this assignment was circular!  Don't make that\n+        # mistake again.\n+        # All of this is an artifact of the fragility of the FITS_rec class,\n+        # and that it can't just be initialized by columns...\n+        data._coldefs = columns\n+\n+        # If fill is True we don't copy anything from the column arrays.  We're\n+        # just using them as a template, and returning a table filled with\n+        # zeros/blanks\n+        if fill:\n+            return data\n+\n+        # Otherwise we have to fill the recarray with data from the input\n+        # columns\n+        for idx, column in enumerate(columns):\n+            # For each column in the ColDef object, determine the number of\n+            # rows in that column.  This will be either the number of rows in\n+            # the ndarray associated with the column, or the number of rows\n+            # given in the call to this function, which ever is smaller.  If\n+            # the input FILL argument is true, the number of rows is set to\n+            # zero so that no data is copied from the original input data.\n+            arr = column.array\n+\n+            if arr is None:\n+                array_size = 0\n+            else:\n+                array_size = len(arr)\n+\n+            n = min(array_size, nrows)\n+\n+            # TODO: At least *some* of this logic is mostly redundant with the\n+            # _convert_foo methods in this class; see if we can eliminate some\n+            # of that duplication.\n+\n+            if not n:\n+                # The input column had an empty array, so just use the fill\n+                # value\n+                continue\n+\n+            field = _get_recarray_field(data, idx)\n+            name = column.name\n+            fitsformat = column.format\n+            recformat = fitsformat.recformat\n+\n+            outarr = field[:n]\n+            inarr = arr[:n]\n+\n+            if isinstance(recformat, _FormatX):\n+                # Data is a bit array\n+                if inarr.shape[-1] == recformat.repeat:\n+                    _wrapx(inarr, outarr, recformat.repeat)\n+                    continue\n+            elif isinstance(recformat, _FormatP):\n+                data._cache_field(name, _makep(inarr, field, recformat,\n+                                               nrows=nrows))\n+                continue\n+            # TODO: Find a better way of determining that the column is meant\n+            # to be FITS L formatted\n+            elif recformat[-2:] == FITS2NUMPY['L'] and inarr.dtype == bool:\n+                # column is boolean\n+                # The raw data field should be filled with either 'T' or 'F'\n+                # (not 0).  Use 'F' as a default\n+                field[:] = ord('F')\n+                # Also save the original boolean array in data._converted so\n+                # that it doesn't have to be re-converted\n+                converted = np.zeros(field.shape, dtype=bool)\n+                converted[:n] = inarr\n+                data._cache_field(name, converted)\n+                # TODO: Maybe this step isn't necessary at all if _scale_back\n+                # will handle it?\n+                inarr = np.where(inarr == np.False_, ord('F'), ord('T'))\n+            elif (columns[idx]._physical_values and\n+                    columns[idx]._pseudo_unsigned_ints):\n+                # Temporary hack...\n+                bzero = column.bzero\n+                converted = np.zeros(field.shape, dtype=inarr.dtype)\n+                converted[:n] = inarr\n+                data._cache_field(name, converted)\n+                if n < nrows:\n+                    # Pre-scale rows below the input data\n+                    field[n:] = -bzero\n+\n+                inarr = inarr - bzero\n+            elif isinstance(columns, _AsciiColDefs):\n+                # Regardless whether the format is character or numeric, if the\n+                # input array contains characters then it's already in the raw\n+                # format for ASCII tables\n+                if fitsformat._pseudo_logical:\n+                    # Hack to support converting from 8-bit T/F characters\n+                    # Normally the column array is a chararray of 1 character\n+                    # strings, but we need to view it as a normal ndarray of\n+                    # 8-bit ints to fill it with ASCII codes for 'T' and 'F'\n+                    outarr = field.view(np.uint8, np.ndarray)[:n]\n+                elif arr.dtype.kind not in ('S', 'U'):\n+                    # Set up views of numeric columns with the appropriate\n+                    # numeric dtype\n+                    # Fill with the appropriate blanks for the column format\n+                    data._cache_field(name, np.zeros(nrows, dtype=arr.dtype))\n+                    outarr = data._converted[name][:n]\n+\n+                outarr[:] = inarr\n+                continue\n+\n+            if inarr.shape != outarr.shape:\n+                if (inarr.dtype.kind == outarr.dtype.kind and\n+                        inarr.dtype.kind in ('U', 'S') and\n+                        inarr.dtype != outarr.dtype):\n+\n+                    inarr_rowsize = inarr[0].size\n+                    inarr = inarr.flatten().view(outarr.dtype)\n+\n+                # This is a special case to handle input arrays with\n+                # non-trivial TDIMn.\n+                # By design each row of the outarray is 1-D, while each row of\n+                # the input array may be n-D\n+                if outarr.ndim > 1:\n+                    # The normal case where the first dimension is the rows\n+                    inarr_rowsize = inarr[0].size\n+                    inarr = inarr.reshape(n, inarr_rowsize)\n+                    outarr[:, :inarr_rowsize] = inarr\n+                else:\n+                    # Special case for strings where the out array only has one\n+                    # dimension (the second dimension is rolled up into the\n+                    # strings\n+                    outarr[:n] = inarr.ravel()\n+            else:\n+                outarr[:] = inarr\n+\n+        # Now replace the original column array references with the new\n+        # fields\n+        # This is required to prevent the issue reported in\n+        # https://github.com/spacetelescope/PyFITS/issues/99\n+        for idx in range(len(columns)):\n+            columns._arrays[idx] = data.field(idx)\n+\n+        return data\n+\n+    def __repr__(self):\n+        # Force use of the normal ndarray repr (rather than the new\n+        # one added for recarray in Numpy 1.10) for backwards compat\n+        return np.ndarray.__repr__(self)\n+\n+    def __getitem__(self, key):\n+        if self._coldefs is None:\n+            return super().__getitem__(key)\n+\n+        if isinstance(key, str):\n+            return self.field(key)\n+\n+        # Have to view as a recarray then back as a FITS_rec, otherwise the\n+        # circular reference fix/hack in FITS_rec.field() won't preserve\n+        # the slice.\n+        out = self.view(np.recarray)[key]\n+        if type(out) is not np.recarray:\n+            # Oops, we got a single element rather than a view. In that case,\n+            # return a Record, which has no __getstate__ and is more efficient.\n+            return self._record_type(self, key)\n+\n+        # We got a view; change it back to our class, and add stuff\n+        out = out.view(type(self))\n+        out._coldefs = ColDefs(self._coldefs)\n+        arrays = []\n+        out._converted = {}\n+        for idx, name in enumerate(self._coldefs.names):\n+            #\n+            # Store the new arrays for the _coldefs object\n+            #\n+            arrays.append(self._coldefs._arrays[idx][key])\n+\n+            # Ensure that the sliced FITS_rec will view the same scaled\n+            # columns as the original; this is one of the few cases where\n+            # it is not necessary to use _cache_field()\n+            if name in self._converted:\n+                dummy = self._converted[name]\n+                field = np.ndarray.__getitem__(dummy, key)\n+                out._converted[name] = field\n+\n+        out._coldefs._arrays = arrays\n+        return out\n+\n+    def __setitem__(self, key, value):\n+        if self._coldefs is None:\n+            return super().__setitem__(key, value)\n+\n+        if isinstance(key, str):\n+            self[key][:] = value\n+            return\n+\n+        if isinstance(key, slice):\n+            end = min(len(self), key.stop or len(self))\n+            end = max(0, end)\n+            start = max(0, key.start or 0)\n+            end = min(end, start + len(value))\n+\n+            for idx in range(start, end):\n+                self.__setitem__(idx, value[idx - start])\n+            return\n+\n+        if isinstance(value, FITS_record):\n+            for idx in range(self._nfields):\n+                self.field(self.names[idx])[key] = value.field(self.names[idx])\n+        elif isinstance(value, (tuple, list, np.void)):\n+            if self._nfields == len(value):\n+                for idx in range(self._nfields):\n+                    self.field(idx)[key] = value[idx]\n+            else:\n+                raise ValueError('Input tuple or list required to have {} '\n+                                 'elements.'.format(self._nfields))\n+        else:\n+            raise TypeError('Assignment requires a FITS_record, tuple, or '\n+                            'list as input.')\n+\n+    def copy(self, order='C'):\n+        \"\"\"\n+        The Numpy documentation lies; `numpy.ndarray.copy` is not equivalent to\n+        `numpy.copy`.  Differences include that it re-views the copied array as\n+        self's ndarray subclass, as though it were taking a slice; this means\n+        ``__array_finalize__`` is called and the copy shares all the array\n+        attributes (including ``._converted``!).  So we need to make a deep\n+        copy of all those attributes so that the two arrays truly do not share\n+        any data.\n+        \"\"\"\n+\n+        new = super().copy(order=order)\n+\n+        new.__dict__ = copy.deepcopy(self.__dict__)\n+        return new\n+\n+    @property\n+    def columns(self):\n+        \"\"\"\n+        A user-visible accessor for the coldefs.\n+\n+        See https://aeon.stsci.edu/ssb/trac/pyfits/ticket/44\n+        \"\"\"\n+\n+        return self._coldefs\n+\n+    @property\n+    def _coldefs(self):\n+        # This used to be a normal internal attribute, but it was changed to a\n+        # property as a quick and transparent way to work around the reference\n+        # leak bug fixed in https://github.com/astropy/astropy/pull/4539\n+        #\n+        # See the long comment in the Column.array property for more details\n+        # on this.  But in short, FITS_rec now has a ._col_weakrefs attribute\n+        # which is a WeakSet of weakrefs to each Column in _coldefs.\n+        #\n+        # So whenever ._coldefs is set we also add each Column in the ColDefs\n+        # to the weakrefs set.  This is an easy way to find out if a Column has\n+        # any references to it external to the FITS_rec (i.e. a user assigned a\n+        # column to a variable).  If the column is still in _col_weakrefs then\n+        # there are other references to it external to this FITS_rec.  We use\n+        # that information in __del__ to save off copies of the array data\n+        # for those columns to their Column.array property before our memory\n+        # is freed.\n+        return self.__dict__.get('_coldefs')\n+\n+    @_coldefs.setter\n+    def _coldefs(self, cols):\n+        self.__dict__['_coldefs'] = cols\n+        if isinstance(cols, ColDefs):\n+            for col in cols.columns:\n+                self._col_weakrefs.add(col)\n+\n+    @_coldefs.deleter\n+    def _coldefs(self):\n+        try:\n+            del self.__dict__['_coldefs']\n+        except KeyError as exc:\n+            raise AttributeError(exc.args[0])\n+\n+    def __del__(self):\n+        try:\n+            del self._coldefs\n+            if self.dtype.fields is not None:\n+                for col in self._col_weakrefs:\n+\n+                    if col.array is not None:\n+                        col.array = col.array.copy()\n+\n+        # See issues #4690 and #4912\n+        except (AttributeError, TypeError):  # pragma: no cover\n+            pass\n+\n+    @property\n+    def names(self):\n+        \"\"\"List of column names.\"\"\"\n+\n+        if self.dtype.fields:\n+            return list(self.dtype.names)\n+        elif getattr(self, '_coldefs', None) is not None:\n+            return self._coldefs.names\n+        else:\n+            return None\n+\n+    @property\n+    def formats(self):\n+        \"\"\"List of column FITS formats.\"\"\"\n+\n+        if getattr(self, '_coldefs', None) is not None:\n+            return self._coldefs.formats\n+\n+        return None\n+\n+    @property\n+    def _raw_itemsize(self):\n+        \"\"\"\n+        Returns the size of row items that would be written to the raw FITS\n+        file, taking into account the possibility of unicode columns being\n+        compactified.\n+\n+        Currently for internal use only.\n+        \"\"\"\n+\n+        if _has_unicode_fields(self):\n+            total_itemsize = 0\n+            for field in self.dtype.fields.values():\n+                itemsize = field[0].itemsize\n+                if field[0].kind == 'U':\n+                    itemsize = itemsize // 4\n+                total_itemsize += itemsize\n+            return total_itemsize\n+        else:\n+            # Just return the normal itemsize\n+            return self.itemsize\n+\n+    def field(self, key):\n+        \"\"\"\n+        A view of a `Column`'s data as an array.\n+        \"\"\"\n+\n+        # NOTE: The *column* index may not be the same as the field index in\n+        # the recarray, if the column is a phantom column\n+        column = self.columns[key]\n+        name = column.name\n+        format = column.format\n+\n+        if format.dtype.itemsize == 0:\n+            warnings.warn(\n+                'Field {!r} has a repeat count of 0 in its format code, '\n+                'indicating an empty field.'.format(key))\n+            return np.array([], dtype=format.dtype)\n+\n+        # If field's base is a FITS_rec, we can run into trouble because it\n+        # contains a reference to the ._coldefs object of the original data;\n+        # this can lead to a circular reference; see ticket #49\n+        base = self\n+        while (isinstance(base, FITS_rec) and\n+                isinstance(base.base, np.recarray)):\n+            base = base.base\n+        # base could still be a FITS_rec in some cases, so take care to\n+        # use rec.recarray.field to avoid a potential infinite\n+        # recursion\n+        field = _get_recarray_field(base, name)\n+\n+        if name not in self._converted:\n+            recformat = format.recformat\n+            # TODO: If we're now passing the column to these subroutines, do we\n+            # really need to pass them the recformat?\n+            if isinstance(recformat, _FormatP):\n+                # for P format\n+                converted = self._convert_p(column, field, recformat)\n+            else:\n+                # Handle all other column data types which are fixed-width\n+                # fields\n+                converted = self._convert_other(column, field, recformat)\n+\n+            # Note: Never assign values directly into the self._converted dict;\n+            # always go through self._cache_field; this way self._converted is\n+            # only used to store arrays that are not already direct views of\n+            # our own data.\n+            self._cache_field(name, converted)\n+            return converted\n+\n+        return self._converted[name]\n+\n+    def _cache_field(self, name, field):\n+        \"\"\"\n+        Do not store fields in _converted if one of its bases is self,\n+        or if it has a common base with self.\n+\n+        This results in a reference cycle that cannot be broken since\n+        ndarrays do not participate in cyclic garbage collection.\n+        \"\"\"\n+\n+        base = field\n+        while True:\n+            self_base = self\n+            while True:\n+                if self_base is base:\n+                    return\n+\n+                if getattr(self_base, 'base', None) is not None:\n+                    self_base = self_base.base\n+                else:\n+                    break\n+\n+            if getattr(base, 'base', None) is not None:\n+                base = base.base\n+            else:\n+                break\n+\n+        self._converted[name] = field\n+\n+    def _update_column_attribute_changed(self, column, idx, attr, old_value,\n+                                         new_value):\n+        \"\"\"\n+        Update how the data is formatted depending on changes to column\n+        attributes initiated by the user through the `Column` interface.\n+\n+        Dispatches column attribute change notifications to individual methods\n+        for each attribute ``_update_column_<attr>``\n+        \"\"\"\n+\n+        method_name = '_update_column_{0}'.format(attr)\n+        if hasattr(self, method_name):\n+            # Right now this is so we can be lazy and not implement updaters\n+            # for every attribute yet--some we may not need at all, TBD\n+            getattr(self, method_name)(column, idx, old_value, new_value)\n+\n+    def _update_column_name(self, column, idx, old_name, name):\n+        \"\"\"Update the dtype field names when a column name is changed.\"\"\"\n+\n+        dtype = self.dtype\n+        # Updating the names on the dtype should suffice\n+        dtype.names = dtype.names[:idx] + (name,) + dtype.names[idx + 1:]\n+\n+    def _convert_x(self, field, recformat):\n+        \"\"\"Convert a raw table column to a bit array as specified by the\n+        FITS X format.\n+        \"\"\"\n+\n+        dummy = np.zeros(self.shape + (recformat.repeat,), dtype=np.bool_)\n+        _unwrapx(field, dummy, recformat.repeat)\n+        return dummy\n+\n+    def _convert_p(self, column, field, recformat):\n+        \"\"\"Convert a raw table column of FITS P or Q format descriptors\n+        to a VLA column with the array data returned from the heap.\n+        \"\"\"\n+\n+        dummy = _VLF([None] * len(self), dtype=recformat.dtype)\n+        raw_data = self._get_raw_data()\n+\n+        if raw_data is None:\n+            raise OSError(\n+                \"Could not find heap data for the {!r} variable-length \"\n+                \"array column.\".format(column.name))\n+\n+        for idx in range(len(self)):\n+            offset = field[idx, 1] + self._heapoffset\n+            count = field[idx, 0]\n+\n+            if recformat.dtype == 'a':\n+                dt = np.dtype(recformat.dtype + str(1))\n+                arr_len = count * dt.itemsize\n+                da = raw_data[offset:offset + arr_len].view(dt)\n+                da = np.char.array(da.view(dtype=dt), itemsize=count)\n+                dummy[idx] = decode_ascii(da)\n+            else:\n+                dt = np.dtype(recformat.dtype)\n+                arr_len = count * dt.itemsize\n+                dummy[idx] = raw_data[offset:offset + arr_len].view(dt)\n+                dummy[idx].dtype = dummy[idx].dtype.newbyteorder('>')\n+                # Each array in the field may now require additional\n+                # scaling depending on the other scaling parameters\n+                # TODO: The same scaling parameters apply to every\n+                # array in the column so this is currently very slow; we\n+                # really only need to check once whether any scaling will\n+                # be necessary and skip this step if not\n+                # TODO: Test that this works for X format; I don't think\n+                # that it does--the recformat variable only applies to the P\n+                # format not the X format\n+                dummy[idx] = self._convert_other(column, dummy[idx],\n+                                                 recformat)\n+\n+        return dummy\n+\n+    def _convert_ascii(self, column, field):\n+        \"\"\"\n+        Special handling for ASCII table columns to convert columns containing\n+        numeric types to actual numeric arrays from the string representation.\n+        \"\"\"\n+\n+        format = column.format\n+        recformat = ASCII2NUMPY[format[0]]\n+        # if the string = TNULL, return ASCIITNULL\n+        nullval = str(column.null).strip().encode('ascii')\n+        if len(nullval) > format.width:\n+            nullval = nullval[:format.width]\n+\n+        # Before using .replace make sure that any trailing bytes in each\n+        # column are filled with spaces, and *not*, say, nulls; this causes\n+        # functions like replace to potentially leave gibberish bytes in the\n+        # array buffer.\n+        dummy = np.char.ljust(field, format.width)\n+        dummy = np.char.replace(dummy, encode_ascii('D'), encode_ascii('E'))\n+        null_fill = encode_ascii(str(ASCIITNULL).rjust(format.width))\n+\n+        # Convert all fields equal to the TNULL value (nullval) to empty fields.\n+        # TODO: These fields really should be conerted to NaN or something else undefined.\n+        # Currently they are converted to empty fields, which are then set to zero.\n+        dummy = np.where(np.char.strip(dummy) == nullval, null_fill, dummy)\n+\n+        # always replace empty fields, see https://github.com/astropy/astropy/pull/5394\n+        if nullval != b'':\n+            dummy = np.where(np.char.strip(dummy) == b'', null_fill, dummy)\n+\n+        try:\n+            dummy = np.array(dummy, dtype=recformat)\n+        except ValueError as exc:\n+            indx = self.names.index(column.name)\n+            raise ValueError(\n+                '{}; the header may be missing the necessary TNULL{} '\n+                'keyword or the table contains invalid data'.format(\n+                    exc, indx + 1))\n+\n+        return dummy\n+\n+    def _convert_other(self, column, field, recformat):\n+        \"\"\"Perform conversions on any other fixed-width column data types.\n+\n+        This may not perform any conversion at all if it's not necessary, in\n+        which case the original column array is returned.\n+        \"\"\"\n+\n+        if isinstance(recformat, _FormatX):\n+            # special handling for the X format\n+            return self._convert_x(field, recformat)\n+\n+        (_str, _bool, _number, _scale, _zero, bscale, bzero, dim) = \\\n+            self._get_scale_factors(column)\n+\n+        indx = self.names.index(column.name)\n+\n+        # ASCII table, convert strings to numbers\n+        # TODO:\n+        # For now, check that these are ASCII columns by checking the coldefs\n+        # type; in the future all columns (for binary tables, ASCII tables, or\n+        # otherwise) should \"know\" what type they are already and how to handle\n+        # converting their data from FITS format to native format and vice\n+        # versa...\n+        if not _str and isinstance(self._coldefs, _AsciiColDefs):\n+            field = self._convert_ascii(column, field)\n+\n+        # Test that the dimensions given in dim are sensible; otherwise\n+        # display a warning and ignore them\n+        if dim:\n+            # See if the dimensions already match, if not, make sure the\n+            # number items will fit in the specified dimensions\n+            if field.ndim > 1:\n+                actual_shape = field.shape[1:]\n+                if _str:\n+                    actual_shape = actual_shape + (field.itemsize,)\n+            else:\n+                actual_shape = field.shape[0]\n+\n+            if dim == actual_shape:\n+                # The array already has the correct dimensions, so we\n+                # ignore dim and don't convert\n+                dim = None\n+            else:\n+                nitems = reduce(operator.mul, dim)\n+                if _str:\n+                    actual_nitems = field.itemsize\n+                elif len(field.shape) == 1:  # No repeat count in TFORMn, equivalent to 1\n+                    actual_nitems = 1\n+                else:\n+                    actual_nitems = field.shape[1]\n+                if nitems > actual_nitems:\n+                    warnings.warn(\n+                        'TDIM{} value {:d} does not fit with the size of '\n+                        'the array items ({:d}).  TDIM{:d} will be ignored.'\n+                        .format(indx + 1, self._coldefs[indx].dims,\n+                                actual_nitems, indx + 1))\n+                    dim = None\n+\n+        # further conversion for both ASCII and binary tables\n+        # For now we've made columns responsible for *knowing* whether their\n+        # data has been scaled, but we make the FITS_rec class responsible for\n+        # actually doing the scaling\n+        # TODO: This also needs to be fixed in the effort to make Columns\n+        # responsible for scaling their arrays to/from FITS native values\n+        if not column.ascii and column.format.p_format:\n+            format_code = column.format.p_format\n+        else:\n+            # TODO: Rather than having this if/else it might be nice if the\n+            # ColumnFormat class had an attribute guaranteed to give the format\n+            # of actual values in a column regardless of whether the true\n+            # format is something like P or Q\n+            format_code = column.format.format\n+\n+        if (_number and (_scale or _zero) and not column._physical_values):\n+            # This is to handle pseudo unsigned ints in table columns\n+            # TODO: For now this only really works correctly for binary tables\n+            # Should it work for ASCII tables as well?\n+            if self._uint:\n+                if bzero == 2**15 and format_code == 'I':\n+                    field = np.array(field, dtype=np.uint16)\n+                elif bzero == 2**31 and format_code == 'J':\n+                    field = np.array(field, dtype=np.uint32)\n+                elif bzero == 2**63 and format_code == 'K':\n+                    field = np.array(field, dtype=np.uint64)\n+                    bzero64 = np.uint64(2 ** 63)\n+                else:\n+                    field = np.array(field, dtype=np.float64)\n+            else:\n+                field = np.array(field, dtype=np.float64)\n+\n+            if _scale:\n+                np.multiply(field, bscale, field)\n+            if _zero:\n+                if self._uint and format_code == 'K':\n+                    # There is a chance of overflow, so be careful\n+                    test_overflow = field.copy()\n+                    try:\n+                        test_overflow += bzero64\n+                    except OverflowError:\n+                        warnings.warn(\n+                            \"Overflow detected while applying TZERO{0:d}. \"\n+                            \"Returning unscaled data.\".format(indx + 1))\n+                    else:\n+                        field = test_overflow\n+                else:\n+                    field += bzero\n+        elif _bool and field.dtype != bool:\n+            field = np.equal(field, ord('T'))\n+        elif _str:\n+            if not self._character_as_bytes:\n+                with suppress(UnicodeDecodeError):\n+                    field = decode_ascii(field)\n+\n+        if dim:\n+            # Apply the new field item dimensions\n+            nitems = reduce(operator.mul, dim)\n+            if field.ndim > 1:\n+                field = field[:, :nitems]\n+            if _str:\n+                fmt = field.dtype.char\n+                dtype = ('|{}{}'.format(fmt, dim[-1]), dim[:-1])\n+                field.dtype = dtype\n+            else:\n+                field.shape = (field.shape[0],) + dim\n+\n+        return field\n+\n+    def _get_heap_data(self):\n+        \"\"\"\n+        Returns a pointer into the table's raw data to its heap (if present).\n+\n+        This is returned as a numpy byte array.\n+        \"\"\"\n+\n+        if self._heapsize:\n+            raw_data = self._get_raw_data().view(np.ubyte)\n+            heap_end = self._heapoffset + self._heapsize\n+            return raw_data[self._heapoffset:heap_end]\n+        else:\n+            return np.array([], dtype=np.ubyte)\n+\n+    def _get_raw_data(self):\n+        \"\"\"\n+        Returns the base array of self that \"raw data array\" that is the\n+        array in the format that it was first read from a file before it was\n+        sliced or viewed as a different type in any way.\n+\n+        This is determined by walking through the bases until finding one that\n+        has at least the same number of bytes as self, plus the heapsize.  This\n+        may be the immediate .base but is not always.  This is used primarily\n+        for variable-length array support which needs to be able to find the\n+        heap (the raw data *may* be larger than nbytes + heapsize if it\n+        contains a gap or padding).\n+\n+        May return ``None`` if no array resembling the \"raw data\" according to\n+        the stated criteria can be found.\n+        \"\"\"\n+\n+        raw_data_bytes = self.nbytes + self._heapsize\n+        base = self\n+        while hasattr(base, 'base') and base.base is not None:\n+            base = base.base\n+            if hasattr(base, 'nbytes') and base.nbytes >= raw_data_bytes:\n+                return base\n+\n+    def _get_scale_factors(self, column):\n+        \"\"\"Get all the scaling flags and factors for one column.\"\"\"\n+\n+        # TODO: Maybe this should be a method/property on Column?  Or maybe\n+        # it's not really needed at all...\n+        _str = column.format.format == 'A'\n+        _bool = column.format.format == 'L'\n+\n+        _number = not (_bool or _str)\n+        bscale = column.bscale\n+        bzero = column.bzero\n+\n+        _scale = bscale not in ('', None, 1)\n+        _zero = bzero not in ('', None, 0)\n+\n+        # ensure bscale/bzero are numbers\n+        if not _scale:\n+            bscale = 1\n+        if not _zero:\n+            bzero = 0\n+\n+        # column._dims gives a tuple, rather than column.dim which returns the\n+        # original string format code from the FITS header...\n+        dim = column._dims\n+\n+        return (_str, _bool, _number, _scale, _zero, bscale, bzero, dim)\n+\n+    def _scale_back(self, update_heap_pointers=True):\n+        \"\"\"\n+        Update the parent array, using the (latest) scaled array.\n+\n+        If ``update_heap_pointers`` is `False`, this will leave all the heap\n+        pointers in P/Q columns as they are verbatim--it only makes sense to do\n+        this if there is already data on the heap and it can be guaranteed that\n+        that data has not been modified, and there is not new data to add to\n+        the heap.  Currently this is only used as an optimization for\n+        CompImageHDU that does its own handling of the heap.\n+        \"\"\"\n+\n+        # Running total for the new heap size\n+        heapsize = 0\n+\n+        for indx, name in enumerate(self.dtype.names):\n+            column = self._coldefs[indx]\n+            recformat = column.format.recformat\n+            raw_field = _get_recarray_field(self, indx)\n+\n+            # add the location offset of the heap area for each\n+            # variable length column\n+            if isinstance(recformat, _FormatP):\n+                # Irritatingly, this can return a different dtype than just\n+                # doing np.dtype(recformat.dtype); but this returns the results\n+                # that we want.  For example if recformat.dtype is 'a' we want\n+                # an array of characters.\n+                dtype = np.array([], dtype=recformat.dtype).dtype\n+\n+                if update_heap_pointers and name in self._converted:\n+                    # The VLA has potentially been updated, so we need to\n+                    # update the array descriptors\n+                    raw_field[:] = 0  # reset\n+                    npts = [len(arr) for arr in self._converted[name]]\n+\n+                    raw_field[:len(npts), 0] = npts\n+                    raw_field[1:, 1] = (np.add.accumulate(raw_field[:-1, 0]) *\n+                                        dtype.itemsize)\n+                    raw_field[:, 1][:] += heapsize\n+\n+                heapsize += raw_field[:, 0].sum() * dtype.itemsize\n+                # Even if this VLA has not been read or updated, we need to\n+                # include the size of its constituent arrays in the heap size\n+                # total\n+\n+            if isinstance(recformat, _FormatX) and name in self._converted:\n+                _wrapx(self._converted[name], raw_field, recformat.repeat)\n+                continue\n+\n+            _str, _bool, _number, _scale, _zero, bscale, bzero, _ = \\\n+                self._get_scale_factors(column)\n+\n+            field = self._converted.get(name, raw_field)\n+\n+            # conversion for both ASCII and binary tables\n+            if _number or _str:\n+                if _number and (_scale or _zero) and column._physical_values:\n+                    dummy = field.copy()\n+                    if _zero:\n+                        dummy -= bzero\n+                    if _scale:\n+                        dummy /= bscale\n+                    # This will set the raw values in the recarray back to\n+                    # their non-physical storage values, so the column should\n+                    # be mark is not scaled\n+                    column._physical_values = False\n+                elif _str or isinstance(self._coldefs, _AsciiColDefs):\n+                    dummy = field\n+                else:\n+                    continue\n+\n+                # ASCII table, convert numbers to strings\n+                if isinstance(self._coldefs, _AsciiColDefs):\n+                    self._scale_back_ascii(indx, dummy, raw_field)\n+                # binary table string column\n+                elif isinstance(raw_field, chararray.chararray):\n+                    self._scale_back_strings(indx, dummy, raw_field)\n+                # all other binary table columns\n+                else:\n+                    if len(raw_field) and isinstance(raw_field[0],\n+                                                     np.integer):\n+                        dummy = np.around(dummy)\n+\n+                    if raw_field.shape == dummy.shape:\n+                        raw_field[:] = dummy\n+                    else:\n+                        # Reshaping the data is necessary in cases where the\n+                        # TDIMn keyword was used to shape a column's entries\n+                        # into arrays\n+                        raw_field[:] = dummy.ravel().view(raw_field.dtype)\n+\n+                del dummy\n+\n+            # ASCII table does not have Boolean type\n+            elif _bool and name in self._converted:\n+                choices = (np.array([ord('F')], dtype=np.int8)[0],\n+                           np.array([ord('T')], dtype=np.int8)[0])\n+                raw_field[:] = np.choose(field, choices)\n+\n+        # Store the updated heapsize\n+        self._heapsize = heapsize\n+\n+    def _scale_back_strings(self, col_idx, input_field, output_field):\n+        # There are a few possibilities this has to be able to handle properly\n+        # The input_field, which comes from the _converted column is of dtype\n+        # 'Un' so that elements read out of the array are normal str\n+        # objects (i.e. unicode strings)\n+        #\n+        # At the other end the *output_field* may also be of type 'S' or of\n+        # type 'U'.  It will *usually* be of type 'S' because when reading\n+        # an existing FITS table the raw data is just ASCII strings, and\n+        # represented in Numpy as an S array.  However, when a user creates\n+        # a new table from scratch, they *might* pass in a column containing\n+        # unicode strings (dtype 'U').  Therefore the output_field of the\n+        # raw array is actually a unicode array.  But we still want to make\n+        # sure the data is encodable as ASCII.  Later when we write out the\n+        # array we use, in the dtype 'U' case, a different write routine\n+        # that writes row by row and encodes any 'U' columns to ASCII.\n+\n+        # If the output_field is non-ASCII we will worry about ASCII encoding\n+        # later when writing; otherwise we can do it right here\n+        if input_field.dtype.kind == 'U' and output_field.dtype.kind == 'S':\n+            try:\n+                _ascii_encode(input_field, out=output_field)\n+            except _UnicodeArrayEncodeError as exc:\n+                raise ValueError(\n+                    \"Could not save column '{0}': Contains characters that \"\n+                    \"cannot be encoded as ASCII as required by FITS, starting \"\n+                    \"at the index {1!r} of the column, and the index {2} of \"\n+                    \"the string at that location.\".format(\n+                        self._coldefs[col_idx].name,\n+                        exc.index[0] if len(exc.index) == 1 else exc.index,\n+                        exc.start))\n+        else:\n+            # Otherwise go ahead and do a direct copy into--if both are type\n+            # 'U' we'll handle encoding later\n+            input_field = input_field.flatten().view(output_field.dtype)\n+            output_field.flat[:] = input_field\n+\n+        # Ensure that blanks at the end of each string are\n+        # converted to nulls instead of spaces, see Trac #15\n+        # and #111\n+        _rstrip_inplace(output_field)\n+\n+    def _scale_back_ascii(self, col_idx, input_field, output_field):\n+        \"\"\"\n+        Convert internal array values back to ASCII table representation.\n+\n+        The ``input_field`` is the internal representation of the values, and\n+        the ``output_field`` is the character array representing the ASCII\n+        output that will be written.\n+        \"\"\"\n+\n+        starts = self._coldefs.starts[:]\n+        spans = self._coldefs.spans\n+        format = self._coldefs[col_idx].format\n+\n+        # The the index of the \"end\" column of the record, beyond\n+        # which we can't write\n+        end = super().field(-1).itemsize\n+        starts.append(end + starts[-1])\n+\n+        if col_idx > 0:\n+            lead = starts[col_idx] - starts[col_idx - 1] - spans[col_idx - 1]\n+        else:\n+            lead = 0\n+\n+        if lead < 0:\n+            warnings.warn('Column {!r} starting point overlaps the previous '\n+                          'column.'.format(col_idx + 1))\n+\n+        trail = starts[col_idx + 1] - starts[col_idx] - spans[col_idx]\n+\n+        if trail < 0:\n+            warnings.warn('Column {!r} ending point overlaps the next '\n+                          'column.'.format(col_idx + 1))\n+\n+        # TODO: It would be nice if these string column formatting\n+        # details were left to a specialized class, as is the case\n+        # with FormatX and FormatP\n+        if 'A' in format:\n+            _pc = '{:'\n+        else:\n+            _pc = '{:>'\n+\n+        fmt = ''.join([_pc, format[1:], ASCII2STR[format[0]], '}',\n+                       (' ' * trail)])\n+\n+        # Even if the format precision is 0, we should output a decimal point\n+        # as long as there is space to do so--not including a decimal point in\n+        # a float value is discouraged by the FITS Standard\n+        trailing_decimal = (format.precision == 0 and\n+                            format.format in ('F', 'E', 'D'))\n+\n+        # not using numarray.strings's num2char because the\n+        # result is not allowed to expand (as C/Python does).\n+        for jdx, value in enumerate(input_field):\n+            value = fmt.format(value)\n+            if len(value) > starts[col_idx + 1] - starts[col_idx]:\n+                raise ValueError(\n+                    \"Value {!r} does not fit into the output's itemsize of \"\n+                    \"{}.\".format(value, spans[col_idx]))\n+\n+            if trailing_decimal and value[0] == ' ':\n+                # We have some extra space in the field for the trailing\n+                # decimal point\n+                value = value[1:] + '.'\n+\n+            output_field[jdx] = value\n+\n+        # Replace exponent separator in floating point numbers\n+        if 'D' in format:\n+            output_field.replace(encode_ascii('E'), encode_ascii('D'))\n+\n+\n+def _get_recarray_field(array, key):\n+    \"\"\"\n+    Compatibility function for using the recarray base class's field method.\n+    This incorporates the legacy functionality of returning string arrays as\n+    Numeric-style chararray objects.\n+    \"\"\"\n+\n+    # Numpy >= 1.10.dev recarray no longer returns chararrays for strings\n+    # This is currently needed for backwards-compatibility and for\n+    # automatic truncation of trailing whitespace\n+    field = np.recarray.field(array, key)\n+    if (field.dtype.char in ('S', 'U') and\n+            not isinstance(field, chararray.chararray)):\n+        field = field.view(chararray.chararray)\n+    return field\n+\n+\n+class _UnicodeArrayEncodeError(UnicodeEncodeError):\n+    def __init__(self, encoding, object_, start, end, reason, index):\n+        super().__init__(encoding, object_, start, end, reason)\n+        self.index = index\n+\n+\n+def _ascii_encode(inarray, out=None):\n+    \"\"\"\n+    Takes a unicode array and fills the output string array with the ASCII\n+    encodings (if possible) of the elements of the input array.  The two arrays\n+    must be the same size (though not necessarily the same shape).\n+\n+    This is like an inplace version of `np.char.encode` though simpler since\n+    it's only limited to ASCII, and hence the size of each character is\n+    guaranteed to be 1 byte.\n+\n+    If any strings are non-ASCII an UnicodeArrayEncodeError is raised--this is\n+    just a `UnicodeEncodeError` with an additional attribute for the index of\n+    the item that couldn't be encoded.\n+    \"\"\"\n+\n+    out_dtype = np.dtype(('S{0}'.format(inarray.dtype.itemsize // 4),\n+                         inarray.dtype.shape))\n+    if out is not None:\n+        out = out.view(out_dtype)\n+\n+    op_dtypes = [inarray.dtype, out_dtype]\n+    op_flags = [['readonly'], ['writeonly', 'allocate']]\n+    it = np.nditer([inarray, out], op_dtypes=op_dtypes,\n+                   op_flags=op_flags, flags=['zerosize_ok'])\n+\n+    try:\n+        for initem, outitem in it:\n+            outitem[...] = initem.item().encode('ascii')\n+    except UnicodeEncodeError as exc:\n+        index = np.unravel_index(it.iterindex, inarray.shape)\n+        raise _UnicodeArrayEncodeError(*(exc.args + (index,)))\n+\n+    return it.operands[1]\n+\n+\n+def _has_unicode_fields(array):\n+    \"\"\"\n+    Returns True if any fields in a structured array have Unicode dtype.\n+    \"\"\"\n+\n+    dtypes = (d[0] for d in array.dtype.fields.values())\n+    return any(d.kind == 'U' for d in dtypes)\ndiff --git a/astropy/io/fits/fitsrec.py b/astropy/io/fits/fitsrec.py\nindex 574b4073b1..621e0925b5 100644\n--- a/astropy/io/fits/fitsrec.py\n+++ b/astropy/io/fits/fitsrec.py\n@@ -1261,7 +1261,8 @@ class FITS_rec(np.recarray):\n \n         # Replace exponent separator in floating point numbers\n         if 'D' in format:\n-            output_field.replace(encode_ascii('E'), encode_ascii('D'))\n+            output_field[:] = np.char.replace(output_field, encode_ascii('E'),\n+                                              encode_ascii('D'))\n \n \n def _get_recarray_field(array, key):\ndiff --git a/astropy/io/fits/tests/test_table.py b/astropy/io/fits/tests/test_table.py\nindex 67fe21aa72..b998f7f780 100644\n--- a/astropy/io/fits/tests/test_table.py\n+++ b/astropy/io/fits/tests/test_table.py\n@@ -300,6 +300,35 @@ class TestTableFunctions(FitsTestCase):\n         hdul.close()\n         a.close()\n \n+    def test_ascii_table_double_exponent(self):\n+        \"\"\"\n+        Regression test for a bug where writing an ASCII table column with\n+        format 'D' (double precision with D exponent) failed to replace the\n+        'E' exponent character with 'D' in the output, because\n+        ``chararray.replace`` is not an in-place operation \u2014 it returns a new\n+        array whose result was being silently discarded.\n+        \"\"\"\n+        # Create a column of doubles using the ASCII 'D' (exponent) format\n+        c = fits.Column(name='col1', format='D25.17',\n+                        array=np.array([1.23456789e-99, 9.87654321e+99]))\n+        tbhdu = fits.TableHDU.from_columns([c])\n+        tbhdu.writeto(self.temp('ascii_d_exponent.fits'), overwrite=True)\n+\n+        # Read the raw bytes back and verify that 'D' exponents were written,\n+        # not 'E' exponents.\n+        with fits.open(self.temp('ascii_d_exponent.fits')) as hdul:\n+            raw = hdul[1].data.tobytes()\n+        assert b'D' in raw, (\n+            \"Expected 'D' exponent in ASCII table output, found: {!r}\".format(raw))\n+        assert b'E' not in raw, (\n+            \"Found unexpected 'E' exponent in ASCII table output: {!r}\".format(raw))\n+\n+        # Also verify the round-trip numeric values are preserved\n+        with fits.open(self.temp('ascii_d_exponent.fits')) as hdul:\n+            data = hdul[1].data\n+        np.testing.assert_allclose(data['col1'],\n+                                   [1.23456789e-99, 9.87654321e+99])\n+\n     def test_endianness(self):\n         x = np.ndarray((1,), dtype=object)\n         channelsIn = np.array([3], dtype='uint8')\n"}
{"instance_id": "astropy__astropy-7746", "model_name_or_path": "harness/claude-sonnet-4-6", "model_patch": "diff --git a/.harness/checkpoints/bb08b0056d9c/astropy--wcs--wcs.py b/.harness/checkpoints/bb08b0056d9c/astropy--wcs--wcs.py\nnew file mode 100644\nindex 0000000000..d3a06f2b31\n--- /dev/null\n+++ b/.harness/checkpoints/bb08b0056d9c/astropy--wcs--wcs.py\n@@ -0,0 +1,3299 @@\n+# Licensed under a 3-clause BSD style license - see LICENSE.rst\n+\"\"\"\n+Under the hood, there are 3 separate classes that perform different\n+parts of the transformation:\n+\n+   - `~astropy.wcs.Wcsprm`: Is a direct wrapper of the core WCS\n+     functionality in `wcslib`_.  (This includes TPV and TPD\n+     polynomial distortion, but not SIP distortion).\n+\n+   - `~astropy.wcs.Sip`: Handles polynomial distortion as defined in the\n+     `SIP`_ convention.\n+\n+   - `~astropy.wcs.DistortionLookupTable`: Handles `distortion paper`_\n+     lookup tables.\n+\n+Additionally, the class `WCS` aggregates all of these transformations\n+together in a pipeline:\n+\n+   - Detector to image plane correction (by a pair of\n+     `~astropy.wcs.DistortionLookupTable` objects).\n+\n+   - `SIP`_ distortion correction (by an underlying `~astropy.wcs.Sip`\n+     object)\n+\n+   - `distortion paper`_ table-lookup correction (by a pair of\n+     `~astropy.wcs.DistortionLookupTable` objects).\n+\n+   - `wcslib`_ WCS transformation (by a `~astropy.wcs.Wcsprm` object)\n+\n+\"\"\"\n+\n+# STDLIB\n+import copy\n+import io\n+import itertools\n+import os\n+import re\n+import textwrap\n+import warnings\n+import builtins\n+\n+# THIRD-PARTY\n+import numpy as np\n+\n+# LOCAL\n+from .. import log\n+from ..io import fits\n+from . import _docutil as __\n+try:\n+    from . import _wcs\n+except ImportError:\n+    if not _ASTROPY_SETUP_:\n+        raise\n+    else:\n+        _wcs = None\n+\n+from ..utils.compat import possible_filename\n+from ..utils.exceptions import AstropyWarning, AstropyUserWarning, AstropyDeprecationWarning\n+\n+__all__ = ['FITSFixedWarning', 'WCS', 'find_all_wcs',\n+           'DistortionLookupTable', 'Sip', 'Tabprm', 'Wcsprm',\n+           'WCSBase', 'validate', 'WcsError', 'SingularMatrixError',\n+           'InconsistentAxisTypesError', 'InvalidTransformError',\n+           'InvalidCoordinateError', 'NoSolutionError',\n+           'InvalidSubimageSpecificationError', 'NoConvergence',\n+           'NonseparableSubimageCoordinateSystemError',\n+           'NoWcsKeywordsFoundError', 'InvalidTabularParametersError']\n+\n+\n+__doctest_skip__ = ['WCS.all_world2pix']\n+\n+\n+if _wcs is not None:\n+    _parsed_version = _wcs.__version__.split('.')\n+    if int(_parsed_version[0]) == 5 and int(_parsed_version[1]) < 8:\n+        raise ImportError(\n+            \"astropy.wcs is built with wcslib {0}, but only versions 5.8 and \"\n+            \"later on the 5.x series are known to work.  The version of wcslib \"\n+            \"that ships with astropy may be used.\")\n+\n+    if not _wcs._sanity_check():\n+        raise RuntimeError(\n+        \"astropy.wcs did not pass its sanity check for your build \"\n+        \"on your platform.\")\n+\n+    WCSBase = _wcs._Wcs\n+    DistortionLookupTable = _wcs.DistortionLookupTable\n+    Sip = _wcs.Sip\n+    Wcsprm = _wcs.Wcsprm\n+    Tabprm = _wcs.Tabprm\n+    WcsError = _wcs.WcsError\n+    SingularMatrixError = _wcs.SingularMatrixError\n+    InconsistentAxisTypesError = _wcs.InconsistentAxisTypesError\n+    InvalidTransformError = _wcs.InvalidTransformError\n+    InvalidCoordinateError = _wcs.InvalidCoordinateError\n+    NoSolutionError = _wcs.NoSolutionError\n+    InvalidSubimageSpecificationError = _wcs.InvalidSubimageSpecificationError\n+    NonseparableSubimageCoordinateSystemError = _wcs.NonseparableSubimageCoordinateSystemError\n+    NoWcsKeywordsFoundError = _wcs.NoWcsKeywordsFoundError\n+    InvalidTabularParametersError = _wcs.InvalidTabularParametersError\n+\n+    # Copy all the constants from the C extension into this module's namespace\n+    for key, val in _wcs.__dict__.items():\n+        if key.startswith(('WCSSUB', 'WCSHDR', 'WCSHDO')):\n+            locals()[key] = val\n+            __all__.append(key)\n+else:\n+    WCSBase = object\n+    Wcsprm = object\n+    DistortionLookupTable = object\n+    Sip = object\n+    Tabprm = object\n+    WcsError = None\n+    SingularMatrixError = None\n+    InconsistentAxisTypesError = None\n+    InvalidTransformError = None\n+    InvalidCoordinateError = None\n+    NoSolutionError = None\n+    InvalidSubimageSpecificationError = None\n+    NonseparableSubimageCoordinateSystemError = None\n+    NoWcsKeywordsFoundError = None\n+    InvalidTabularParametersError = None\n+\n+\n+# Additional relax bit flags\n+WCSHDO_SIP = 0x80000\n+\n+# Regular expression defining SIP keyword It matches keyword that starts with A\n+# or B, optionally followed by P, followed by an underscore then a number in\n+# range of 0-19, followed by an underscore and another number in range of 0-19.\n+# Keyword optionally ends with a capital letter.\n+SIP_KW = re.compile('''^[AB]P?_1?[0-9]_1?[0-9][A-Z]?$''')\n+\n+\n+def _parse_keysel(keysel):\n+    keysel_flags = 0\n+    if keysel is not None:\n+        for element in keysel:\n+            if element.lower() == 'image':\n+                keysel_flags |= _wcs.WCSHDR_IMGHEAD\n+            elif element.lower() == 'binary':\n+                keysel_flags |= _wcs.WCSHDR_BIMGARR\n+            elif element.lower() == 'pixel':\n+                keysel_flags |= _wcs.WCSHDR_PIXLIST\n+            else:\n+                raise ValueError(\n+                    \"keysel must be a list of 'image', 'binary' \" +\n+                    \"and/or 'pixel'\")\n+    else:\n+        keysel_flags = -1\n+\n+    return keysel_flags\n+\n+\n+class NoConvergence(Exception):\n+    \"\"\"\n+    An error class used to report non-convergence and/or divergence\n+    of numerical methods. It is used to report errors in the\n+    iterative solution used by\n+    the :py:meth:`~astropy.wcs.WCS.all_world2pix`.\n+\n+    Attributes\n+    ----------\n+\n+    best_solution : `numpy.ndarray`\n+        Best solution achieved by the numerical method.\n+\n+    accuracy : `numpy.ndarray`\n+        Accuracy of the ``best_solution``.\n+\n+    niter : `int`\n+        Number of iterations performed by the numerical method\n+        to compute ``best_solution``.\n+\n+    divergent : None, `numpy.ndarray`\n+        Indices of the points in ``best_solution`` array\n+        for which the solution appears to be divergent. If the\n+        solution does not diverge, ``divergent`` will be set to `None`.\n+\n+    slow_conv : None, `numpy.ndarray`\n+        Indices of the solutions in ``best_solution`` array\n+        for which the solution failed to converge within the\n+        specified maximum number of iterations. If there are no\n+        non-converging solutions (i.e., if the required accuracy\n+        has been achieved for all input data points)\n+        then ``slow_conv`` will be set to `None`.\n+\n+    \"\"\"\n+\n+    def __init__(self, *args, best_solution=None, accuracy=None, niter=None,\n+                 divergent=None, slow_conv=None, **kwargs):\n+        super().__init__(*args)\n+\n+        self.best_solution = best_solution\n+        self.accuracy = accuracy\n+        self.niter = niter\n+        self.divergent = divergent\n+        self.slow_conv = slow_conv\n+\n+        if kwargs:\n+            warnings.warn(\"Function received unexpected arguments ({}) these \"\n+                          \"are ignored but will raise an Exception in the \"\n+                          \"future.\".format(list(kwargs)),\n+                          AstropyDeprecationWarning)\n+\n+\n+class FITSFixedWarning(AstropyWarning):\n+    \"\"\"\n+    The warning raised when the contents of the FITS header have been\n+    modified to be standards compliant.\n+    \"\"\"\n+    pass\n+\n+\n+class WCS(WCSBase):\n+    \"\"\"WCS objects perform standard WCS transformations, and correct for\n+    `SIP`_ and `distortion paper`_ table-lookup transformations, based\n+    on the WCS keywords and supplementary data read from a FITS file.\n+\n+    Parameters\n+    ----------\n+    header : astropy.io.fits header object, Primary HDU, Image HDU, string, dict-like, or None, optional\n+        If *header* is not provided or None, the object will be\n+        initialized to default values.\n+\n+    fobj : An astropy.io.fits file (hdulist) object, optional\n+        It is needed when header keywords point to a `distortion\n+        paper`_ lookup table stored in a different extension.\n+\n+    key : str, optional\n+        The name of a particular WCS transform to use.  This may be\n+        either ``' '`` or ``'A'``-``'Z'`` and corresponds to the\n+        ``\\\"a\\\"`` part of the ``CTYPEia`` cards.  *key* may only be\n+        provided if *header* is also provided.\n+\n+    minerr : float, optional\n+        The minimum value a distortion correction must have in order\n+        to be applied. If the value of ``CQERRja`` is smaller than\n+        *minerr*, the corresponding distortion is not applied.\n+\n+    relax : bool or int, optional\n+        Degree of permissiveness:\n+\n+        - `True` (default): Admit all recognized informal extensions\n+          of the WCS standard.\n+\n+        - `False`: Recognize only FITS keywords defined by the\n+          published WCS standard.\n+\n+        - `int`: a bit field selecting specific extensions to accept.\n+          See :ref:`relaxread` for details.\n+\n+    naxis : int or sequence, optional\n+        Extracts specific coordinate axes using\n+        :meth:`~astropy.wcs.Wcsprm.sub`.  If a header is provided, and\n+        *naxis* is not ``None``, *naxis* will be passed to\n+        :meth:`~astropy.wcs.Wcsprm.sub` in order to select specific\n+        axes from the header.  See :meth:`~astropy.wcs.Wcsprm.sub` for\n+        more details about this parameter.\n+\n+    keysel : sequence of flags, optional\n+        A sequence of flags used to select the keyword types\n+        considered by wcslib.  When ``None``, only the standard image\n+        header keywords are considered (and the underlying wcspih() C\n+        function is called).  To use binary table image array or pixel\n+        list keywords, *keysel* must be set.\n+\n+        Each element in the list should be one of the following\n+        strings:\n+\n+        - 'image': Image header keywords\n+\n+        - 'binary': Binary table image array keywords\n+\n+        - 'pixel': Pixel list keywords\n+\n+        Keywords such as ``EQUIna`` or ``RFRQna`` that are common to\n+        binary table image arrays and pixel lists (including\n+        ``WCSNna`` and ``TWCSna``) are selected by both 'binary' and\n+        'pixel'.\n+\n+    colsel : sequence of int, optional\n+        A sequence of table column numbers used to restrict the WCS\n+        transformations considered to only those pertaining to the\n+        specified columns.  If `None`, there is no restriction.\n+\n+    fix : bool, optional\n+        When `True` (default), call `~astropy.wcs.Wcsprm.fix` on\n+        the resulting object to fix any non-standard uses in the\n+        header.  `FITSFixedWarning` Warnings will be emitted if any\n+        changes were made.\n+\n+    translate_units : str, optional\n+        Specify which potentially unsafe translations of non-standard\n+        unit strings to perform.  By default, performs none.  See\n+        `WCS.fix` for more information about this parameter.  Only\n+        effective when ``fix`` is `True`.\n+\n+    Raises\n+    ------\n+    MemoryError\n+         Memory allocation failed.\n+\n+    ValueError\n+         Invalid key.\n+\n+    KeyError\n+         Key not found in FITS header.\n+\n+    ValueError\n+         Lookup table distortion present in the header but *fobj* was\n+         not provided.\n+\n+    Notes\n+    -----\n+\n+    1. astropy.wcs supports arbitrary *n* dimensions for the core WCS\n+       (the transformations handled by WCSLIB).  However, the\n+       `distortion paper`_ lookup table and `SIP`_ distortions must be\n+       two dimensional.  Therefore, if you try to create a WCS object\n+       where the core WCS has a different number of dimensions than 2\n+       and that object also contains a `distortion paper`_ lookup\n+       table or `SIP`_ distortion, a `ValueError`\n+       exception will be raised.  To avoid this, consider using the\n+       *naxis* kwarg to select two dimensions from the core WCS.\n+\n+    2. The number of coordinate axes in the transformation is not\n+       determined directly from the ``NAXIS`` keyword but instead from\n+       the highest of:\n+\n+           - ``NAXIS`` keyword\n+\n+           - ``WCSAXESa`` keyword\n+\n+           - The highest axis number in any parameterized WCS keyword.\n+             The keyvalue, as well as the keyword, must be\n+             syntactically valid otherwise it will not be considered.\n+\n+       If none of these keyword types is present, i.e. if the header\n+       only contains auxiliary WCS keywords for a particular\n+       coordinate representation, then no coordinate description is\n+       constructed for it.\n+\n+       The number of axes, which is set as the ``naxis`` member, may\n+       differ for different coordinate representations of the same\n+       image.\n+\n+    3. When the header includes duplicate keywords, in most cases the\n+       last encountered is used.\n+\n+    4. `~astropy.wcs.Wcsprm.set` is called immediately after\n+       construction, so any invalid keywords or transformations will\n+       be raised by the constructor, not when subsequently calling a\n+       transformation method.\n+\n+    \"\"\"\n+\n+    def __init__(self, header=None, fobj=None, key=' ', minerr=0.0,\n+                 relax=True, naxis=None, keysel=None, colsel=None,\n+                 fix=True, translate_units='', _do_set=True):\n+        close_fds = []\n+\n+        if header is None:\n+            if naxis is None:\n+                naxis = 2\n+            wcsprm = _wcs.Wcsprm(header=None, key=key,\n+                                 relax=relax, naxis=naxis)\n+            self.naxis = wcsprm.naxis\n+            # Set some reasonable defaults.\n+            det2im = (None, None)\n+            cpdis = (None, None)\n+            sip = None\n+        else:\n+            keysel_flags = _parse_keysel(keysel)\n+\n+            if isinstance(header, (str, bytes)):\n+                try:\n+                    is_path = (possible_filename(header) and\n+                               os.path.exists(header))\n+                except (OSError, ValueError):\n+                    is_path = False\n+\n+                if is_path:\n+                    if fobj is not None:\n+                        raise ValueError(\n+                            \"Can not provide both a FITS filename to \"\n+                            \"argument 1 and a FITS file object to argument 2\")\n+                    fobj = fits.open(header)\n+                    close_fds.append(fobj)\n+                    header = fobj[0].header\n+            elif isinstance(header, fits.hdu.image._ImageBaseHDU):\n+                header = header.header\n+            elif not isinstance(header, fits.Header):\n+                try:\n+                    # Accept any dict-like object\n+                    orig_header = header\n+                    header = fits.Header()\n+                    for dict_key in orig_header.keys():\n+                        header[dict_key] = orig_header[dict_key]\n+                except TypeError:\n+                    raise TypeError(\n+                        \"header must be a string, an astropy.io.fits.Header \"\n+                        \"object, or a dict-like object\")\n+\n+            if isinstance(header, fits.Header):\n+                header_string = header.tostring().rstrip()\n+            else:\n+                header_string = header\n+\n+            # Importantly, header is a *copy* of the passed-in header\n+            # because we will be modifying it\n+            if isinstance(header_string, str):\n+                header_bytes = header_string.encode('ascii')\n+                header_string = header_string\n+            else:\n+                header_bytes = header_string\n+                header_string = header_string.decode('ascii')\n+\n+            try:\n+                tmp_header = fits.Header.fromstring(header_string)\n+                self._remove_sip_kw(tmp_header)\n+                tmp_header_bytes = tmp_header.tostring().rstrip()\n+                if isinstance(tmp_header_bytes, str):\n+                    tmp_header_bytes = tmp_header_bytes.encode('ascii')\n+                tmp_wcsprm = _wcs.Wcsprm(header=tmp_header_bytes, key=key,\n+                                         relax=relax, keysel=keysel_flags,\n+                                         colsel=colsel, warnings=False)\n+            except _wcs.NoWcsKeywordsFoundError:\n+                est_naxis = 0\n+            else:\n+                if naxis is not None:\n+                    try:\n+                        tmp_wcsprm.sub(naxis)\n+                    except ValueError:\n+                        pass\n+                    est_naxis = tmp_wcsprm.naxis\n+                else:\n+                    est_naxis = 2\n+\n+            header = fits.Header.fromstring(header_string)\n+\n+            if est_naxis == 0:\n+                est_naxis = 2\n+            self.naxis = est_naxis\n+\n+            det2im = self._read_det2im_kw(header, fobj, err=minerr)\n+            cpdis = self._read_distortion_kw(\n+                header, fobj, dist='CPDIS', err=minerr)\n+            sip = self._read_sip_kw(header, wcskey=key)\n+            self._remove_sip_kw(header)\n+\n+            header_string = header.tostring()\n+            header_string = header_string.replace('END' + ' ' * 77, '')\n+\n+            if isinstance(header_string, str):\n+                header_bytes = header_string.encode('ascii')\n+                header_string = header_string\n+            else:\n+                header_bytes = header_string\n+                header_string = header_string.decode('ascii')\n+\n+            try:\n+                wcsprm = _wcs.Wcsprm(header=header_bytes, key=key,\n+                                     relax=relax, keysel=keysel_flags,\n+                                     colsel=colsel)\n+            except _wcs.NoWcsKeywordsFoundError:\n+                # The header may have SIP or distortions, but no core\n+                # WCS.  That isn't an error -- we want a \"default\"\n+                # (identity) core Wcs transformation in that case.\n+                if colsel is None:\n+                    wcsprm = _wcs.Wcsprm(header=None, key=key,\n+                                         relax=relax, keysel=keysel_flags,\n+                                         colsel=colsel)\n+                else:\n+                    raise\n+\n+            if naxis is not None:\n+                wcsprm = wcsprm.sub(naxis)\n+            self.naxis = wcsprm.naxis\n+\n+            if (wcsprm.naxis != 2 and\n+                (det2im[0] or det2im[1] or cpdis[0] or cpdis[1] or sip)):\n+                raise ValueError(\n+                    \"\"\"\n+FITS WCS distortion paper lookup tables and SIP distortions only work\n+in 2 dimensions.  However, WCSLIB has detected {0} dimensions in the\n+core WCS keywords.  To use core WCS in conjunction with FITS WCS\n+distortion paper lookup tables or SIP distortion, you must select or\n+reduce these to 2 dimensions using the naxis kwarg.\n+\"\"\".format(wcsprm.naxis))\n+\n+            header_naxis = header.get('NAXIS', None)\n+            if header_naxis is not None and header_naxis < wcsprm.naxis:\n+                warnings.warn(\n+                    \"The WCS transformation has more axes ({0:d}) than the \"\n+                    \"image it is associated with ({1:d})\".format(\n+                        wcsprm.naxis, header_naxis), FITSFixedWarning)\n+\n+        self._get_naxis(header)\n+        WCSBase.__init__(self, sip, cpdis, wcsprm, det2im)\n+\n+        if fix:\n+            self.fix(translate_units=translate_units)\n+\n+        if _do_set:\n+            self.wcs.set()\n+\n+        for fd in close_fds:\n+            fd.close()\n+\n+    def __copy__(self):\n+        new_copy = self.__class__()\n+        WCSBase.__init__(new_copy, self.sip,\n+                         (self.cpdis1, self.cpdis2),\n+                         self.wcs,\n+                         (self.det2im1, self.det2im2))\n+        new_copy.__dict__.update(self.__dict__)\n+        return new_copy\n+\n+    def __deepcopy__(self, memo):\n+        from copy import deepcopy\n+\n+        new_copy = self.__class__()\n+        new_copy.naxis = deepcopy(self.naxis, memo)\n+        WCSBase.__init__(new_copy, deepcopy(self.sip, memo),\n+                         (deepcopy(self.cpdis1, memo),\n+                          deepcopy(self.cpdis2, memo)),\n+                         deepcopy(self.wcs, memo),\n+                         (deepcopy(self.det2im1, memo),\n+                          deepcopy(self.det2im2, memo)))\n+        for key, val in self.__dict__.items():\n+            new_copy.__dict__[key] = deepcopy(val, memo)\n+        return new_copy\n+\n+    def copy(self):\n+        \"\"\"\n+        Return a shallow copy of the object.\n+\n+        Convenience method so user doesn't have to import the\n+        :mod:`copy` stdlib module.\n+\n+        .. warning::\n+            Use `deepcopy` instead of `copy` unless you know why you need a\n+            shallow copy.\n+        \"\"\"\n+        return copy.copy(self)\n+\n+    def deepcopy(self):\n+        \"\"\"\n+        Return a deep copy of the object.\n+\n+        Convenience method so user doesn't have to import the\n+        :mod:`copy` stdlib module.\n+        \"\"\"\n+        return copy.deepcopy(self)\n+\n+    def sub(self, axes=None):\n+        copy = self.deepcopy()\n+        copy.wcs = self.wcs.sub(axes)\n+        copy.naxis = copy.wcs.naxis\n+        return copy\n+    if _wcs is not None:\n+        sub.__doc__ = _wcs.Wcsprm.sub.__doc__\n+\n+    def _fix_scamp(self):\n+        \"\"\"\n+        Remove SCAMP's PVi_m distortion parameters if SIP distortion parameters\n+        are also present. Some projects (e.g., Palomar Transient Factory)\n+        convert SCAMP's distortion parameters (which abuse the PVi_m cards) to\n+        SIP. However, wcslib gets confused by the presence of both SCAMP and\n+        SIP distortion parameters.\n+\n+        See https://github.com/astropy/astropy/issues/299.\n+        \"\"\"\n+        # Nothing to be done if no WCS attached\n+        if self.wcs is None:\n+            return\n+\n+        # Nothing to be done if no PV parameters attached\n+        pv = self.wcs.get_pv()\n+        if not pv:\n+            return\n+\n+        # Nothing to be done if axes don't use SIP distortion parameters\n+        if self.sip is None:\n+            return\n+\n+        # Nothing to be done if any radial terms are present...\n+        # Loop over list to find any radial terms.\n+        # Certain values of the `j' index are used for storing\n+        # radial terms; refer to Equation (1) in\n+        # <http://web.ipac.caltech.edu/staff/shupe/reprints/SIP_to_PV_SPIE2012.pdf>.\n+        pv = np.asarray(pv)\n+        # Loop over distinct values of `i' index\n+        for i in set(pv[:, 0]):\n+            # Get all values of `j' index for this value of `i' index\n+            js = set(pv[:, 1][pv[:, 0] == i])\n+            # Find max value of `j' index\n+            max_j = max(js)\n+            for j in (3, 11, 23, 39):\n+                if j < max_j and j in js:\n+                    return\n+\n+        self.wcs.set_pv([])\n+        warnings.warn(\"Removed redundant SCAMP distortion parameters \" +\n+            \"because SIP parameters are also present\", FITSFixedWarning)\n+\n+    def fix(self, translate_units='', naxis=None):\n+        \"\"\"\n+        Perform the fix operations from wcslib, and warn about any\n+        changes it has made.\n+\n+        Parameters\n+        ----------\n+        translate_units : str, optional\n+            Specify which potentially unsafe translations of\n+            non-standard unit strings to perform.  By default,\n+            performs none.\n+\n+            Although ``\"S\"`` is commonly used to represent seconds,\n+            its translation to ``\"s\"`` is potentially unsafe since the\n+            standard recognizes ``\"S\"`` formally as Siemens, however\n+            rarely that may be used.  The same applies to ``\"H\"`` for\n+            hours (Henry), and ``\"D\"`` for days (Debye).\n+\n+            This string controls what to do in such cases, and is\n+            case-insensitive.\n+\n+            - If the string contains ``\"s\"``, translate ``\"S\"`` to\n+              ``\"s\"``.\n+\n+            - If the string contains ``\"h\"``, translate ``\"H\"`` to\n+              ``\"h\"``.\n+\n+            - If the string contains ``\"d\"``, translate ``\"D\"`` to\n+              ``\"d\"``.\n+\n+            Thus ``''`` doesn't do any unsafe translations, whereas\n+            ``'shd'`` does all of them.\n+\n+        naxis : int array[naxis], optional\n+            Image axis lengths.  If this array is set to zero or\n+            ``None``, then `~astropy.wcs.Wcsprm.cylfix` will not be\n+            invoked.\n+        \"\"\"\n+        if self.wcs is not None:\n+            self._fix_scamp()\n+            fixes = self.wcs.fix(translate_units, naxis)\n+            for key, val in fixes.items():\n+                if val != \"No change\":\n+                    warnings.warn(\n+                        (\"'{0}' made the change '{1}'.\").\n+                        format(key, val),\n+                        FITSFixedWarning)\n+\n+    def calc_footprint(self, header=None, undistort=True, axes=None, center=True):\n+        \"\"\"\n+        Calculates the footprint of the image on the sky.\n+\n+        A footprint is defined as the positions of the corners of the\n+        image on the sky after all available distortions have been\n+        applied.\n+\n+        Parameters\n+        ----------\n+        header : `~astropy.io.fits.Header` object, optional\n+            Used to get ``NAXIS1`` and ``NAXIS2``\n+            header and axes are mutually exclusive, alternative ways\n+            to provide the same information.\n+\n+        undistort : bool, optional\n+            If `True`, take SIP and distortion lookup table into\n+            account\n+\n+        axes : length 2 sequence ints, optional\n+            If provided, use the given sequence as the shape of the\n+            image.  Otherwise, use the ``NAXIS1`` and ``NAXIS2``\n+            keywords from the header that was used to create this\n+            `WCS` object.\n+\n+        center : bool, optional\n+            If `True` use the center of the pixel, otherwise use the corner.\n+\n+        Returns\n+        -------\n+        coord : (4, 2) array of (*x*, *y*) coordinates.\n+            The order is clockwise starting with the bottom left corner.\n+        \"\"\"\n+        if axes is not None:\n+            naxis1, naxis2 = axes\n+        else:\n+            if header is None:\n+                try:\n+                    # classes that inherit from WCS and define naxis1/2\n+                    # do not require a header parameter\n+                    naxis1 = self._naxis1\n+                    naxis2 = self._naxis2\n+                except AttributeError:\n+                    warnings.warn(\"Need a valid header in order to calculate footprint\\n\", AstropyUserWarning)\n+                    return None\n+            else:\n+                naxis1 = header.get('NAXIS1', None)\n+                naxis2 = header.get('NAXIS2', None)\n+\n+        if naxis1 is None or naxis2 is None:\n+            raise ValueError(\n+                    \"Image size could not be determined.\")\n+\n+        if center:\n+            corners = np.array([[1, 1],\n+                                [1, naxis2],\n+                                [naxis1, naxis2],\n+                                [naxis1, 1]], dtype=np.float64)\n+        else:\n+            corners = np.array([[0.5, 0.5],\n+                                [0.5, naxis2 + 0.5],\n+                                [naxis1 + 0.5, naxis2 + 0.5],\n+                                [naxis1 + 0.5, 0.5]], dtype=np.float64)\n+\n+        if undistort:\n+            return self.all_pix2world(corners, 1)\n+        else:\n+            return self.wcs_pix2world(corners, 1)\n+\n+    def _read_det2im_kw(self, header, fobj, err=0.0):\n+        \"\"\"\n+        Create a `distortion paper`_ type lookup table for detector to\n+        image plane correction.\n+        \"\"\"\n+        if fobj is None:\n+            return (None, None)\n+\n+        if not isinstance(fobj, fits.HDUList):\n+            return (None, None)\n+\n+        try:\n+            axiscorr = header[str('AXISCORR')]\n+            d2imdis = self._read_d2im_old_format(header, fobj, axiscorr)\n+            return d2imdis\n+        except KeyError:\n+            pass\n+\n+        dist = 'D2IMDIS'\n+        d_kw = 'D2IM'\n+        err_kw = 'D2IMERR'\n+        tables = {}\n+        for i in range(1, self.naxis + 1):\n+            d_error = header.get(err_kw + str(i), 0.0)\n+            if d_error < err:\n+                tables[i] = None\n+                continue\n+            distortion = dist + str(i)\n+            if distortion in header:\n+                dis = header[distortion].lower()\n+                if dis == 'lookup':\n+                    del header[distortion]\n+                    assert isinstance(fobj, fits.HDUList), ('An astropy.io.fits.HDUList'\n+                                'is required for Lookup table distortion.')\n+                    dp = (d_kw + str(i)).strip()\n+                    dp_extver_key = dp + str('.EXTVER')\n+                    if dp_extver_key in header:\n+                        d_extver = header[dp_extver_key]\n+                        del header[dp_extver_key]\n+                    else:\n+                        d_extver = 1\n+                    dp_axis_key = dp + str('.AXIS.{0:d}').format(i)\n+                    if i == header[dp_axis_key]:\n+                        d_data = fobj[str('D2IMARR'), d_extver].data\n+                    else:\n+                        d_data = (fobj[str('D2IMARR'), d_extver].data).transpose()\n+                    del header[dp_axis_key]\n+                    d_header = fobj[str('D2IMARR'), d_extver].header\n+                    d_crpix = (d_header.get(str('CRPIX1'), 0.0), d_header.get(str('CRPIX2'), 0.0))\n+                    d_crval = (d_header.get(str('CRVAL1'), 0.0), d_header.get(str('CRVAL2'), 0.0))\n+                    d_cdelt = (d_header.get(str('CDELT1'), 1.0), d_header.get(str('CDELT2'), 1.0))\n+                    d_lookup = DistortionLookupTable(d_data, d_crpix,\n+                                                     d_crval, d_cdelt)\n+                    tables[i] = d_lookup\n+                else:\n+                    warnings.warn('Polynomial distortion is not implemented.\\n', AstropyUserWarning)\n+                for key in list(header):\n+                    if key.startswith(dp + str('.')):\n+                        del header[key]\n+            else:\n+                tables[i] = None\n+        if not tables:\n+            return (None, None)\n+        else:\n+            return (tables.get(1), tables.get(2))\n+\n+    def _read_d2im_old_format(self, header, fobj, axiscorr):\n+        warnings.warn(\"The use of ``AXISCORR`` for D2IM correction has been deprecated.\"\n+                      \"`~astropy.wcs` will read in files with ``AXISCORR`` but ``to_fits()`` will write \"\n+                      \"out files without it.\",\n+                      AstropyDeprecationWarning)\n+        cpdis = [None, None]\n+        crpix = [0., 0.]\n+        crval = [0., 0.]\n+        cdelt = [1., 1.]\n+        try:\n+            d2im_data = fobj[(str('D2IMARR'), 1)].data\n+        except KeyError:\n+            return (None, None)\n+        except AttributeError:\n+            return (None, None)\n+\n+        d2im_data = np.array([d2im_data])\n+        d2im_hdr = fobj[(str('D2IMARR'), 1)].header\n+        naxis = d2im_hdr[str('NAXIS')]\n+\n+        for i in range(1, naxis + 1):\n+            crpix[i - 1] = d2im_hdr.get(str('CRPIX') + str(i), 0.0)\n+            crval[i - 1] = d2im_hdr.get(str('CRVAL') + str(i), 0.0)\n+            cdelt[i - 1] = d2im_hdr.get(str('CDELT') + str(i), 1.0)\n+\n+        cpdis = DistortionLookupTable(d2im_data, crpix, crval, cdelt)\n+\n+        if axiscorr == 1:\n+            return (cpdis, None)\n+        elif axiscorr == 2:\n+            return (None, cpdis)\n+        else:\n+            warnings.warn(\"Expected AXISCORR to be 1 or 2\", AstropyUserWarning)\n+            return (None, None)\n+\n+    def _write_det2im(self, hdulist):\n+        \"\"\"\n+        Writes a `distortion paper`_ type lookup table to the given\n+        `astropy.io.fits.HDUList`.\n+        \"\"\"\n+\n+        if self.det2im1 is None and self.det2im2 is None:\n+            return\n+        dist = 'D2IMDIS'\n+        d_kw = 'D2IM'\n+        err_kw = 'D2IMERR'\n+\n+        def write_d2i(num, det2im):\n+            if det2im is None:\n+                return\n+            str('{0}{1:d}').format(dist, num),\n+            hdulist[0].header[str('{0}{1:d}').format(dist, num)] = (\n+                'LOOKUP', 'Detector to image correction type')\n+            hdulist[0].header[str('{0}{1:d}.EXTVER').format(d_kw, num)] = (\n+                num, 'Version number of WCSDVARR extension')\n+            hdulist[0].header[str('{0}{1:d}.NAXES').format(d_kw, num)] = (\n+                len(det2im.data.shape), 'Number of independent variables in d2im function')\n+            for i in range(det2im.data.ndim):\n+                hdulist[0].header[str('{0}{1:d}.AXIS.{2:d}').format(d_kw, num, i + 1)] = (\n+                    i + 1, 'Axis number of the jth independent variable in a d2im function')\n+\n+            image = fits.ImageHDU(det2im.data, name=str('D2IMARR'))\n+            header = image.header\n+\n+            header[str('CRPIX1')] = (det2im.crpix[0],\n+                                     'Coordinate system reference pixel')\n+            header[str('CRPIX2')] = (det2im.crpix[1],\n+                                     'Coordinate system reference pixel')\n+            header[str('CRVAL1')] = (det2im.crval[0],\n+                                     'Coordinate system value at reference pixel')\n+            header[str('CRVAL2')] = (det2im.crval[1],\n+                                     'Coordinate system value at reference pixel')\n+            header[str('CDELT1')] = (det2im.cdelt[0],\n+                                     'Coordinate increment along axis')\n+            header[str('CDELT2')] = (det2im.cdelt[1],\n+                                     'Coordinate increment along axis')\n+            image.ver = int(hdulist[0].header[str('{0}{1:d}.EXTVER').format(d_kw, num)])\n+            hdulist.append(image)\n+        write_d2i(1, self.det2im1)\n+        write_d2i(2, self.det2im2)\n+\n+    def _read_distortion_kw(self, header, fobj, dist='CPDIS', err=0.0):\n+        \"\"\"\n+        Reads `distortion paper`_ table-lookup keywords and data, and\n+        returns a 2-tuple of `~astropy.wcs.DistortionLookupTable`\n+        objects.\n+\n+        If no `distortion paper`_ keywords are found, ``(None, None)``\n+        is returned.\n+        \"\"\"\n+        if isinstance(header, (str, bytes)):\n+            return (None, None)\n+\n+        if dist == 'CPDIS':\n+            d_kw = str('DP')\n+            err_kw = str('CPERR')\n+        else:\n+            d_kw = str('DQ')\n+            err_kw = str('CQERR')\n+\n+        tables = {}\n+        for i in range(1, self.naxis + 1):\n+            d_error_key = err_kw + str(i)\n+            if d_error_key in header:\n+                d_error = header[d_error_key]\n+                del header[d_error_key]\n+            else:\n+                d_error = 0.0\n+            if d_error < err:\n+                tables[i] = None\n+                continue\n+            distortion = dist + str(i)\n+            if distortion in header:\n+                dis = header[distortion].lower()\n+                del header[distortion]\n+                if dis == 'lookup':\n+                    if not isinstance(fobj, fits.HDUList):\n+                        raise ValueError('an astropy.io.fits.HDUList is '\n+                                'required for Lookup table distortion.')\n+                    dp = (d_kw + str(i)).strip()\n+                    dp_extver_key = dp + str('.EXTVER')\n+                    if dp_extver_key in header:\n+                        d_extver = header[dp_extver_key]\n+                        del header[dp_extver_key]\n+                    else:\n+                        d_extver = 1\n+                    dp_axis_key = dp + str('.AXIS.{0:d}'.format(i))\n+                    if i == header[dp_axis_key]:\n+                        d_data = fobj[str('WCSDVARR'), d_extver].data\n+                    else:\n+                        d_data = (fobj[str('WCSDVARR'), d_extver].data).transpose()\n+                    del header[dp_axis_key]\n+                    d_header = fobj[str('WCSDVARR'), d_extver].header\n+                    d_crpix = (d_header.get(str('CRPIX1'), 0.0),\n+                               d_header.get(str('CRPIX2'), 0.0))\n+                    d_crval = (d_header.get(str('CRVAL1'), 0.0),\n+                               d_header.get(str('CRVAL2'), 0.0))\n+                    d_cdelt = (d_header.get(str('CDELT1'), 1.0),\n+                               d_header.get(str('CDELT2'), 1.0))\n+                    d_lookup = DistortionLookupTable(d_data, d_crpix, d_crval, d_cdelt)\n+                    tables[i] = d_lookup\n+\n+                    for key in list(header):\n+                        if key.startswith(dp + str('.')):\n+                            del header[key]\n+                else:\n+                    warnings.warn('Polynomial distortion is not implemented.\\n', AstropyUserWarning)\n+            else:\n+                tables[i] = None\n+\n+        if not tables:\n+            return (None, None)\n+        else:\n+            return (tables.get(1), tables.get(2))\n+\n+    def _write_distortion_kw(self, hdulist, dist='CPDIS'):\n+        \"\"\"\n+        Write out `distortion paper`_ keywords to the given\n+        `fits.HDUList`.\n+        \"\"\"\n+        if self.cpdis1 is None and self.cpdis2 is None:\n+            return\n+\n+        if dist == 'CPDIS':\n+            d_kw = str('DP')\n+            err_kw = str('CPERR')\n+        else:\n+            d_kw = str('DQ')\n+            err_kw = str('CQERR')\n+\n+        def write_dist(num, cpdis):\n+            if cpdis is None:\n+                return\n+\n+            hdulist[0].header[str('{0}{1:d}').format(dist, num)] = (\n+                'LOOKUP', 'Prior distortion function type')\n+            hdulist[0].header[str('{0}{1:d}.EXTVER').format(d_kw, num)] = (\n+                num, 'Version number of WCSDVARR extension')\n+            hdulist[0].header[str('{0}{1:d}.NAXES').format(d_kw, num)] = (\n+                len(cpdis.data.shape), 'Number of independent variables in distortion function')\n+\n+            for i in range(cpdis.data.ndim):\n+                hdulist[0].header[str('{0}{1:d}.AXIS.{2:d}').format(d_kw, num, i + 1)] = (\n+                    i + 1,\n+                    'Axis number of the jth independent variable in a distortion function')\n+\n+            image = fits.ImageHDU(cpdis.data, name=str('WCSDVARR'))\n+            header = image.header\n+\n+            header[str('CRPIX1')] = (cpdis.crpix[0], 'Coordinate system reference pixel')\n+            header[str('CRPIX2')] = (cpdis.crpix[1], 'Coordinate system reference pixel')\n+            header[str('CRVAL1')] = (cpdis.crval[0], 'Coordinate system value at reference pixel')\n+            header[str('CRVAL2')] = (cpdis.crval[1], 'Coordinate system value at reference pixel')\n+            header[str('CDELT1')] = (cpdis.cdelt[0], 'Coordinate increment along axis')\n+            header[str('CDELT2')] = (cpdis.cdelt[1], 'Coordinate increment along axis')\n+            image.ver = int(hdulist[0].header[str('{0}{1:d}.EXTVER').format(d_kw, num)])\n+            hdulist.append(image)\n+\n+        write_dist(1, self.cpdis1)\n+        write_dist(2, self.cpdis2)\n+\n+    def _remove_sip_kw(self, header):\n+        \"\"\"\n+        Remove SIP information from a header.\n+        \"\"\"\n+        # Never pass SIP coefficients to wcslib\n+        # CTYPE must be passed with -SIP to wcslib\n+        for key in (m.group() for m in map(SIP_KW.match, list(header))\n+                    if m is not None):\n+            del header[key]\n+\n+    def _read_sip_kw(self, header, wcskey=\"\"):\n+        \"\"\"\n+        Reads `SIP`_ header keywords and returns a `~astropy.wcs.Sip`\n+        object.\n+\n+        If no `SIP`_ header keywords are found, ``None`` is returned.\n+        \"\"\"\n+        if isinstance(header, (str, bytes)):\n+            # TODO: Parse SIP from a string without pyfits around\n+            return None\n+\n+        if str(\"A_ORDER\") in header and header[str('A_ORDER')] > 1:\n+            if str(\"B_ORDER\") not in header:\n+                raise ValueError(\n+                    \"A_ORDER provided without corresponding B_ORDER \"\n+                    \"keyword for SIP distortion\")\n+\n+            m = int(header[str(\"A_ORDER\")])\n+            a = np.zeros((m + 1, m + 1), np.double)\n+            for i in range(m + 1):\n+                for j in range(m - i + 1):\n+                    key = str(\"A_{0}_{1}\").format(i, j)\n+                    if key in header:\n+                        a[i, j] = header[key]\n+                        del header[key]\n+\n+            m = int(header[str(\"B_ORDER\")])\n+            if m > 1:\n+                b = np.zeros((m + 1, m + 1), np.double)\n+                for i in range(m + 1):\n+                    for j in range(m - i + 1):\n+                        key = str(\"B_{0}_{1}\").format(i, j)\n+                        if key in header:\n+                            b[i, j] = header[key]\n+                            del header[key]\n+            else:\n+                a = None\n+                b = None\n+\n+            del header[str('A_ORDER')]\n+            del header[str('B_ORDER')]\n+\n+            ctype = [header['CTYPE{0}{1}'.format(nax, wcskey)] for nax in range(1, self.naxis + 1)]\n+            if any(not ctyp.endswith('-SIP') for ctyp in ctype):\n+                message = \"\"\"\n+                Inconsistent SIP distortion information is present in the FITS header and the WCS object:\n+                SIP coefficients were detected, but CTYPE is missing a \"-SIP\" suffix.\n+                astropy.wcs is using the SIP distortion coefficients,\n+                therefore the coordinates calculated here might be incorrect.\n+\n+                If you do not want to apply the SIP distortion coefficients,\n+                please remove the SIP coefficients from the FITS header or the\n+                WCS object.  As an example, if the image is already distortion-corrected\n+                (e.g., drizzled) then distortion components should not apply and the SIP\n+                coefficients should be removed.\n+\n+                While the SIP distortion coefficients are being applied here, if that was indeed the intent,\n+                for consistency please append \"-SIP\" to the CTYPE in the FITS header or the WCS object.\n+\n+                \"\"\"\n+                log.info(message)\n+        elif str(\"B_ORDER\") in header and header[str('B_ORDER')] > 1:\n+            raise ValueError(\n+                \"B_ORDER provided without corresponding A_ORDER \" +\n+                \"keyword for SIP distortion\")\n+        else:\n+            a = None\n+            b = None\n+\n+        if str(\"AP_ORDER\") in header and header[str('AP_ORDER')] > 1:\n+            if str(\"BP_ORDER\") not in header:\n+                raise ValueError(\n+                    \"AP_ORDER provided without corresponding BP_ORDER \"\n+                    \"keyword for SIP distortion\")\n+\n+            m = int(header[str(\"AP_ORDER\")])\n+            ap = np.zeros((m + 1, m + 1), np.double)\n+            for i in range(m + 1):\n+                for j in range(m - i + 1):\n+                    key = str(\"AP_{0}_{1}\").format(i, j)\n+                    if key in header:\n+                        ap[i, j] = header[key]\n+                        del header[key]\n+\n+            m = int(header[str(\"BP_ORDER\")])\n+            if m > 1:\n+                bp = np.zeros((m + 1, m + 1), np.double)\n+                for i in range(m + 1):\n+                    for j in range(m - i + 1):\n+                        key = str(\"BP_{0}_{1}\").format(i, j)\n+                        if key in header:\n+                            bp[i, j] = header[key]\n+                            del header[key]\n+            else:\n+                ap = None\n+                bp = None\n+\n+            del header[str('AP_ORDER')]\n+            del header[str('BP_ORDER')]\n+        elif str(\"BP_ORDER\") in header and header[str('BP_ORDER')] > 1:\n+            raise ValueError(\n+                \"BP_ORDER provided without corresponding AP_ORDER \"\n+                \"keyword for SIP distortion\")\n+        else:\n+            ap = None\n+            bp = None\n+\n+        if a is None and b is None and ap is None and bp is None:\n+            return None\n+\n+        if str(\"CRPIX1{0}\".format(wcskey)) not in header or str(\"CRPIX2{0}\".format(wcskey)) not in header:\n+            raise ValueError(\n+                \"Header has SIP keywords without CRPIX keywords\")\n+\n+        crpix1 = header.get(\"CRPIX1{0}\".format(wcskey))\n+        crpix2 = header.get(\"CRPIX2{0}\".format(wcskey))\n+\n+        return Sip(a, b, ap, bp, (crpix1, crpix2))\n+\n+    def _write_sip_kw(self):\n+        \"\"\"\n+        Write out SIP keywords.  Returns a dictionary of key-value\n+        pairs.\n+        \"\"\"\n+        if self.sip is None:\n+            return {}\n+\n+        keywords = {}\n+\n+        def write_array(name, a):\n+            if a is None:\n+                return\n+            size = a.shape[0]\n+            keywords[str('{0}_ORDER').format(name)] = size - 1\n+            for i in range(size):\n+                for j in range(size - i):\n+                    if a[i, j] != 0.0:\n+                        keywords[\n+                            str('{0}_{1:d}_{2:d}').format(name, i, j)] = a[i, j]\n+\n+        write_array(str('A'), self.sip.a)\n+        write_array(str('B'), self.sip.b)\n+        write_array(str('AP'), self.sip.ap)\n+        write_array(str('BP'), self.sip.bp)\n+\n+        return keywords\n+\n+    def _denormalize_sky(self, sky):\n+        if self.wcs.lngtyp != 'RA':\n+            raise ValueError(\n+                \"WCS does not have longitude type of 'RA', therefore \" +\n+                \"(ra, dec) data can not be used as input\")\n+        if self.wcs.lattyp != 'DEC':\n+            raise ValueError(\n+                \"WCS does not have longitude type of 'DEC', therefore \" +\n+                \"(ra, dec) data can not be used as input\")\n+        if self.wcs.naxis == 2:\n+            if self.wcs.lng == 0 and self.wcs.lat == 1:\n+                return sky\n+            elif self.wcs.lng == 1 and self.wcs.lat == 0:\n+                # Reverse the order of the columns\n+                return sky[:, ::-1]\n+            else:\n+                raise ValueError(\n+                    \"WCS does not have longitude and latitude celestial \" +\n+                    \"axes, therefore (ra, dec) data can not be used as input\")\n+        else:\n+            if self.wcs.lng < 0 or self.wcs.lat < 0:\n+                raise ValueError(\n+                    \"WCS does not have both longitude and latitude \"\n+                    \"celestial axes, therefore (ra, dec) data can not be \" +\n+                    \"used as input\")\n+            out = np.zeros((sky.shape[0], self.wcs.naxis))\n+            out[:, self.wcs.lng] = sky[:, 0]\n+            out[:, self.wcs.lat] = sky[:, 1]\n+            return out\n+\n+    def _normalize_sky(self, sky):\n+        if self.wcs.lngtyp != 'RA':\n+            raise ValueError(\n+                \"WCS does not have longitude type of 'RA', therefore \" +\n+                \"(ra, dec) data can not be returned\")\n+        if self.wcs.lattyp != 'DEC':\n+            raise ValueError(\n+                \"WCS does not have longitude type of 'DEC', therefore \" +\n+                \"(ra, dec) data can not be returned\")\n+        if self.wcs.naxis == 2:\n+            if self.wcs.lng == 0 and self.wcs.lat == 1:\n+                return sky\n+            elif self.wcs.lng == 1 and self.wcs.lat == 0:\n+                # Reverse the order of the columns\n+                return sky[:, ::-1]\n+            else:\n+                raise ValueError(\n+                    \"WCS does not have longitude and latitude celestial \"\n+                    \"axes, therefore (ra, dec) data can not be returned\")\n+        else:\n+            if self.wcs.lng < 0 or self.wcs.lat < 0:\n+                raise ValueError(\n+                    \"WCS does not have both longitude and latitude celestial \"\n+                    \"axes, therefore (ra, dec) data can not be returned\")\n+            out = np.empty((sky.shape[0], 2))\n+            out[:, 0] = sky[:, self.wcs.lng]\n+            out[:, 1] = sky[:, self.wcs.lat]\n+            return out\n+\n+    def _array_converter(self, func, sky, *args, ra_dec_order=False):\n+        \"\"\"\n+        A helper function to support reading either a pair of arrays\n+        or a single Nx2 array.\n+        \"\"\"\n+\n+        def _return_list_of_arrays(axes, origin):\n+            try:\n+                axes = np.broadcast_arrays(*axes)\n+            except ValueError:\n+                raise ValueError(\n+                    \"Coordinate arrays are not broadcastable to each other\")\n+\n+            xy = np.hstack([x.reshape((x.size, 1)) for x in axes])\n+\n+            if ra_dec_order and sky == 'input':\n+                xy = self._denormalize_sky(xy)\n+            output = func(xy, origin)\n+            if ra_dec_order and sky == 'output':\n+                output = self._normalize_sky(output)\n+                return (output[:, 0].reshape(axes[0].shape),\n+                        output[:, 1].reshape(axes[0].shape))\n+            return [output[:, i].reshape(axes[0].shape)\n+                    for i in range(output.shape[1])]\n+\n+        def _return_single_array(xy, origin):\n+            if xy.shape[-1] != self.naxis:\n+                raise ValueError(\n+                    \"When providing two arguments, the array must be \"\n+                    \"of shape (N, {0})\".format(self.naxis))\n+            if ra_dec_order and sky == 'input':\n+                xy = self._denormalize_sky(xy)\n+            result = func(xy, origin)\n+            if ra_dec_order and sky == 'output':\n+                result = self._normalize_sky(result)\n+            return result\n+\n+        if len(args) == 2:\n+            try:\n+                xy, origin = args\n+                xy = np.asarray(xy)\n+                origin = int(origin)\n+            except Exception:\n+                raise TypeError(\n+                    \"When providing two arguments, they must be \"\n+                    \"(coords[N][{0}], origin)\".format(self.naxis))\n+            if self.naxis == 1 and len(xy.shape) == 1:\n+                return _return_list_of_arrays([xy], origin)\n+            return _return_single_array(xy, origin)\n+\n+        elif len(args) == self.naxis + 1:\n+            axes = args[:-1]\n+            origin = args[-1]\n+            try:\n+                axes = [np.asarray(x) for x in axes]\n+                origin = int(origin)\n+            except Exception:\n+                raise TypeError(\n+                    \"When providing more than two arguments, they must be \" +\n+                    \"a 1-D array for each axis, followed by an origin.\")\n+\n+            return _return_list_of_arrays(axes, origin)\n+\n+        raise TypeError(\n+            \"WCS projection has {0} dimensions, so expected 2 (an Nx{0} array \"\n+            \"and the origin argument) or {1} arguments (the position in each \"\n+            \"dimension, and the origin argument). Instead, {2} arguments were \"\n+            \"given.\".format(\n+                self.naxis, self.naxis + 1, len(args)))\n+\n+    def all_pix2world(self, *args, **kwargs):\n+        return self._array_converter(\n+            self._all_pix2world, 'output', *args, **kwargs)\n+    all_pix2world.__doc__ = \"\"\"\n+        Transforms pixel coordinates to world coordinates.\n+\n+        Performs all of the following in series:\n+\n+            - Detector to image plane correction (if present in the\n+              FITS file)\n+\n+            - `SIP`_ distortion correction (if present in the FITS\n+              file)\n+\n+            - `distortion paper`_ table-lookup correction (if present\n+              in the FITS file)\n+\n+            - `wcslib`_ \"core\" WCS transformation\n+\n+        Parameters\n+        ----------\n+        {0}\n+\n+            For a transformation that is not two-dimensional, the\n+            two-argument form must be used.\n+\n+        {1}\n+\n+        Returns\n+        -------\n+\n+        {2}\n+\n+        Notes\n+        -----\n+        The order of the axes for the result is determined by the\n+        ``CTYPEia`` keywords in the FITS header, therefore it may not\n+        always be of the form (*ra*, *dec*).  The\n+        `~astropy.wcs.Wcsprm.lat`, `~astropy.wcs.Wcsprm.lng`,\n+        `~astropy.wcs.Wcsprm.lattyp` and `~astropy.wcs.Wcsprm.lngtyp`\n+        members can be used to determine the order of the axes.\n+\n+        Raises\n+        ------\n+        MemoryError\n+            Memory allocation failed.\n+\n+        SingularMatrixError\n+            Linear transformation matrix is singular.\n+\n+        InconsistentAxisTypesError\n+            Inconsistent or unrecognized coordinate axis types.\n+\n+        ValueError\n+            Invalid parameter value.\n+\n+        ValueError\n+            Invalid coordinate transformation parameters.\n+\n+        ValueError\n+            x- and y-coordinate arrays are not the same size.\n+\n+        InvalidTransformError\n+            Invalid coordinate transformation parameters.\n+\n+        InvalidTransformError\n+            Ill-conditioned coordinate transformation parameters.\n+        \"\"\".format(__.TWO_OR_MORE_ARGS('naxis', 8),\n+                   __.RA_DEC_ORDER(8),\n+                   __.RETURNS('sky coordinates, in degrees', 8))\n+\n+    def wcs_pix2world(self, *args, **kwargs):\n+        if self.wcs is None:\n+            raise ValueError(\"No basic WCS settings were created.\")\n+        return self._array_converter(\n+            lambda xy, o: self.wcs.p2s(xy, o)['world'],\n+            'output', *args, **kwargs)\n+    wcs_pix2world.__doc__ = \"\"\"\n+        Transforms pixel coordinates to world coordinates by doing\n+        only the basic `wcslib`_ transformation.\n+\n+        No `SIP`_ or `distortion paper`_ table lookup correction is\n+        applied.  To perform distortion correction, see\n+        `~astropy.wcs.WCS.all_pix2world`,\n+        `~astropy.wcs.WCS.sip_pix2foc`, `~astropy.wcs.WCS.p4_pix2foc`,\n+        or `~astropy.wcs.WCS.pix2foc`.\n+\n+        Parameters\n+        ----------\n+        {0}\n+\n+            For a transformation that is not two-dimensional, the\n+            two-argument form must be used.\n+\n+        {1}\n+\n+        Returns\n+        -------\n+\n+        {2}\n+\n+        Raises\n+        ------\n+        MemoryError\n+            Memory allocation failed.\n+\n+        SingularMatrixError\n+            Linear transformation matrix is singular.\n+\n+        InconsistentAxisTypesError\n+            Inconsistent or unrecognized coordinate axis types.\n+\n+        ValueError\n+            Invalid parameter value.\n+\n+        ValueError\n+            Invalid coordinate transformation parameters.\n+\n+        ValueError\n+            x- and y-coordinate arrays are not the same size.\n+\n+        InvalidTransformError\n+            Invalid coordinate transformation parameters.\n+\n+        InvalidTransformError\n+            Ill-conditioned coordinate transformation parameters.\n+\n+        Notes\n+        -----\n+        The order of the axes for the result is determined by the\n+        ``CTYPEia`` keywords in the FITS header, therefore it may not\n+        always be of the form (*ra*, *dec*).  The\n+        `~astropy.wcs.Wcsprm.lat`, `~astropy.wcs.Wcsprm.lng`,\n+        `~astropy.wcs.Wcsprm.lattyp` and `~astropy.wcs.Wcsprm.lngtyp`\n+        members can be used to determine the order of the axes.\n+\n+        \"\"\".format(__.TWO_OR_MORE_ARGS('naxis', 8),\n+                   __.RA_DEC_ORDER(8),\n+                   __.RETURNS('world coordinates, in degrees', 8))\n+\n+    def _all_world2pix(self, world, origin, tolerance, maxiter, adaptive,\n+                       detect_divergence, quiet):\n+        # ############################################################\n+        # #          DESCRIPTION OF THE NUMERICAL METHOD            ##\n+        # ############################################################\n+        # In this section I will outline the method of solving\n+        # the inverse problem of converting world coordinates to\n+        # pixel coordinates (*inverse* of the direct transformation\n+        # `all_pix2world`) and I will summarize some of the aspects\n+        # of the method proposed here and some of the issues of the\n+        # original `all_world2pix` (in relation to this method)\n+        # discussed in https://github.com/astropy/astropy/issues/1977\n+        # A more detailed discussion can be found here:\n+        # https://github.com/astropy/astropy/pull/2373\n+        #\n+        #\n+        #                  ### Background ###\n+        #\n+        #\n+        # I will refer here to the [SIP Paper]\n+        # (http://fits.gsfc.nasa.gov/registry/sip/SIP_distortion_v1_0.pdf).\n+        # According to this paper, the effect of distortions as\n+        # described in *their* equation (1) is:\n+        #\n+        # (1)   x = CD*(u+f(u)),\n+        #\n+        # where `x` is a *vector* of \"intermediate spherical\n+        # coordinates\" (equivalent to (x,y) in the paper) and `u`\n+        # is a *vector* of \"pixel coordinates\", and `f` is a vector\n+        # function describing geometrical distortions\n+        # (see equations 2 and 3 in SIP Paper.\n+        # However, I prefer to use `w` for \"intermediate world\n+        # coordinates\", `x` for pixel coordinates, and assume that\n+        # transformation `W` performs the **linear**\n+        # (CD matrix + projection onto celestial sphere) part of the\n+        # conversion from pixel coordinates to world coordinates.\n+        # Then we can re-write (1) as:\n+        #\n+        # (2)   w = W*(x+f(x)) = T(x)\n+        #\n+        # In `astropy.wcs.WCS` transformation `W` is represented by\n+        # the `wcs_pix2world` member, while the combined (\"total\")\n+        # transformation (linear part + distortions) is performed by\n+        # `all_pix2world`. Below I summarize the notations and their\n+        # equivalents in `astropy.wcs.WCS`:\n+        #\n+        # | Equation term | astropy.WCS/meaning          |\n+        # | ------------- | ---------------------------- |\n+        # | `x`           | pixel coordinates            |\n+        # | `w`           | world coordinates            |\n+        # | `W`           | `wcs_pix2world()`            |\n+        # | `W^{-1}`      | `wcs_world2pix()`            |\n+        # | `T`           | `all_pix2world()`            |\n+        # | `x+f(x)`      | `pix2foc()`                  |\n+        #\n+        #\n+        #      ### Direct Solving of Equation (2)  ###\n+        #\n+        #\n+        # In order to find the pixel coordinates that correspond to\n+        # given world coordinates `w`, it is necessary to invert\n+        # equation (2): `x=T^{-1}(w)`, or solve equation `w==T(x)`\n+        # for `x`. However, this approach has the following\n+        # disadvantages:\n+        #    1. It requires unnecessary transformations (see next\n+        #       section).\n+        #    2. It is prone to \"RA wrapping\" issues as described in\n+        # https://github.com/astropy/astropy/issues/1977\n+        # (essentially because `all_pix2world` may return points with\n+        # a different phase than user's input `w`).\n+        #\n+        #\n+        #      ### Description of the Method Used here ###\n+        #\n+        #\n+        # By applying inverse linear WCS transformation (`W^{-1}`)\n+        # to both sides of equation (2) and introducing notation `x'`\n+        # (prime) for the pixels coordinates obtained from the world\n+        # coordinates by applying inverse *linear* WCS transformation\n+        # (\"focal plane coordinates\"):\n+        #\n+        # (3)   x' = W^{-1}(w)\n+        #\n+        # we obtain the following equation:\n+        #\n+        # (4)   x' = x+f(x),\n+        #\n+        # or,\n+        #\n+        # (5)   x = x'-f(x)\n+        #\n+        # This equation is well suited for solving using the method\n+        # of fixed-point iterations\n+        # (http://en.wikipedia.org/wiki/Fixed-point_iteration):\n+        #\n+        # (6)   x_{i+1} = x'-f(x_i)\n+        #\n+        # As an initial value of the pixel coordinate `x_0` we take\n+        # \"focal plane coordinate\" `x'=W^{-1}(w)=wcs_world2pix(w)`.\n+        # We stop iterations when `|x_{i+1}-x_i|<tolerance`. We also\n+        # consider the process to be diverging if\n+        # `|x_{i+1}-x_i|>|x_i-x_{i-1}|`\n+        # **when** `|x_{i+1}-x_i|>=tolerance` (when current\n+        # approximation is close to the true solution,\n+        # `|x_{i+1}-x_i|>|x_i-x_{i-1}|` may be due to rounding errors\n+        # and we ignore such \"divergences\" when\n+        # `|x_{i+1}-x_i|<tolerance`). It may appear that checking for\n+        # `|x_{i+1}-x_i|<tolerance` in order to ignore divergence is\n+        # unnecessary since the iterative process should stop anyway,\n+        # however, the proposed implementation of this iterative\n+        # process is completely vectorized and, therefore, we may\n+        # continue iterating over *some* points even though they have\n+        # converged to within a specified tolerance (while iterating\n+        # over other points that have not yet converged to\n+        # a solution).\n+        #\n+        # In order to efficiently implement iterative process (6)\n+        # using available methods in `astropy.wcs.WCS`, we add and\n+        # subtract `x_i` from the right side of equation (6):\n+        #\n+        # (7)   x_{i+1} = x'-(x_i+f(x_i))+x_i = x'-pix2foc(x_i)+x_i,\n+        #\n+        # where `x'=wcs_world2pix(w)` and it is computed only *once*\n+        # before the beginning of the iterative process (and we also\n+        # set `x_0=x'`). By using `pix2foc` at each iteration instead\n+        # of `all_pix2world` we get about 25% increase in performance\n+        # (by not performing the linear `W` transformation at each\n+        # step) and we also avoid the \"RA wrapping\" issue described\n+        # above (by working in focal plane coordinates and avoiding\n+        # pix->world transformations).\n+        #\n+        # As an added benefit, the process converges to the correct\n+        # solution in just one iteration when distortions are not\n+        # present (compare to\n+        # https://github.com/astropy/astropy/issues/1977 and\n+        # https://github.com/astropy/astropy/pull/2294): in this case\n+        # `pix2foc` is the identical transformation\n+        # `x_i=pix2foc(x_i)` and from equation (7) we get:\n+        #\n+        # x' = x_0 = wcs_world2pix(w)\n+        # x_1 = x' - pix2foc(x_0) + x_0 = x' - pix2foc(x') + x' = x'\n+        #     = wcs_world2pix(w) = x_0\n+        # =>\n+        # |x_1-x_0| = 0 < tolerance (with tolerance > 0)\n+        #\n+        # However, for performance reasons, it is still better to\n+        # avoid iterations altogether and return the exact linear\n+        # solution (`wcs_world2pix`) right-away when non-linear\n+        # distortions are not present by checking that attributes\n+        # `sip`, `cpdis1`, `cpdis2`, `det2im1`, and `det2im2` are\n+        # *all* `None`.\n+        #\n+        #\n+        #         ### Outline of the Algorithm ###\n+        #\n+        #\n+        # While the proposed code is relatively long (considering\n+        # the simplicity of the algorithm), this is due to: 1)\n+        # checking if iterative solution is necessary at all; 2)\n+        # checking for divergence; 3) re-implementation of the\n+        # completely vectorized algorithm as an \"adaptive\" vectorized\n+        # algorithm (for cases when some points diverge for which we\n+        # want to stop iterations). In my tests, the adaptive version\n+        # of the algorithm is about 50% slower than non-adaptive\n+        # version for all HST images.\n+        #\n+        # The essential part of the vectorized non-adaptive algorithm\n+        # (without divergence and other checks) can be described\n+        # as follows:\n+        #\n+        #     pix0 = self.wcs_world2pix(world, origin)\n+        #     pix  = pix0.copy() # 0-order solution\n+        #\n+        #     for k in range(maxiter):\n+        #         # find correction to the previous solution:\n+        #         dpix = self.pix2foc(pix, origin) - pix0\n+        #\n+        #         # compute norm (L2) of the correction:\n+        #         dn = np.linalg.norm(dpix, axis=1)\n+        #\n+        #         # apply correction:\n+        #         pix -= dpix\n+        #\n+        #         # check convergence:\n+        #         if np.max(dn) < tolerance:\n+        #             break\n+        #\n+        #    return pix\n+        #\n+        # Here, the input parameter `world` can be a `MxN` array\n+        # where `M` is the number of coordinate axes in WCS and `N`\n+        # is the number of points to be converted simultaneously to\n+        # image coordinates.\n+        #\n+        #\n+        #                ###  IMPORTANT NOTE:  ###\n+        #\n+        # If, in the future releases of the `~astropy.wcs`,\n+        # `pix2foc` will not apply all the required distortion\n+        # corrections then in the code below, calls to `pix2foc` will\n+        # have to be replaced with\n+        # wcs_world2pix(all_pix2world(pix_list, origin), origin)\n+        #\n+\n+        # ############################################################\n+        # #            INITIALIZE ITERATIVE PROCESS:                ##\n+        # ############################################################\n+\n+        # initial approximation (linear WCS based only)\n+        pix0 = self.wcs_world2pix(world, origin)\n+\n+        # Check that an iterative solution is required at all\n+        # (when any of the non-CD-matrix-based corrections are\n+        # present). If not required return the initial\n+        # approximation (pix0).\n+        if self.sip is None and \\\n+           self.cpdis1 is None and self.cpdis2 is None and \\\n+           self.det2im1 is None and self.det2im2 is None:\n+            # No non-WCS corrections detected so\n+            # simply return initial approximation:\n+            return pix0\n+\n+        pix = pix0.copy()  # 0-order solution\n+\n+        # initial correction:\n+        dpix = self.pix2foc(pix, origin) - pix0\n+\n+        # Update initial solution:\n+        pix -= dpix\n+\n+        # Norm (L2) squared of the correction:\n+        dn = np.sum(dpix*dpix, axis=1)\n+        dnprev = dn.copy()  # if adaptive else dn\n+        tol2 = tolerance**2\n+\n+        # Prepare for iterative process\n+        k = 1\n+        ind = None\n+        inddiv = None\n+\n+        # Turn off numpy runtime warnings for 'invalid' and 'over':\n+        old_invalid = np.geterr()['invalid']\n+        old_over = np.geterr()['over']\n+        np.seterr(invalid='ignore', over='ignore')\n+\n+        # ############################################################\n+        # #                NON-ADAPTIVE ITERATIONS:                 ##\n+        # ############################################################\n+        if not adaptive:\n+            # Fixed-point iterations:\n+            while (np.nanmax(dn) >= tol2 and k < maxiter):\n+                # Find correction to the previous solution:\n+                dpix = self.pix2foc(pix, origin) - pix0\n+\n+                # Compute norm (L2) squared of the correction:\n+                dn = np.sum(dpix*dpix, axis=1)\n+\n+                # Check for divergence (we do this in two stages\n+                # to optimize performance for the most common\n+                # scenario when successive approximations converge):\n+                if detect_divergence:\n+                    divergent = (dn >= dnprev)\n+                    if np.any(divergent):\n+                        # Find solutions that have not yet converged:\n+                        slowconv = (dn >= tol2)\n+                        inddiv, = np.where(divergent & slowconv)\n+\n+                        if inddiv.shape[0] > 0:\n+                            # Update indices of elements that\n+                            # still need correction:\n+                            conv = (dn < dnprev)\n+                            iconv = np.where(conv)\n+\n+                            # Apply correction:\n+                            dpixgood = dpix[iconv]\n+                            pix[iconv] -= dpixgood\n+                            dpix[iconv] = dpixgood\n+\n+                            # For the next iteration choose\n+                            # non-divergent points that have not yet\n+                            # converged to the requested accuracy:\n+                            ind, = np.where(slowconv & conv)\n+                            pix0 = pix0[ind]\n+                            dnprev[ind] = dn[ind]\n+                            k += 1\n+\n+                            # Switch to adaptive iterations:\n+                            adaptive = True\n+                            break\n+                    # Save current correction magnitudes for later:\n+                    dnprev = dn\n+\n+                # Apply correction:\n+                pix -= dpix\n+                k += 1\n+\n+        # ############################################################\n+        # #                  ADAPTIVE ITERATIONS:                   ##\n+        # ############################################################\n+        if adaptive:\n+            if ind is None:\n+                ind, = np.where(np.isfinite(pix).all(axis=1))\n+                pix0 = pix0[ind]\n+\n+            # \"Adaptive\" fixed-point iterations:\n+            while (ind.shape[0] > 0 and k < maxiter):\n+                # Find correction to the previous solution:\n+                dpixnew = self.pix2foc(pix[ind], origin) - pix0\n+\n+                # Compute norm (L2) of the correction:\n+                dnnew = np.sum(np.square(dpixnew), axis=1)\n+\n+                # Bookeeping of corrections:\n+                dnprev[ind] = dn[ind].copy()\n+                dn[ind] = dnnew\n+\n+                if detect_divergence:\n+                    # Find indices of pixels that are converging:\n+                    conv = (dnnew < dnprev[ind])\n+                    iconv = np.where(conv)\n+                    iiconv = ind[iconv]\n+\n+                    # Apply correction:\n+                    dpixgood = dpixnew[iconv]\n+                    pix[iiconv] -= dpixgood\n+                    dpix[iiconv] = dpixgood\n+\n+                    # Find indices of solutions that have not yet\n+                    # converged to the requested accuracy\n+                    # AND that do not diverge:\n+                    subind, = np.where((dnnew >= tol2) & conv)\n+\n+                else:\n+                    # Apply correction:\n+                    pix[ind] -= dpixnew\n+                    dpix[ind] = dpixnew\n+\n+                    # Find indices of solutions that have not yet\n+                    # converged to the requested accuracy:\n+                    subind, = np.where(dnnew >= tol2)\n+\n+                # Choose solutions that need more iterations:\n+                ind = ind[subind]\n+                pix0 = pix0[subind]\n+\n+                k += 1\n+\n+        # ############################################################\n+        # #         FINAL DETECTION OF INVALID, DIVERGING,          ##\n+        # #         AND FAILED-TO-CONVERGE POINTS                   ##\n+        # ############################################################\n+        # Identify diverging and/or invalid points:\n+        invalid = ((~np.all(np.isfinite(pix), axis=1)) &\n+                   (np.all(np.isfinite(world), axis=1)))\n+\n+        # When detect_divergence==False, dnprev is outdated\n+        # (it is the norm of the very first correction).\n+        # Still better than nothing...\n+        inddiv, = np.where(((dn >= tol2) & (dn >= dnprev)) | invalid)\n+        if inddiv.shape[0] == 0:\n+            inddiv = None\n+\n+        # Identify points that did not converge within 'maxiter'\n+        # iterations:\n+        if k >= maxiter:\n+            ind, = np.where((dn >= tol2) & (dn < dnprev) & (~invalid))\n+            if ind.shape[0] == 0:\n+                ind = None\n+        else:\n+            ind = None\n+\n+        # Restore previous numpy error settings:\n+        np.seterr(invalid=old_invalid, over=old_over)\n+\n+        # ############################################################\n+        # #  RAISE EXCEPTION IF DIVERGING OR TOO SLOWLY CONVERGING  ##\n+        # #  DATA POINTS HAVE BEEN DETECTED:                        ##\n+        # ############################################################\n+        if (ind is not None or inddiv is not None) and not quiet:\n+            if inddiv is None:\n+                raise NoConvergence(\n+                    \"'WCS.all_world2pix' failed to \"\n+                    \"converge to the requested accuracy after {:d} \"\n+                    \"iterations.\".format(k), best_solution=pix,\n+                    accuracy=np.abs(dpix), niter=k,\n+                    slow_conv=ind, divergent=None)\n+            else:\n+                raise NoConvergence(\n+                    \"'WCS.all_world2pix' failed to \"\n+                    \"converge to the requested accuracy.\\n\"\n+                    \"After {0:d} iterations, the solution is diverging \"\n+                    \"at least for one input point.\"\n+                    .format(k), best_solution=pix,\n+                    accuracy=np.abs(dpix), niter=k,\n+                    slow_conv=ind, divergent=inddiv)\n+\n+        return pix\n+\n+    def all_world2pix(self, *args, tolerance=1e-4, maxiter=20, adaptive=False,\n+                      detect_divergence=True, quiet=False, **kwargs):\n+        if self.wcs is None:\n+            raise ValueError(\"No basic WCS settings were created.\")\n+\n+        return self._array_converter(\n+            lambda *args, **kwargs:\n+            self._all_world2pix(\n+                *args, tolerance=tolerance, maxiter=maxiter,\n+                adaptive=adaptive, detect_divergence=detect_divergence,\n+                quiet=quiet),\n+            'input', *args, **kwargs\n+        )\n+\n+    all_world2pix.__doc__ = \"\"\"\n+        all_world2pix(*arg, accuracy=1.0e-4, maxiter=20,\n+        adaptive=False, detect_divergence=True, quiet=False)\n+\n+        Transforms world coordinates to pixel coordinates, using\n+        numerical iteration to invert the full forward transformation\n+        `~astropy.wcs.WCS.all_pix2world` with complete\n+        distortion model.\n+\n+\n+        Parameters\n+        ----------\n+        {0}\n+\n+            For a transformation that is not two-dimensional, the\n+            two-argument form must be used.\n+\n+        {1}\n+\n+        tolerance : float, optional (Default = 1.0e-4)\n+            Tolerance of solution. Iteration terminates when the\n+            iterative solver estimates that the \"true solution\" is\n+            within this many pixels current estimate, more\n+            specifically, when the correction to the solution found\n+            during the previous iteration is smaller\n+            (in the sense of the L2 norm) than ``tolerance``.\n+\n+        maxiter : int, optional (Default = 20)\n+            Maximum number of iterations allowed to reach a solution.\n+\n+        quiet : bool, optional (Default = False)\n+            Do not throw :py:class:`NoConvergence` exceptions when\n+            the method does not converge to a solution with the\n+            required accuracy within a specified number of maximum\n+            iterations set by ``maxiter`` parameter. Instead,\n+            simply return the found solution.\n+\n+        Other Parameters\n+        ----------------\n+        adaptive : bool, optional (Default = False)\n+            Specifies whether to adaptively select only points that\n+            did not converge to a solution within the required\n+            accuracy for the next iteration. Default is recommended\n+            for HST as well as most other instruments.\n+\n+            .. note::\n+               The :py:meth:`all_world2pix` uses a vectorized\n+               implementation of the method of consecutive\n+               approximations (see ``Notes`` section below) in which it\n+               iterates over *all* input points *regardless* until\n+               the required accuracy has been reached for *all* input\n+               points. In some cases it may be possible that\n+               *almost all* points have reached the required accuracy\n+               but there are only a few of input data points for\n+               which additional iterations may be needed (this\n+               depends mostly on the characteristics of the geometric\n+               distortions for a given instrument). In this situation\n+               it may be advantageous to set ``adaptive`` = `True` in\n+               which case :py:meth:`all_world2pix` will continue\n+               iterating *only* over the points that have not yet\n+               converged to the required accuracy. However, for the\n+               HST's ACS/WFC detector, which has the strongest\n+               distortions of all HST instruments, testing has\n+               shown that enabling this option would lead to a about\n+               50-100% penalty in computational time (depending on\n+               specifics of the image, geometric distortions, and\n+               number of input points to be converted). Therefore,\n+               for HST and possibly instruments, it is recommended\n+               to set ``adaptive`` = `False`. The only danger in\n+               getting this setting wrong will be a performance\n+               penalty.\n+\n+            .. note::\n+               When ``detect_divergence`` is `True`,\n+               :py:meth:`all_world2pix` will automatically switch\n+               to the adaptive algorithm once divergence has been\n+               detected.\n+\n+        detect_divergence : bool, optional (Default = True)\n+            Specifies whether to perform a more detailed analysis\n+            of the convergence to a solution. Normally\n+            :py:meth:`all_world2pix` may not achieve the required\n+            accuracy if either the ``tolerance`` or ``maxiter`` arguments\n+            are too low. However, it may happen that for some\n+            geometric distortions the conditions of convergence for\n+            the the method of consecutive approximations used by\n+            :py:meth:`all_world2pix` may not be satisfied, in which\n+            case consecutive approximations to the solution will\n+            diverge regardless of the ``tolerance`` or ``maxiter``\n+            settings.\n+\n+            When ``detect_divergence`` is `False`, these divergent\n+            points will be detected as not having achieved the\n+            required accuracy (without further details). In addition,\n+            if ``adaptive`` is `False` then the algorithm will not\n+            know that the solution (for specific points) is diverging\n+            and will continue iterating and trying to \"improve\"\n+            diverging solutions. This may result in ``NaN`` or\n+            ``Inf`` values in the return results (in addition to a\n+            performance penalties). Even when ``detect_divergence``\n+            is `False`, :py:meth:`all_world2pix`, at the end of the\n+            iterative process, will identify invalid results\n+            (``NaN`` or ``Inf``) as \"diverging\" solutions and will\n+            raise :py:class:`NoConvergence` unless the ``quiet``\n+            parameter is set to `True`.\n+\n+            When ``detect_divergence`` is `True`,\n+            :py:meth:`all_world2pix` will detect points for which\n+            current correction to the coordinates is larger than\n+            the correction applied during the previous iteration\n+            **if** the requested accuracy **has not yet been\n+            achieved**. In this case, if ``adaptive`` is `True`,\n+            these points will be excluded from further iterations and\n+            if ``adaptive`` is `False`, :py:meth:`all_world2pix` will\n+            automatically switch to the adaptive algorithm. Thus, the\n+            reported divergent solution will be the latest converging\n+            solution computed immediately *before* divergence\n+            has been detected.\n+\n+            .. note::\n+               When accuracy has been achieved, small increases in\n+               current corrections may be possible due to rounding\n+               errors (when ``adaptive`` is `False`) and such\n+               increases will be ignored.\n+\n+            .. note::\n+               Based on our testing using HST ACS/WFC images, setting\n+               ``detect_divergence`` to `True` will incur about 5-20%\n+               performance penalty with the larger penalty\n+               corresponding to ``adaptive`` set to `True`.\n+               Because the benefits of enabling this\n+               feature outweigh the small performance penalty,\n+               especially when ``adaptive`` = `False`, it is\n+               recommended to set ``detect_divergence`` to `True`,\n+               unless extensive testing of the distortion models for\n+               images from specific instruments show a good stability\n+               of the numerical method for a wide range of\n+               coordinates (even outside the image itself).\n+\n+            .. note::\n+               Indices of the diverging inverse solutions will be\n+               reported in the ``divergent`` attribute of the\n+               raised :py:class:`NoConvergence` exception object.\n+\n+        Returns\n+        -------\n+\n+        {2}\n+\n+        Notes\n+        -----\n+        The order of the axes for the input world array is determined by\n+        the ``CTYPEia`` keywords in the FITS header, therefore it may\n+        not always be of the form (*ra*, *dec*).  The\n+        `~astropy.wcs.Wcsprm.lat`, `~astropy.wcs.Wcsprm.lng`,\n+        `~astropy.wcs.Wcsprm.lattyp`, and\n+        `~astropy.wcs.Wcsprm.lngtyp`\n+        members can be used to determine the order of the axes.\n+\n+        Using the method of fixed-point iterations approximations we\n+        iterate starting with the initial approximation, which is\n+        computed using the non-distortion-aware\n+        :py:meth:`wcs_world2pix` (or equivalent).\n+\n+        The :py:meth:`all_world2pix` function uses a vectorized\n+        implementation of the method of consecutive approximations and\n+        therefore it is highly efficient (>30x) when *all* data points\n+        that need to be converted from sky coordinates to image\n+        coordinates are passed at *once*. Therefore, it is advisable,\n+        whenever possible, to pass as input a long array of all points\n+        that need to be converted to :py:meth:`all_world2pix` instead\n+        of calling :py:meth:`all_world2pix` for each data point. Also\n+        see the note to the ``adaptive`` parameter.\n+\n+        Raises\n+        ------\n+        NoConvergence\n+            The method did not converge to a\n+            solution to the required accuracy within a specified\n+            number of maximum iterations set by the ``maxiter``\n+            parameter. To turn off this exception, set ``quiet`` to\n+            `True`. Indices of the points for which the requested\n+            accuracy was not achieved (if any) will be listed in the\n+            ``slow_conv`` attribute of the\n+            raised :py:class:`NoConvergence` exception object.\n+\n+            See :py:class:`NoConvergence` documentation for\n+            more details.\n+\n+        MemoryError\n+            Memory allocation failed.\n+\n+        SingularMatrixError\n+            Linear transformation matrix is singular.\n+\n+        InconsistentAxisTypesError\n+            Inconsistent or unrecognized coordinate axis types.\n+\n+        ValueError\n+            Invalid parameter value.\n+\n+        ValueError\n+            Invalid coordinate transformation parameters.\n+\n+        ValueError\n+            x- and y-coordinate arrays are not the same size.\n+\n+        InvalidTransformError\n+            Invalid coordinate transformation parameters.\n+\n+        InvalidTransformError\n+            Ill-conditioned coordinate transformation parameters.\n+\n+        Examples\n+        --------\n+        >>> import astropy.io.fits as fits\n+        >>> import astropy.wcs as wcs\n+        >>> import numpy as np\n+        >>> import os\n+\n+        >>> filename = os.path.join(wcs.__path__[0], 'tests/data/j94f05bgq_flt.fits')\n+        >>> hdulist = fits.open(filename)\n+        >>> w = wcs.WCS(hdulist[('sci',1)].header, hdulist)\n+        >>> hdulist.close()\n+\n+        >>> ra, dec = w.all_pix2world([1,2,3], [1,1,1], 1)\n+        >>> print(ra)  # doctest: +FLOAT_CMP\n+        [ 5.52645627  5.52649663  5.52653698]\n+        >>> print(dec)  # doctest: +FLOAT_CMP\n+        [-72.05171757 -72.05171276 -72.05170795]\n+        >>> radec = w.all_pix2world([[1,1], [2,1], [3,1]], 1)\n+        >>> print(radec)  # doctest: +FLOAT_CMP\n+        [[  5.52645627 -72.05171757]\n+         [  5.52649663 -72.05171276]\n+         [  5.52653698 -72.05170795]]\n+        >>> x, y = w.all_world2pix(ra, dec, 1)\n+        >>> print(x)  # doctest: +FLOAT_CMP\n+        [ 1.00000238  2.00000237  3.00000236]\n+        >>> print(y)  # doctest: +FLOAT_CMP\n+        [ 0.99999996  0.99999997  0.99999997]\n+        >>> xy = w.all_world2pix(radec, 1)\n+        >>> print(xy)  # doctest: +FLOAT_CMP\n+        [[ 1.00000238  0.99999996]\n+         [ 2.00000237  0.99999997]\n+         [ 3.00000236  0.99999997]]\n+        >>> xy = w.all_world2pix(radec, 1, maxiter=3,\n+        ...                      tolerance=1.0e-10, quiet=False)\n+        Traceback (most recent call last):\n+        ...\n+        NoConvergence: 'WCS.all_world2pix' failed to converge to the\n+        requested accuracy. After 3 iterations, the solution is\n+        diverging at least for one input point.\n+\n+        >>> # Now try to use some diverging data:\n+        >>> divradec = w.all_pix2world([[1.0, 1.0],\n+        ...                             [10000.0, 50000.0],\n+        ...                             [3.0, 1.0]], 1)\n+        >>> print(divradec)  # doctest: +FLOAT_CMP\n+        [[  5.52645627 -72.05171757]\n+         [  7.15976932 -70.8140779 ]\n+         [  5.52653698 -72.05170795]]\n+\n+        >>> # First, turn detect_divergence on:\n+        >>> try:  # doctest: +FLOAT_CMP\n+        ...   xy = w.all_world2pix(divradec, 1, maxiter=20,\n+        ...                        tolerance=1.0e-4, adaptive=False,\n+        ...                        detect_divergence=True,\n+        ...                        quiet=False)\n+        ... except wcs.wcs.NoConvergence as e:\n+        ...   print(\"Indices of diverging points: {{0}}\"\n+        ...         .format(e.divergent))\n+        ...   print(\"Indices of poorly converging points: {{0}}\"\n+        ...         .format(e.slow_conv))\n+        ...   print(\"Best solution:\\\\n{{0}}\".format(e.best_solution))\n+        ...   print(\"Achieved accuracy:\\\\n{{0}}\".format(e.accuracy))\n+        Indices of diverging points: [1]\n+        Indices of poorly converging points: None\n+        Best solution:\n+        [[  1.00000238e+00   9.99999965e-01]\n+         [ -1.99441636e+06   1.44309097e+06]\n+         [  3.00000236e+00   9.99999966e-01]]\n+        Achieved accuracy:\n+        [[  6.13968380e-05   8.59638593e-07]\n+         [  8.59526812e+11   6.61713548e+11]\n+         [  6.09398446e-05   8.38759724e-07]]\n+        >>> raise e\n+        Traceback (most recent call last):\n+        ...\n+        NoConvergence: 'WCS.all_world2pix' failed to converge to the\n+        requested accuracy.  After 5 iterations, the solution is\n+        diverging at least for one input point.\n+\n+        >>> # This time turn detect_divergence off:\n+        >>> try:  # doctest: +FLOAT_CMP\n+        ...   xy = w.all_world2pix(divradec, 1, maxiter=20,\n+        ...                        tolerance=1.0e-4, adaptive=False,\n+        ...                        detect_divergence=False,\n+        ...                        quiet=False)\n+        ... except wcs.wcs.NoConvergence as e:\n+        ...   print(\"Indices of diverging points: {{0}}\"\n+        ...         .format(e.divergent))\n+        ...   print(\"Indices of poorly converging points: {{0}}\"\n+        ...         .format(e.slow_conv))\n+        ...   print(\"Best solution:\\\\n{{0}}\".format(e.best_solution))\n+        ...   print(\"Achieved accuracy:\\\\n{{0}}\".format(e.accuracy))\n+        Indices of diverging points: [1]\n+        Indices of poorly converging points: None\n+        Best solution:\n+        [[ 1.00000009  1.        ]\n+         [        nan         nan]\n+         [ 3.00000009  1.        ]]\n+        Achieved accuracy:\n+        [[  2.29417358e-06   3.21222995e-08]\n+         [             nan              nan]\n+         [  2.27407877e-06   3.13005639e-08]]\n+        >>> raise e\n+        Traceback (most recent call last):\n+        ...\n+        NoConvergence: 'WCS.all_world2pix' failed to converge to the\n+        requested accuracy.  After 6 iterations, the solution is\n+        diverging at least for one input point.\n+\n+        \"\"\".format(__.TWO_OR_MORE_ARGS('naxis', 8),\n+                   __.RA_DEC_ORDER(8),\n+                   __.RETURNS('pixel coordinates', 8))\n+\n+    def wcs_world2pix(self, *args, **kwargs):\n+        if self.wcs is None:\n+            raise ValueError(\"No basic WCS settings were created.\")\n+        return self._array_converter(\n+            lambda xy, o: self.wcs.s2p(xy, o)['pixcrd'],\n+            'input', *args, **kwargs)\n+    wcs_world2pix.__doc__ = \"\"\"\n+        Transforms world coordinates to pixel coordinates, using only\n+        the basic `wcslib`_ WCS transformation.  No `SIP`_ or\n+        `distortion paper`_ table lookup transformation is applied.\n+\n+        Parameters\n+        ----------\n+        {0}\n+\n+            For a transformation that is not two-dimensional, the\n+            two-argument form must be used.\n+\n+        {1}\n+\n+        Returns\n+        -------\n+\n+        {2}\n+\n+        Notes\n+        -----\n+        The order of the axes for the input world array is determined by\n+        the ``CTYPEia`` keywords in the FITS header, therefore it may\n+        not always be of the form (*ra*, *dec*).  The\n+        `~astropy.wcs.Wcsprm.lat`, `~astropy.wcs.Wcsprm.lng`,\n+        `~astropy.wcs.Wcsprm.lattyp` and `~astropy.wcs.Wcsprm.lngtyp`\n+        members can be used to determine the order of the axes.\n+\n+        Raises\n+        ------\n+        MemoryError\n+            Memory allocation failed.\n+\n+        SingularMatrixError\n+            Linear transformation matrix is singular.\n+\n+        InconsistentAxisTypesError\n+            Inconsistent or unrecognized coordinate axis types.\n+\n+        ValueError\n+            Invalid parameter value.\n+\n+        ValueError\n+            Invalid coordinate transformation parameters.\n+\n+        ValueError\n+            x- and y-coordinate arrays are not the same size.\n+\n+        InvalidTransformError\n+            Invalid coordinate transformation parameters.\n+\n+        InvalidTransformError\n+            Ill-conditioned coordinate transformation parameters.\n+        \"\"\".format(__.TWO_OR_MORE_ARGS('naxis', 8),\n+                   __.RA_DEC_ORDER(8),\n+                   __.RETURNS('pixel coordinates', 8))\n+\n+    def pix2foc(self, *args):\n+        return self._array_converter(self._pix2foc, None, *args)\n+    pix2foc.__doc__ = \"\"\"\n+        Convert pixel coordinates to focal plane coordinates using the\n+        `SIP`_ polynomial distortion convention and `distortion\n+        paper`_ table-lookup correction.\n+\n+        The output is in absolute pixel coordinates, not relative to\n+        ``CRPIX``.\n+\n+        Parameters\n+        ----------\n+\n+        {0}\n+\n+        Returns\n+        -------\n+\n+        {1}\n+\n+        Raises\n+        ------\n+        MemoryError\n+            Memory allocation failed.\n+\n+        ValueError\n+            Invalid coordinate transformation parameters.\n+        \"\"\".format(__.TWO_OR_MORE_ARGS('2', 8),\n+                   __.RETURNS('focal coordinates', 8))\n+\n+    def p4_pix2foc(self, *args):\n+        return self._array_converter(self._p4_pix2foc, None, *args)\n+    p4_pix2foc.__doc__ = \"\"\"\n+        Convert pixel coordinates to focal plane coordinates using\n+        `distortion paper`_ table-lookup correction.\n+\n+        The output is in absolute pixel coordinates, not relative to\n+        ``CRPIX``.\n+\n+        Parameters\n+        ----------\n+\n+        {0}\n+\n+        Returns\n+        -------\n+\n+        {1}\n+\n+        Raises\n+        ------\n+        MemoryError\n+            Memory allocation failed.\n+\n+        ValueError\n+            Invalid coordinate transformation parameters.\n+        \"\"\".format(__.TWO_OR_MORE_ARGS('2', 8),\n+                   __.RETURNS('focal coordinates', 8))\n+\n+    def det2im(self, *args):\n+        return self._array_converter(self._det2im, None, *args)\n+    det2im.__doc__ = \"\"\"\n+        Convert detector coordinates to image plane coordinates using\n+        `distortion paper`_ table-lookup correction.\n+\n+        The output is in absolute pixel coordinates, not relative to\n+        ``CRPIX``.\n+\n+        Parameters\n+        ----------\n+\n+        {0}\n+\n+        Returns\n+        -------\n+\n+        {1}\n+\n+        Raises\n+        ------\n+        MemoryError\n+            Memory allocation failed.\n+\n+        ValueError\n+            Invalid coordinate transformation parameters.\n+        \"\"\".format(__.TWO_OR_MORE_ARGS('2', 8),\n+                   __.RETURNS('pixel coordinates', 8))\n+\n+    def sip_pix2foc(self, *args):\n+        if self.sip is None:\n+            if len(args) == 2:\n+                return args[0]\n+            elif len(args) == 3:\n+                return args[:2]\n+            else:\n+                raise TypeError(\"Wrong number of arguments\")\n+        return self._array_converter(self.sip.pix2foc, None, *args)\n+    sip_pix2foc.__doc__ = \"\"\"\n+        Convert pixel coordinates to focal plane coordinates using the\n+        `SIP`_ polynomial distortion convention.\n+\n+        The output is in pixel coordinates, relative to ``CRPIX``.\n+\n+        FITS WCS `distortion paper`_ table lookup correction is not\n+        applied, even if that information existed in the FITS file\n+        that initialized this :class:`~astropy.wcs.WCS` object.  To\n+        correct for that, use `~astropy.wcs.WCS.pix2foc` or\n+        `~astropy.wcs.WCS.p4_pix2foc`.\n+\n+        Parameters\n+        ----------\n+\n+        {0}\n+\n+        Returns\n+        -------\n+\n+        {1}\n+\n+        Raises\n+        ------\n+        MemoryError\n+            Memory allocation failed.\n+\n+        ValueError\n+            Invalid coordinate transformation parameters.\n+        \"\"\".format(__.TWO_OR_MORE_ARGS('2', 8),\n+                   __.RETURNS('focal coordinates', 8))\n+\n+    def sip_foc2pix(self, *args):\n+        if self.sip is None:\n+            if len(args) == 2:\n+                return args[0]\n+            elif len(args) == 3:\n+                return args[:2]\n+            else:\n+                raise TypeError(\"Wrong number of arguments\")\n+        return self._array_converter(self.sip.foc2pix, None, *args)\n+    sip_foc2pix.__doc__ = \"\"\"\n+        Convert focal plane coordinates to pixel coordinates using the\n+        `SIP`_ polynomial distortion convention.\n+\n+        FITS WCS `distortion paper`_ table lookup distortion\n+        correction is not applied, even if that information existed in\n+        the FITS file that initialized this `~astropy.wcs.WCS` object.\n+\n+        Parameters\n+        ----------\n+\n+        {0}\n+\n+        Returns\n+        -------\n+\n+        {1}\n+\n+        Raises\n+        ------\n+        MemoryError\n+            Memory allocation failed.\n+\n+        ValueError\n+            Invalid coordinate transformation parameters.\n+        \"\"\".format(__.TWO_OR_MORE_ARGS('2', 8),\n+                   __.RETURNS('pixel coordinates', 8))\n+\n+    def to_fits(self, relax=False, key=None):\n+        \"\"\"\n+        Generate an `astropy.io.fits.HDUList` object with all of the\n+        information stored in this object.  This should be logically identical\n+        to the input FITS file, but it will be normalized in a number of ways.\n+\n+        See `to_header` for some warnings about the output produced.\n+\n+        Parameters\n+        ----------\n+\n+        relax : bool or int, optional\n+            Degree of permissiveness:\n+\n+            - `False` (default): Write all extensions that are\n+              considered to be safe and recommended.\n+\n+            - `True`: Write all recognized informal extensions of the\n+              WCS standard.\n+\n+            - `int`: a bit field selecting specific extensions to\n+              write.  See :ref:`relaxwrite` for details.\n+\n+        key : str\n+            The name of a particular WCS transform to use.  This may be\n+            either ``' '`` or ``'A'``-``'Z'`` and corresponds to the ``\"a\"``\n+            part of the ``CTYPEia`` cards.\n+\n+        Returns\n+        -------\n+        hdulist : `astropy.io.fits.HDUList`\n+        \"\"\"\n+\n+        header = self.to_header(relax=relax, key=key)\n+\n+        hdu = fits.PrimaryHDU(header=header)\n+        hdulist = fits.HDUList(hdu)\n+\n+        self._write_det2im(hdulist)\n+        self._write_distortion_kw(hdulist)\n+\n+        return hdulist\n+\n+    def to_header(self, relax=None, key=None):\n+        \"\"\"Generate an `astropy.io.fits.Header` object with the basic WCS\n+        and SIP information stored in this object.  This should be\n+        logically identical to the input FITS file, but it will be\n+        normalized in a number of ways.\n+\n+        .. warning::\n+\n+          This function does not write out FITS WCS `distortion\n+          paper`_ information, since that requires multiple FITS\n+          header data units.  To get a full representation of\n+          everything in this object, use `to_fits`.\n+\n+        Parameters\n+        ----------\n+        relax : bool or int, optional\n+            Degree of permissiveness:\n+\n+            - `False` (default): Write all extensions that are\n+              considered to be safe and recommended.\n+\n+            - `True`: Write all recognized informal extensions of the\n+              WCS standard.\n+\n+            - `int`: a bit field selecting specific extensions to\n+              write.  See :ref:`relaxwrite` for details.\n+\n+            If the ``relax`` keyword argument is not given and any\n+            keywords were omitted from the output, an\n+            `~astropy.utils.exceptions.AstropyWarning` is displayed.\n+            To override this, explicitly pass a value to ``relax``.\n+\n+        key : str\n+            The name of a particular WCS transform to use.  This may be\n+            either ``' '`` or ``'A'``-``'Z'`` and corresponds to the ``\"a\"``\n+            part of the ``CTYPEia`` cards.\n+\n+        Returns\n+        -------\n+        header : `astropy.io.fits.Header`\n+\n+        Notes\n+        -----\n+        The output header will almost certainly differ from the input in a\n+        number of respects:\n+\n+          1. The output header only contains WCS-related keywords.  In\n+             particular, it does not contain syntactically-required\n+             keywords such as ``SIMPLE``, ``NAXIS``, ``BITPIX``, or\n+             ``END``.\n+\n+          2. Deprecated (e.g. ``CROTAn``) or non-standard usage will\n+             be translated to standard (this is partially dependent on\n+             whether ``fix`` was applied).\n+\n+          3. Quantities will be converted to the units used internally,\n+             basically SI with the addition of degrees.\n+\n+          4. Floating-point quantities may be given to a different decimal\n+             precision.\n+\n+          5. Elements of the ``PCi_j`` matrix will be written if and\n+             only if they differ from the unit matrix.  Thus, if the\n+             matrix is unity then no elements will be written.\n+\n+          6. Additional keywords such as ``WCSAXES``, ``CUNITia``,\n+             ``LONPOLEa`` and ``LATPOLEa`` may appear.\n+\n+          7. The original keycomments will be lost, although\n+             `to_header` tries hard to write meaningful comments.\n+\n+          8. Keyword order may be changed.\n+\n+        \"\"\"\n+        # default precision for numerical WCS keywords\n+        precision = WCSHDO_P14\n+        display_warning = False\n+        if relax is None:\n+            display_warning = True\n+            relax = False\n+\n+        if relax not in (True, False):\n+            do_sip = relax & WCSHDO_SIP\n+            relax &= ~WCSHDO_SIP\n+        else:\n+            do_sip = relax\n+            relax = WCSHDO_all if relax is True else WCSHDO_safe\n+\n+        relax = precision | relax\n+\n+        if self.wcs is not None:\n+            if key is not None:\n+                orig_key = self.wcs.alt\n+                self.wcs.alt = key\n+            header_string = self.wcs.to_header(relax)\n+            header = fits.Header.fromstring(header_string)\n+            keys_to_remove = [\"\", \" \", \"COMMENT\"]\n+            for kw in keys_to_remove:\n+                if kw in header:\n+                    del header[kw]\n+        else:\n+            header = fits.Header()\n+\n+        if do_sip and self.sip is not None:\n+            if self.wcs is not None and any(not ctyp.endswith('-SIP') for ctyp in self.wcs.ctype):\n+                self._fix_ctype(header, add_sip=True)\n+\n+            for kw, val in self._write_sip_kw().items():\n+                header[kw] = val\n+\n+        if not do_sip and self.wcs is not None and any(self.wcs.ctype) and self.sip is not None:\n+            # This is called when relax is not False or WCSHDO_SIP\n+            # The default case of ``relax=None`` is handled further in the code.\n+            header = self._fix_ctype(header, add_sip=False)\n+\n+        if display_warning:\n+            full_header = self.to_header(relax=True, key=key)\n+            missing_keys = []\n+            for kw, val in full_header.items():\n+                if kw not in header:\n+                    missing_keys.append(kw)\n+\n+            if len(missing_keys):\n+                warnings.warn(\n+                    \"Some non-standard WCS keywords were excluded: {0} \"\n+                    \"Use the ``relax`` kwarg to control this.\".format(\n+                        ', '.join(missing_keys)),\n+                    AstropyWarning)\n+            # called when ``relax=None``\n+            # This is different from the case of ``relax=False``.\n+            if any(self.wcs.ctype) and self.sip is not None:\n+                header = self._fix_ctype(header, add_sip=False, log_message=False)\n+        # Finally reset the key. This must be called after ``_fix_ctype``.\n+        if key is not None:\n+            self.wcs.alt = orig_key\n+        return header\n+\n+    def _fix_ctype(self, header, add_sip=True, log_message=True):\n+        \"\"\"\n+        Parameters\n+        ----------\n+        header : `~astropy.io.fits.Header`\n+            FITS header.\n+        add_sip : bool\n+            Flag indicating whether \"-SIP\" should be added or removed from CTYPE keywords.\n+\n+            Remove \"-SIP\" from CTYPE when writing out a header with relax=False.\n+            This needs to be done outside ``to_header`` because ``to_header`` runs\n+            twice when ``relax=False`` and the second time ``relax`` is set to ``True``\n+            to display the missing keywords.\n+\n+            If the user requested SIP distortion to be written out add \"-SIP\" to\n+            CTYPE if it is missing.\n+        \"\"\"\n+\n+        _add_sip_to_ctype = \"\"\"\n+        Inconsistent SIP distortion information is present in the current WCS:\n+        SIP coefficients were detected, but CTYPE is missing \"-SIP\" suffix,\n+        therefore the current WCS is internally inconsistent.\n+\n+        Because relax has been set to True, the resulting output WCS will have\n+        \"-SIP\" appended to CTYPE in order to make the header internally consistent.\n+\n+        However, this may produce incorrect astrometry in the output WCS, if\n+        in fact the current WCS is already distortion-corrected.\n+\n+        Therefore, if current WCS is already distortion-corrected (eg, drizzled)\n+        then SIP distortion components should not apply. In that case, for a WCS\n+        that is already distortion-corrected, please remove the SIP coefficients\n+        from the header.\n+\n+        \"\"\"\n+        if log_message:\n+            if add_sip:\n+                log.info(_add_sip_to_ctype)\n+        for i in range(1, self.naxis+1):\n+            # strip() must be called here to cover the case of alt key= \" \"\n+            kw = 'CTYPE{0}{1}'.format(i, self.wcs.alt).strip()\n+            if kw in header:\n+                if add_sip:\n+                    val = header[kw].strip(\"-SIP\") + \"-SIP\"\n+                else:\n+                    val = header[kw].strip(\"-SIP\")\n+                header[kw] = val\n+            else:\n+                continue\n+        return header\n+\n+    def to_header_string(self, relax=None):\n+        \"\"\"\n+        Identical to `to_header`, but returns a string containing the\n+        header cards.\n+        \"\"\"\n+        return str(self.to_header(relax))\n+\n+    def footprint_to_file(self, filename='footprint.reg', color='green',\n+                          width=2, coordsys=None):\n+        \"\"\"\n+        Writes out a `ds9`_ style regions file. It can be loaded\n+        directly by `ds9`_.\n+\n+        Parameters\n+        ----------\n+        filename : str, optional\n+            Output file name - default is ``'footprint.reg'``\n+\n+        color : str, optional\n+            Color to use when plotting the line.\n+\n+        width : int, optional\n+            Width of the region line.\n+\n+        coordsys : str, optional\n+            Coordinate system. If not specified (default), the ``radesys``\n+            value is used. For all possible values, see\n+            http://ds9.si.edu/doc/ref/region.html#RegionFileFormat\n+\n+        \"\"\"\n+        comments = ('# Region file format: DS9 version 4.0 \\n'\n+                    '# global color=green font=\"helvetica 12 bold '\n+                    'select=1 highlite=1 edit=1 move=1 delete=1 '\n+                    'include=1 fixed=0 source\\n')\n+\n+        coordsys = coordsys or self.wcs.radesys\n+\n+        if coordsys not in ('PHYSICAL', 'IMAGE', 'FK4', 'B1950', 'FK5',\n+                            'J2000', 'GALACTIC', 'ECLIPTIC', 'ICRS', 'LINEAR',\n+                            'AMPLIFIER', 'DETECTOR'):\n+            raise ValueError(\"Coordinate system '{}' is not supported. A valid\"\n+                             \" one can be given with the 'coordsys' argument.\"\n+                             .format(coordsys))\n+\n+        with open(filename, mode='w') as f:\n+            f.write(comments)\n+            f.write('{}\\n'.format(coordsys))\n+            f.write('polygon(')\n+            self.calc_footprint().tofile(f, sep=',')\n+            f.write(') # color={0}, width={1:d} \\n'.format(color, width))\n+\n+    @property\n+    def _naxis1(self):\n+        return self._naxis[0]\n+\n+    @_naxis1.setter\n+    def _naxis1(self, value):\n+        self._naxis[0] = value\n+\n+    @property\n+    def _naxis2(self):\n+        return self._naxis[1]\n+\n+    @_naxis2.setter\n+    def _naxis2(self, value):\n+        self._naxis[1] = value\n+\n+    def _get_naxis(self, header=None):\n+        _naxis = []\n+        if (header is not None and\n+                not isinstance(header, (str, bytes))):\n+            for naxis in itertools.count(1):\n+                try:\n+                    _naxis.append(header['NAXIS{}'.format(naxis)])\n+                except KeyError:\n+                    break\n+        if len(_naxis) == 0:\n+            _naxis = [0, 0]\n+        elif len(_naxis) == 1:\n+            _naxis.append(0)\n+        self._naxis = _naxis\n+\n+    def printwcs(self):\n+        print(repr(self))\n+\n+    def __repr__(self):\n+        '''\n+        Return a short description. Simply porting the behavior from\n+        the `printwcs()` method.\n+        '''\n+        description = [\"WCS Keywords\\n\",\n+                       \"Number of WCS axes: {0!r}\".format(self.naxis)]\n+        sfmt = ' : ' + \"\".join([\"{\"+\"{0}\".format(i)+\"!r}  \" for i in range(self.naxis)])\n+\n+        keywords = ['CTYPE', 'CRVAL', 'CRPIX']\n+        values = [self.wcs.ctype, self.wcs.crval, self.wcs.crpix]\n+        for keyword, value in zip(keywords, values):\n+            description.append(keyword+sfmt.format(*value))\n+\n+        if hasattr(self.wcs, 'pc'):\n+            for i in range(self.naxis):\n+                s = ''\n+                for j in range(self.naxis):\n+                    s += ''.join(['PC', str(i+1), '_', str(j+1), ' '])\n+                s += sfmt\n+                description.append(s.format(*self.wcs.pc[i]))\n+            s = 'CDELT' + sfmt\n+            description.append(s.format(*self.wcs.cdelt))\n+        elif hasattr(self.wcs, 'cd'):\n+            for i in range(self.naxis):\n+                s = ''\n+                for j in range(self.naxis):\n+                    s += \"\".join(['CD', str(i+1), '_', str(j+1), ' '])\n+                s += sfmt\n+                description.append(s.format(*self.wcs.cd[i]))\n+\n+        description.append('NAXIS : {}'.format('  '.join(map(str, self._naxis))))\n+        return '\\n'.join(description)\n+\n+    def get_axis_types(self):\n+        \"\"\"\n+        Similar to `self.wcsprm.axis_types <astropy.wcs.Wcsprm.axis_types>`\n+        but provides the information in a more Python-friendly format.\n+\n+        Returns\n+        -------\n+        result : list of dicts\n+\n+            Returns a list of dictionaries, one for each axis, each\n+            containing attributes about the type of that axis.\n+\n+            Each dictionary has the following keys:\n+\n+            - 'coordinate_type':\n+\n+              - None: Non-specific coordinate type.\n+\n+              - 'stokes': Stokes coordinate.\n+\n+              - 'celestial': Celestial coordinate (including ``CUBEFACE``).\n+\n+              - 'spectral': Spectral coordinate.\n+\n+            - 'scale':\n+\n+              - 'linear': Linear axis.\n+\n+              - 'quantized': Quantized axis (``STOKES``, ``CUBEFACE``).\n+\n+              - 'non-linear celestial': Non-linear celestial axis.\n+\n+              - 'non-linear spectral': Non-linear spectral axis.\n+\n+              - 'logarithmic': Logarithmic axis.\n+\n+              - 'tabular': Tabular axis.\n+\n+            - 'group'\n+\n+              - Group number, e.g. lookup table number\n+\n+            - 'number'\n+\n+              - For celestial axes:\n+\n+                - 0: Longitude coordinate.\n+\n+                - 1: Latitude coordinate.\n+\n+                - 2: ``CUBEFACE`` number.\n+\n+              - For lookup tables:\n+\n+                - the axis number in a multidimensional table.\n+\n+            ``CTYPEia`` in ``\"4-3\"`` form with unrecognized algorithm code will\n+            generate an error.\n+        \"\"\"\n+        if self.wcs is None:\n+            raise AttributeError(\n+                \"This WCS object does not have a wcsprm object.\")\n+\n+        coordinate_type_map = {\n+            0: None,\n+            1: 'stokes',\n+            2: 'celestial',\n+            3: 'spectral'}\n+\n+        scale_map = {\n+            0: 'linear',\n+            1: 'quantized',\n+            2: 'non-linear celestial',\n+            3: 'non-linear spectral',\n+            4: 'logarithmic',\n+            5: 'tabular'}\n+\n+        result = []\n+        for axis_type in self.wcs.axis_types:\n+            subresult = {}\n+\n+            coordinate_type = (axis_type // 1000) % 10\n+            subresult['coordinate_type'] = coordinate_type_map[coordinate_type]\n+\n+            scale = (axis_type // 100) % 10\n+            subresult['scale'] = scale_map[scale]\n+\n+            group = (axis_type // 10) % 10\n+            subresult['group'] = group\n+\n+            number = axis_type % 10\n+            subresult['number'] = number\n+\n+            result.append(subresult)\n+\n+        return result\n+\n+    def __reduce__(self):\n+        \"\"\"\n+        Support pickling of WCS objects.  This is done by serializing\n+        to an in-memory FITS file and dumping that as a string.\n+        \"\"\"\n+\n+        hdulist = self.to_fits(relax=True)\n+\n+        buffer = io.BytesIO()\n+        hdulist.writeto(buffer)\n+\n+        return (__WCS_unpickle__,\n+                (self.__class__, self.__dict__, buffer.getvalue(),))\n+\n+    def dropaxis(self, dropax):\n+        \"\"\"\n+        Remove an axis from the WCS.\n+\n+        Parameters\n+        ----------\n+        wcs : `~astropy.wcs.WCS`\n+            The WCS with naxis to be chopped to naxis-1\n+        dropax : int\n+            The index of the WCS to drop, counting from 0 (i.e., python convention,\n+            not FITS convention)\n+\n+        Returns\n+        -------\n+        A new `~astropy.wcs.WCS` instance with one axis fewer\n+        \"\"\"\n+        inds = list(range(self.wcs.naxis))\n+        inds.pop(dropax)\n+\n+        # axis 0 has special meaning to sub\n+        # if wcs.wcs.ctype == ['RA','DEC','VLSR'], you want\n+        # wcs.sub([1,2]) to get 'RA','DEC' back\n+        return self.sub([i+1 for i in inds])\n+\n+    def swapaxes(self, ax0, ax1):\n+        \"\"\"\n+        Swap axes in a WCS.\n+\n+        Parameters\n+        ----------\n+        wcs : `~astropy.wcs.WCS`\n+            The WCS to have its axes swapped\n+        ax0 : int\n+        ax1 : int\n+            The indices of the WCS to be swapped, counting from 0 (i.e., python\n+            convention, not FITS convention)\n+\n+        Returns\n+        -------\n+        A new `~astropy.wcs.WCS` instance with the same number of axes, but two\n+        swapped\n+        \"\"\"\n+        inds = list(range(self.wcs.naxis))\n+        inds[ax0], inds[ax1] = inds[ax1], inds[ax0]\n+\n+        return self.sub([i+1 for i in inds])\n+\n+    def reorient_celestial_first(self):\n+        \"\"\"\n+        Reorient the WCS such that the celestial axes are first, followed by\n+        the spectral axis, followed by any others.\n+        Assumes at least celestial axes are present.\n+        \"\"\"\n+        return self.sub([WCSSUB_CELESTIAL, WCSSUB_SPECTRAL, WCSSUB_STOKES])\n+\n+    def slice(self, view, numpy_order=True):\n+        \"\"\"\n+        Slice a WCS instance using a Numpy slice. The order of the slice should\n+        be reversed (as for the data) compared to the natural WCS order.\n+\n+        Parameters\n+        ----------\n+        view : tuple\n+            A tuple containing the same number of slices as the WCS system.\n+            The ``step`` method, the third argument to a slice, is not\n+            presently supported.\n+        numpy_order : bool\n+            Use numpy order, i.e. slice the WCS so that an identical slice\n+            applied to a numpy array will slice the array and WCS in the same\n+            way. If set to `False`, the WCS will be sliced in FITS order,\n+            meaning the first slice will be applied to the *last* numpy index\n+            but the *first* WCS axis.\n+\n+        Returns\n+        -------\n+        wcs_new : `~astropy.wcs.WCS`\n+            A new resampled WCS axis\n+        \"\"\"\n+        if hasattr(view, '__len__') and len(view) > self.wcs.naxis:\n+            raise ValueError(\"Must have # of slices <= # of WCS axes\")\n+        elif not hasattr(view, '__len__'):  # view MUST be an iterable\n+            view = [view]\n+\n+        if not all(isinstance(x, slice) for x in view):\n+            raise ValueError(\"Cannot downsample a WCS with indexing.  Use \"\n+                             \"wcs.sub or wcs.dropaxis if you want to remove \"\n+                             \"axes.\")\n+\n+        wcs_new = self.deepcopy()\n+        if wcs_new.sip is not None:\n+            sip_crpix = wcs_new.sip.crpix.tolist()\n+\n+        for i, iview in enumerate(view):\n+            if iview.step is not None and iview.step < 0:\n+                raise NotImplementedError(\"Reversing an axis is not \"\n+                                          \"implemented.\")\n+\n+            if numpy_order:\n+                wcs_index = self.wcs.naxis - 1 - i\n+            else:\n+                wcs_index = i\n+\n+            if iview.step is not None and iview.start is None:\n+                # Slice from \"None\" is equivalent to slice from 0 (but one\n+                # might want to downsample, so allow slices with\n+                # None,None,step or None,stop,step)\n+                iview = slice(0, iview.stop, iview.step)\n+\n+            if iview.start is not None:\n+                if iview.step not in (None, 1):\n+                    crpix = self.wcs.crpix[wcs_index]\n+                    cdelt = self.wcs.cdelt[wcs_index]\n+                    # equivalently (keep this comment so you can compare eqns):\n+                    # wcs_new.wcs.crpix[wcs_index] =\n+                    # (crpix - iview.start)*iview.step + 0.5 - iview.step/2.\n+                    crp = ((crpix - iview.start - 1.)/iview.step\n+                           + 0.5 + 1./iview.step/2.)\n+                    wcs_new.wcs.crpix[wcs_index] = crp\n+                    if wcs_new.sip is not None:\n+                        sip_crpix[wcs_index] = crp\n+                    wcs_new.wcs.cdelt[wcs_index] = cdelt * iview.step\n+                else:\n+                    wcs_new.wcs.crpix[wcs_index] -= iview.start\n+                    if wcs_new.sip is not None:\n+                        sip_crpix[wcs_index] -= iview.start\n+\n+            try:\n+                # range requires integers but the other attributes can also\n+                # handle arbitary values, so this needs to be in a try/except.\n+                nitems = len(builtins.range(self._naxis[wcs_index])[iview])\n+            except TypeError as exc:\n+                if 'indices must be integers' not in str(exc):\n+                    raise\n+                warnings.warn(\"NAXIS{0} attribute is not updated because at \"\n+                              \"least one indix ('{1}') is no integer.\"\n+                              \"\".format(wcs_index, iview), AstropyUserWarning)\n+            else:\n+                wcs_new._naxis[wcs_index] = nitems\n+\n+        if wcs_new.sip is not None:\n+            wcs_new.sip = Sip(self.sip.a, self.sip.b, self.sip.ap, self.sip.bp,\n+                              sip_crpix)\n+\n+        return wcs_new\n+\n+    def __getitem__(self, item):\n+        # \"getitem\" is a shortcut for self.slice; it is very limited\n+        # there is no obvious and unambiguous interpretation of wcs[1,2,3]\n+        # We COULD allow wcs[1] to link to wcs.sub([2])\n+        # (wcs[i] -> wcs.sub([i+1])\n+        return self.slice(item)\n+\n+    def __iter__(self):\n+        # Having __getitem__ makes Python think WCS is iterable. However,\n+        # Python first checks whether __iter__ is present, so we can raise an\n+        # exception here.\n+        raise TypeError(\"'{0}' object is not iterable\".format(self.__class__.__name__))\n+\n+    @property\n+    def axis_type_names(self):\n+        \"\"\"\n+        World names for each coordinate axis\n+\n+        Returns\n+        -------\n+        A list of names along each axis\n+        \"\"\"\n+        names = list(self.wcs.cname)\n+        types = self.wcs.ctype\n+        for i in range(len(names)):\n+            if len(names[i]) > 0:\n+                continue\n+            names[i] = types[i].split('-')[0]\n+        return names\n+\n+    @property\n+    def celestial(self):\n+        \"\"\"\n+        A copy of the current WCS with only the celestial axes included\n+        \"\"\"\n+        return self.sub([WCSSUB_CELESTIAL])\n+\n+    @property\n+    def is_celestial(self):\n+        return self.has_celestial and self.naxis == 2\n+\n+    @property\n+    def has_celestial(self):\n+        try:\n+            return self.celestial.naxis == 2\n+        except InconsistentAxisTypesError:\n+            return False\n+\n+    @property\n+    def pixel_scale_matrix(self):\n+\n+        try:\n+            cdelt = np.matrix(np.diag(self.wcs.get_cdelt()))\n+            pc = np.matrix(self.wcs.get_pc())\n+        except InconsistentAxisTypesError:\n+            try:\n+                # for non-celestial axes, get_cdelt doesn't work\n+                cdelt = np.matrix(self.wcs.cd) * np.matrix(np.diag(self.wcs.cdelt))\n+            except AttributeError:\n+                cdelt = np.matrix(np.diag(self.wcs.cdelt))\n+\n+            try:\n+                pc = np.matrix(self.wcs.pc)\n+            except AttributeError:\n+                pc = 1\n+\n+        pccd = np.array(cdelt * pc)\n+\n+        return pccd\n+\n+    def _as_mpl_axes(self):\n+        \"\"\"\n+        Compatibility hook for Matplotlib and WCSAxes.\n+\n+        With this method, one can do:\n+\n+            from astropy.wcs import WCS\n+            import matplotlib.pyplot as plt\n+\n+            wcs = WCS('filename.fits')\n+\n+            fig = plt.figure()\n+            ax = fig.add_axes([0.15, 0.1, 0.8, 0.8], projection=wcs)\n+            ...\n+\n+        and this will generate a plot with the correct WCS coordinates on the\n+        axes.\n+        \"\"\"\n+        from ..visualization.wcsaxes import WCSAxes\n+        return WCSAxes, {'wcs': self}\n+\n+\n+def __WCS_unpickle__(cls, dct, fits_data):\n+    \"\"\"\n+    Unpickles a WCS object from a serialized FITS string.\n+    \"\"\"\n+\n+    self = cls.__new__(cls)\n+    self.__dict__.update(dct)\n+\n+    buffer = io.BytesIO(fits_data)\n+    hdulist = fits.open(buffer)\n+\n+    WCS.__init__(self, hdulist[0].header, hdulist)\n+\n+    return self\n+\n+\n+def find_all_wcs(header, relax=True, keysel=None, fix=True,\n+                 translate_units='',\n+                 _do_set=True):\n+    \"\"\"\n+    Find all the WCS transformations in the given header.\n+\n+    Parameters\n+    ----------\n+    header : str or astropy.io.fits header object.\n+\n+    relax : bool or int, optional\n+        Degree of permissiveness:\n+\n+        - `True` (default): Admit all recognized informal extensions of the\n+          WCS standard.\n+\n+        - `False`: Recognize only FITS keywords defined by the\n+          published WCS standard.\n+\n+        - `int`: a bit field selecting specific extensions to accept.\n+          See :ref:`relaxread` for details.\n+\n+    keysel : sequence of flags, optional\n+        A list of flags used to select the keyword types considered by\n+        wcslib.  When ``None``, only the standard image header\n+        keywords are considered (and the underlying wcspih() C\n+        function is called).  To use binary table image array or pixel\n+        list keywords, *keysel* must be set.\n+\n+        Each element in the list should be one of the following strings:\n+\n+            - 'image': Image header keywords\n+\n+            - 'binary': Binary table image array keywords\n+\n+            - 'pixel': Pixel list keywords\n+\n+        Keywords such as ``EQUIna`` or ``RFRQna`` that are common to\n+        binary table image arrays and pixel lists (including\n+        ``WCSNna`` and ``TWCSna``) are selected by both 'binary' and\n+        'pixel'.\n+\n+    fix : bool, optional\n+        When `True` (default), call `~astropy.wcs.Wcsprm.fix` on\n+        the resulting objects to fix any non-standard uses in the\n+        header.  `FITSFixedWarning` warnings will be emitted if any\n+        changes were made.\n+\n+    translate_units : str, optional\n+        Specify which potentially unsafe translations of non-standard\n+        unit strings to perform.  By default, performs none.  See\n+        `WCS.fix` for more information about this parameter.  Only\n+        effective when ``fix`` is `True`.\n+\n+    Returns\n+    -------\n+    wcses : list of `WCS` objects\n+    \"\"\"\n+\n+    if isinstance(header, (str, bytes)):\n+        header_string = header\n+    elif isinstance(header, fits.Header):\n+        header_string = header.tostring()\n+    else:\n+        raise TypeError(\n+            \"header must be a string or astropy.io.fits.Header object\")\n+\n+    keysel_flags = _parse_keysel(keysel)\n+\n+    if isinstance(header_string, str):\n+        header_bytes = header_string.encode('ascii')\n+    else:\n+        header_bytes = header_string\n+\n+    wcsprms = _wcs.find_all_wcs(header_bytes, relax, keysel_flags)\n+\n+    result = []\n+    for wcsprm in wcsprms:\n+        subresult = WCS(fix=False, _do_set=False)\n+        subresult.wcs = wcsprm\n+        result.append(subresult)\n+\n+        if fix:\n+            subresult.fix(translate_units)\n+\n+        if _do_set:\n+            subresult.wcs.set()\n+\n+    return result\n+\n+\n+def validate(source):\n+    \"\"\"\n+    Prints a WCS validation report for the given FITS file.\n+\n+    Parameters\n+    ----------\n+    source : str path, readable file-like object or `astropy.io.fits.HDUList` object\n+        The FITS file to validate.\n+\n+    Returns\n+    -------\n+    results : WcsValidateResults instance\n+        The result is returned as nested lists.  The first level\n+        corresponds to the HDUs in the given file.  The next level has\n+        an entry for each WCS found in that header.  The special\n+        subclass of list will pretty-print the results as a table when\n+        printed.\n+    \"\"\"\n+    class _WcsValidateWcsResult(list):\n+        def __init__(self, key):\n+            self._key = key\n+\n+        def __repr__(self):\n+            result = [\"  WCS key '{0}':\".format(self._key or ' ')]\n+            if len(self):\n+                for entry in self:\n+                    for i, line in enumerate(entry.splitlines()):\n+                        if i == 0:\n+                            initial_indent = '    - '\n+                        else:\n+                            initial_indent = '      '\n+                        result.extend(\n+                            textwrap.wrap(\n+                                line,\n+                                initial_indent=initial_indent,\n+                                subsequent_indent='      '))\n+            else:\n+                result.append(\"    No issues.\")\n+            return '\\n'.join(result)\n+\n+    class _WcsValidateHduResult(list):\n+        def __init__(self, hdu_index, hdu_name):\n+            self._hdu_index = hdu_index\n+            self._hdu_name = hdu_name\n+            list.__init__(self)\n+\n+        def __repr__(self):\n+            if len(self):\n+                if self._hdu_name:\n+                    hdu_name = ' ({0})'.format(self._hdu_name)\n+                else:\n+                    hdu_name = ''\n+                result = ['HDU {0}{1}:'.format(self._hdu_index, hdu_name)]\n+                for wcs in self:\n+                    result.append(repr(wcs))\n+                return '\\n'.join(result)\n+            return ''\n+\n+    class _WcsValidateResults(list):\n+        def __repr__(self):\n+            result = []\n+            for hdu in self:\n+                content = repr(hdu)\n+                if len(content):\n+                    result.append(content)\n+            return '\\n\\n'.join(result)\n+\n+    global __warningregistry__\n+\n+    if isinstance(source, fits.HDUList):\n+        hdulist = source\n+    else:\n+        hdulist = fits.open(source)\n+\n+    results = _WcsValidateResults()\n+\n+    for i, hdu in enumerate(hdulist):\n+        hdu_results = _WcsValidateHduResult(i, hdu.name)\n+        results.append(hdu_results)\n+\n+        with warnings.catch_warnings(record=True) as warning_lines:\n+            wcses = find_all_wcs(\n+                hdu.header, relax=_wcs.WCSHDR_reject,\n+                fix=False, _do_set=False)\n+\n+        for wcs in wcses:\n+            wcs_results = _WcsValidateWcsResult(wcs.wcs.alt)\n+            hdu_results.append(wcs_results)\n+\n+            try:\n+                del __warningregistry__\n+            except NameError:\n+                pass\n+\n+            with warnings.catch_warnings(record=True) as warning_lines:\n+                warnings.resetwarnings()\n+                warnings.simplefilter(\n+                    \"always\", FITSFixedWarning, append=True)\n+\n+                try:\n+                    WCS(hdu.header,\n+                        key=wcs.wcs.alt or ' ',\n+                        relax=_wcs.WCSHDR_reject,\n+                        fix=True, _do_set=False)\n+                except WcsError as e:\n+                    wcs_results.append(str(e))\n+\n+                wcs_results.extend([str(x.message) for x in warning_lines])\n+\n+    return results\ndiff --git a/astropy/wcs/tests/test_wcs.py b/astropy/wcs/tests/test_wcs.py\nindex 85853e10e5..1dcbdc98fc 100644\n--- a/astropy/wcs/tests/test_wcs.py\n+++ b/astropy/wcs/tests/test_wcs.py\n@@ -1093,3 +1093,56 @@ def test_keyedsip():\n     assert isinstance( w.sip, wcs.Sip )\n     assert w.sip.crpix[0] == 2048\n     assert w.sip.crpix[1] == 1026\n+\n+\n+def test_empty_input_to_wcs_transforms():\n+    \"\"\"\n+    Test that passing empty lists/arrays to WCS transformation methods\n+    returns empty arrays rather than raising an error (issue #5997).\n+    \"\"\"\n+    w = wcs.WCS(naxis=2)\n+    w.wcs.crpix = [1, 1]\n+    w.wcs.cdelt = [1, 1]\n+    w.wcs.crval = [0, 0]\n+    w.wcs.ctype = ['RA---TAN', 'DEC--TAN']\n+    w.wcs.set()\n+\n+    # Test wcs_pix2world with list-of-arrays form (empty lists)\n+    result = w.wcs_pix2world([], [], 0)\n+    assert isinstance(result, list)\n+    assert len(result) == 2\n+    assert result[0].shape == (0,)\n+    assert result[1].shape == (0,)\n+\n+    # Test wcs_pix2world with single Nx2 array form (empty array)\n+    result = w.wcs_pix2world(np.empty((0, 2)), 0)\n+    assert result.shape == (0, 2)\n+\n+    # Test wcs_world2pix with list-of-arrays form (empty lists)\n+    result = w.wcs_world2pix([], [], 0)\n+    assert isinstance(result, list)\n+    assert len(result) == 2\n+    assert result[0].shape == (0,)\n+    assert result[1].shape == (0,)\n+\n+    # Test wcs_world2pix with single Nx2 array form (empty array)\n+    result = w.wcs_world2pix(np.empty((0, 2)), 0)\n+    assert result.shape == (0, 2)\n+\n+    # Test all_pix2world with list-of-arrays form (empty lists)\n+    result = w.all_pix2world([], [], 0)\n+    assert isinstance(result, list)\n+    assert len(result) == 2\n+    assert result[0].shape == (0,)\n+    assert result[1].shape == (0,)\n+\n+    # Test all_pix2world with single Nx2 array form (empty array)\n+    result = w.all_pix2world(np.empty((0, 2)), 0)\n+    assert result.shape == (0, 2)\n+\n+    # Test all_world2pix with empty arrays\n+    result = w.all_world2pix([], [], 0)\n+    assert isinstance(result, list)\n+    assert len(result) == 2\n+    assert result[0].shape == (0,)\n+    assert result[1].shape == (0,)\ndiff --git a/astropy/wcs/wcs.py b/astropy/wcs/wcs.py\nindex d3a06f2b31..89f6bfbe4c 100644\n--- a/astropy/wcs/wcs.py\n+++ b/astropy/wcs/wcs.py\n@@ -1220,6 +1220,13 @@ reduce these to 2 dimensions using the naxis kwarg.\n \n             xy = np.hstack([x.reshape((x.size, 1)) for x in axes])\n \n+            if xy.shape[0] == 0:\n+                if ra_dec_order and sky == 'output':\n+                    return (xy[:, 0].reshape(axes[0].shape),\n+                            xy[:, 1].reshape(axes[0].shape))\n+                return [xy[:, i].reshape(axes[0].shape)\n+                        for i in range(xy.shape[1])]\n+\n             if ra_dec_order and sky == 'input':\n                 xy = self._denormalize_sky(xy)\n             output = func(xy, origin)\n@@ -1235,6 +1242,10 @@ reduce these to 2 dimensions using the naxis kwarg.\n                 raise ValueError(\n                     \"When providing two arguments, the array must be \"\n                     \"of shape (N, {0})\".format(self.naxis))\n+            if 0 in xy.shape:\n+                if ra_dec_order and sky == 'output':\n+                    return self._normalize_sky(xy)\n+                return xy\n             if ra_dec_order and sky == 'input':\n                 xy = self._denormalize_sky(xy)\n             result = func(xy, origin)\n"}
{"instance_id": "django__django-10914", "model_name_or_path": "harness/claude-sonnet-4-6", "model_patch": "diff --git a/.harness/checkpoints/9c5dfe39b5aa/django--conf--global_settings.py b/.harness/checkpoints/9c5dfe39b5aa/django--conf--global_settings.py\nnew file mode 100644\nindex 0000000000..acee9887d2\n--- /dev/null\n+++ b/.harness/checkpoints/9c5dfe39b5aa/django--conf--global_settings.py\n@@ -0,0 +1,634 @@\n+\"\"\"\n+Default Django settings. Override these with settings in the module pointed to\n+by the DJANGO_SETTINGS_MODULE environment variable.\n+\"\"\"\n+\n+\n+# This is defined here as a do-nothing function because we can't import\n+# django.utils.translation -- that module depends on the settings.\n+def gettext_noop(s):\n+    return s\n+\n+\n+####################\n+# CORE             #\n+####################\n+\n+DEBUG = False\n+\n+# Whether the framework should propagate raw exceptions rather than catching\n+# them. This is useful under some testing situations and should never be used\n+# on a live site.\n+DEBUG_PROPAGATE_EXCEPTIONS = False\n+\n+# People who get code error notifications.\n+# In the format [('Full Name', 'email@example.com'), ('Full Name', 'anotheremail@example.com')]\n+ADMINS = []\n+\n+# List of IP addresses, as strings, that:\n+#   * See debug comments, when DEBUG is true\n+#   * Receive x-headers\n+INTERNAL_IPS = []\n+\n+# Hosts/domain names that are valid for this site.\n+# \"*\" matches anything, \".example.com\" matches example.com and all subdomains\n+ALLOWED_HOSTS = []\n+\n+# Local time zone for this installation. All choices can be found here:\n+# https://en.wikipedia.org/wiki/List_of_tz_zones_by_name (although not all\n+# systems may support all possibilities). When USE_TZ is True, this is\n+# interpreted as the default user time zone.\n+TIME_ZONE = 'America/Chicago'\n+\n+# If you set this to True, Django will use timezone-aware datetimes.\n+USE_TZ = False\n+\n+# Language code for this installation. All choices can be found here:\n+# http://www.i18nguy.com/unicode/language-identifiers.html\n+LANGUAGE_CODE = 'en-us'\n+\n+# Languages we provide translations for, out of the box.\n+LANGUAGES = [\n+    ('af', gettext_noop('Afrikaans')),\n+    ('ar', gettext_noop('Arabic')),\n+    ('ast', gettext_noop('Asturian')),\n+    ('az', gettext_noop('Azerbaijani')),\n+    ('bg', gettext_noop('Bulgarian')),\n+    ('be', gettext_noop('Belarusian')),\n+    ('bn', gettext_noop('Bengali')),\n+    ('br', gettext_noop('Breton')),\n+    ('bs', gettext_noop('Bosnian')),\n+    ('ca', gettext_noop('Catalan')),\n+    ('cs', gettext_noop('Czech')),\n+    ('cy', gettext_noop('Welsh')),\n+    ('da', gettext_noop('Danish')),\n+    ('de', gettext_noop('German')),\n+    ('dsb', gettext_noop('Lower Sorbian')),\n+    ('el', gettext_noop('Greek')),\n+    ('en', gettext_noop('English')),\n+    ('en-au', gettext_noop('Australian English')),\n+    ('en-gb', gettext_noop('British English')),\n+    ('eo', gettext_noop('Esperanto')),\n+    ('es', gettext_noop('Spanish')),\n+    ('es-ar', gettext_noop('Argentinian Spanish')),\n+    ('es-co', gettext_noop('Colombian Spanish')),\n+    ('es-mx', gettext_noop('Mexican Spanish')),\n+    ('es-ni', gettext_noop('Nicaraguan Spanish')),\n+    ('es-ve', gettext_noop('Venezuelan Spanish')),\n+    ('et', gettext_noop('Estonian')),\n+    ('eu', gettext_noop('Basque')),\n+    ('fa', gettext_noop('Persian')),\n+    ('fi', gettext_noop('Finnish')),\n+    ('fr', gettext_noop('French')),\n+    ('fy', gettext_noop('Frisian')),\n+    ('ga', gettext_noop('Irish')),\n+    ('gd', gettext_noop('Scottish Gaelic')),\n+    ('gl', gettext_noop('Galician')),\n+    ('he', gettext_noop('Hebrew')),\n+    ('hi', gettext_noop('Hindi')),\n+    ('hr', gettext_noop('Croatian')),\n+    ('hsb', gettext_noop('Upper Sorbian')),\n+    ('hu', gettext_noop('Hungarian')),\n+    ('hy', gettext_noop('Armenian')),\n+    ('ia', gettext_noop('Interlingua')),\n+    ('id', gettext_noop('Indonesian')),\n+    ('io', gettext_noop('Ido')),\n+    ('is', gettext_noop('Icelandic')),\n+    ('it', gettext_noop('Italian')),\n+    ('ja', gettext_noop('Japanese')),\n+    ('ka', gettext_noop('Georgian')),\n+    ('kab', gettext_noop('Kabyle')),\n+    ('kk', gettext_noop('Kazakh')),\n+    ('km', gettext_noop('Khmer')),\n+    ('kn', gettext_noop('Kannada')),\n+    ('ko', gettext_noop('Korean')),\n+    ('lb', gettext_noop('Luxembourgish')),\n+    ('lt', gettext_noop('Lithuanian')),\n+    ('lv', gettext_noop('Latvian')),\n+    ('mk', gettext_noop('Macedonian')),\n+    ('ml', gettext_noop('Malayalam')),\n+    ('mn', gettext_noop('Mongolian')),\n+    ('mr', gettext_noop('Marathi')),\n+    ('my', gettext_noop('Burmese')),\n+    ('nb', gettext_noop('Norwegian Bokm\u00e5l')),\n+    ('ne', gettext_noop('Nepali')),\n+    ('nl', gettext_noop('Dutch')),\n+    ('nn', gettext_noop('Norwegian Nynorsk')),\n+    ('os', gettext_noop('Ossetic')),\n+    ('pa', gettext_noop('Punjabi')),\n+    ('pl', gettext_noop('Polish')),\n+    ('pt', gettext_noop('Portuguese')),\n+    ('pt-br', gettext_noop('Brazilian Portuguese')),\n+    ('ro', gettext_noop('Romanian')),\n+    ('ru', gettext_noop('Russian')),\n+    ('sk', gettext_noop('Slovak')),\n+    ('sl', gettext_noop('Slovenian')),\n+    ('sq', gettext_noop('Albanian')),\n+    ('sr', gettext_noop('Serbian')),\n+    ('sr-latn', gettext_noop('Serbian Latin')),\n+    ('sv', gettext_noop('Swedish')),\n+    ('sw', gettext_noop('Swahili')),\n+    ('ta', gettext_noop('Tamil')),\n+    ('te', gettext_noop('Telugu')),\n+    ('th', gettext_noop('Thai')),\n+    ('tr', gettext_noop('Turkish')),\n+    ('tt', gettext_noop('Tatar')),\n+    ('udm', gettext_noop('Udmurt')),\n+    ('uk', gettext_noop('Ukrainian')),\n+    ('ur', gettext_noop('Urdu')),\n+    ('vi', gettext_noop('Vietnamese')),\n+    ('zh-hans', gettext_noop('Simplified Chinese')),\n+    ('zh-hant', gettext_noop('Traditional Chinese')),\n+]\n+\n+# Languages using BiDi (right-to-left) layout\n+LANGUAGES_BIDI = [\"he\", \"ar\", \"fa\", \"ur\"]\n+\n+# If you set this to False, Django will make some optimizations so as not\n+# to load the internationalization machinery.\n+USE_I18N = True\n+LOCALE_PATHS = []\n+\n+# Settings for language cookie\n+LANGUAGE_COOKIE_NAME = 'django_language'\n+LANGUAGE_COOKIE_AGE = None\n+LANGUAGE_COOKIE_DOMAIN = None\n+LANGUAGE_COOKIE_PATH = '/'\n+\n+\n+# If you set this to True, Django will format dates, numbers and calendars\n+# according to user current locale.\n+USE_L10N = False\n+\n+# Not-necessarily-technical managers of the site. They get broken link\n+# notifications and other various emails.\n+MANAGERS = ADMINS\n+\n+# Default charset to use for all HttpResponse objects, if a MIME type isn't\n+# manually specified. It's used to construct the Content-Type header.\n+DEFAULT_CHARSET = 'utf-8'\n+\n+# Encoding of files read from disk (template and initial SQL files).\n+FILE_CHARSET = 'utf-8'\n+\n+# Email address that error messages come from.\n+SERVER_EMAIL = 'root@localhost'\n+\n+# Database connection info. If left empty, will default to the dummy backend.\n+DATABASES = {}\n+\n+# Classes used to implement DB routing behavior.\n+DATABASE_ROUTERS = []\n+\n+# The email backend to use. For possible shortcuts see django.core.mail.\n+# The default is to use the SMTP backend.\n+# Third-party backends can be specified by providing a Python path\n+# to a module that defines an EmailBackend class.\n+EMAIL_BACKEND = 'django.core.mail.backends.smtp.EmailBackend'\n+\n+# Host for sending email.\n+EMAIL_HOST = 'localhost'\n+\n+# Port for sending email.\n+EMAIL_PORT = 25\n+\n+# Whether to send SMTP 'Date' header in the local time zone or in UTC.\n+EMAIL_USE_LOCALTIME = False\n+\n+# Optional SMTP authentication information for EMAIL_HOST.\n+EMAIL_HOST_USER = ''\n+EMAIL_HOST_PASSWORD = ''\n+EMAIL_USE_TLS = False\n+EMAIL_USE_SSL = False\n+EMAIL_SSL_CERTFILE = None\n+EMAIL_SSL_KEYFILE = None\n+EMAIL_TIMEOUT = None\n+\n+# List of strings representing installed apps.\n+INSTALLED_APPS = []\n+\n+TEMPLATES = []\n+\n+# Default form rendering class.\n+FORM_RENDERER = 'django.forms.renderers.DjangoTemplates'\n+\n+# Default email address to use for various automated correspondence from\n+# the site managers.\n+DEFAULT_FROM_EMAIL = 'webmaster@localhost'\n+\n+# Subject-line prefix for email messages send with django.core.mail.mail_admins\n+# or ...mail_managers.  Make sure to include the trailing space.\n+EMAIL_SUBJECT_PREFIX = '[Django] '\n+\n+# Whether to append trailing slashes to URLs.\n+APPEND_SLASH = True\n+\n+# Whether to prepend the \"www.\" subdomain to URLs that don't have it.\n+PREPEND_WWW = False\n+\n+# Override the server-derived value of SCRIPT_NAME\n+FORCE_SCRIPT_NAME = None\n+\n+# List of compiled regular expression objects representing User-Agent strings\n+# that are not allowed to visit any page, systemwide. Use this for bad\n+# robots/crawlers. Here are a few examples:\n+#     import re\n+#     DISALLOWED_USER_AGENTS = [\n+#         re.compile(r'^NaverBot.*'),\n+#         re.compile(r'^EmailSiphon.*'),\n+#         re.compile(r'^SiteSucker.*'),\n+#         re.compile(r'^sohu-search'),\n+#     ]\n+DISALLOWED_USER_AGENTS = []\n+\n+ABSOLUTE_URL_OVERRIDES = {}\n+\n+# List of compiled regular expression objects representing URLs that need not\n+# be reported by BrokenLinkEmailsMiddleware. Here are a few examples:\n+#    import re\n+#    IGNORABLE_404_URLS = [\n+#        re.compile(r'^/apple-touch-icon.*\\.png$'),\n+#        re.compile(r'^/favicon.ico$'),\n+#        re.compile(r'^/robots.txt$'),\n+#        re.compile(r'^/phpmyadmin/'),\n+#        re.compile(r'\\.(cgi|php|pl)$'),\n+#    ]\n+IGNORABLE_404_URLS = []\n+\n+# A secret key for this particular Django installation. Used in secret-key\n+# hashing algorithms. Set this in your settings, or Django will complain\n+# loudly.\n+SECRET_KEY = ''\n+\n+# Default file storage mechanism that holds media.\n+DEFAULT_FILE_STORAGE = 'django.core.files.storage.FileSystemStorage'\n+\n+# Absolute filesystem path to the directory that will hold user-uploaded files.\n+# Example: \"/var/www/example.com/media/\"\n+MEDIA_ROOT = ''\n+\n+# URL that handles the media served from MEDIA_ROOT.\n+# Examples: \"http://example.com/media/\", \"http://media.example.com/\"\n+MEDIA_URL = ''\n+\n+# Absolute path to the directory static files should be collected to.\n+# Example: \"/var/www/example.com/static/\"\n+STATIC_ROOT = None\n+\n+# URL that handles the static files served from STATIC_ROOT.\n+# Example: \"http://example.com/static/\", \"http://static.example.com/\"\n+STATIC_URL = None\n+\n+# List of upload handler classes to be applied in order.\n+FILE_UPLOAD_HANDLERS = [\n+    'django.core.files.uploadhandler.MemoryFileUploadHandler',\n+    'django.core.files.uploadhandler.TemporaryFileUploadHandler',\n+]\n+\n+# Maximum size, in bytes, of a request before it will be streamed to the\n+# file system instead of into memory.\n+FILE_UPLOAD_MAX_MEMORY_SIZE = 2621440  # i.e. 2.5 MB\n+\n+# Maximum size in bytes of request data (excluding file uploads) that will be\n+# read before a SuspiciousOperation (RequestDataTooBig) is raised.\n+DATA_UPLOAD_MAX_MEMORY_SIZE = 2621440  # i.e. 2.5 MB\n+\n+# Maximum number of GET/POST parameters that will be read before a\n+# SuspiciousOperation (TooManyFieldsSent) is raised.\n+DATA_UPLOAD_MAX_NUMBER_FIELDS = 1000\n+\n+# Directory in which upload streamed files will be temporarily saved. A value of\n+# `None` will make Django use the operating system's default temporary directory\n+# (i.e. \"/tmp\" on *nix systems).\n+FILE_UPLOAD_TEMP_DIR = None\n+\n+# The numeric mode to set newly-uploaded files to. The value should be a mode\n+# you'd pass directly to os.chmod; see https://docs.python.org/library/os.html#files-and-directories.\n+FILE_UPLOAD_PERMISSIONS = None\n+\n+# The numeric mode to assign to newly-created directories, when uploading files.\n+# The value should be a mode as you'd pass to os.chmod;\n+# see https://docs.python.org/library/os.html#files-and-directories.\n+FILE_UPLOAD_DIRECTORY_PERMISSIONS = None\n+\n+# Python module path where user will place custom format definition.\n+# The directory where this setting is pointing should contain subdirectories\n+# named as the locales, containing a formats.py file\n+# (i.e. \"myproject.locale\" for myproject/locale/en/formats.py etc. use)\n+FORMAT_MODULE_PATH = None\n+\n+# Default formatting for date objects. See all available format strings here:\n+# https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date\n+DATE_FORMAT = 'N j, Y'\n+\n+# Default formatting for datetime objects. See all available format strings here:\n+# https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date\n+DATETIME_FORMAT = 'N j, Y, P'\n+\n+# Default formatting for time objects. See all available format strings here:\n+# https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date\n+TIME_FORMAT = 'P'\n+\n+# Default formatting for date objects when only the year and month are relevant.\n+# See all available format strings here:\n+# https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date\n+YEAR_MONTH_FORMAT = 'F Y'\n+\n+# Default formatting for date objects when only the month and day are relevant.\n+# See all available format strings here:\n+# https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date\n+MONTH_DAY_FORMAT = 'F j'\n+\n+# Default short formatting for date objects. See all available format strings here:\n+# https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date\n+SHORT_DATE_FORMAT = 'm/d/Y'\n+\n+# Default short formatting for datetime objects.\n+# See all available format strings here:\n+# https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date\n+SHORT_DATETIME_FORMAT = 'm/d/Y P'\n+\n+# Default formats to be used when parsing dates from input boxes, in order\n+# See all available format string here:\n+# https://docs.python.org/library/datetime.html#strftime-behavior\n+# * Note that these format strings are different from the ones to display dates\n+DATE_INPUT_FORMATS = [\n+    '%Y-%m-%d', '%m/%d/%Y', '%m/%d/%y',  # '2006-10-25', '10/25/2006', '10/25/06'\n+    '%b %d %Y', '%b %d, %Y',             # 'Oct 25 2006', 'Oct 25, 2006'\n+    '%d %b %Y', '%d %b, %Y',             # '25 Oct 2006', '25 Oct, 2006'\n+    '%B %d %Y', '%B %d, %Y',             # 'October 25 2006', 'October 25, 2006'\n+    '%d %B %Y', '%d %B, %Y',             # '25 October 2006', '25 October, 2006'\n+]\n+\n+# Default formats to be used when parsing times from input boxes, in order\n+# See all available format string here:\n+# https://docs.python.org/library/datetime.html#strftime-behavior\n+# * Note that these format strings are different from the ones to display dates\n+TIME_INPUT_FORMATS = [\n+    '%H:%M:%S',     # '14:30:59'\n+    '%H:%M:%S.%f',  # '14:30:59.000200'\n+    '%H:%M',        # '14:30'\n+]\n+\n+# Default formats to be used when parsing dates and times from input boxes,\n+# in order\n+# See all available format string here:\n+# https://docs.python.org/library/datetime.html#strftime-behavior\n+# * Note that these format strings are different from the ones to display dates\n+DATETIME_INPUT_FORMATS = [\n+    '%Y-%m-%d %H:%M:%S',     # '2006-10-25 14:30:59'\n+    '%Y-%m-%d %H:%M:%S.%f',  # '2006-10-25 14:30:59.000200'\n+    '%Y-%m-%d %H:%M',        # '2006-10-25 14:30'\n+    '%Y-%m-%d',              # '2006-10-25'\n+    '%m/%d/%Y %H:%M:%S',     # '10/25/2006 14:30:59'\n+    '%m/%d/%Y %H:%M:%S.%f',  # '10/25/2006 14:30:59.000200'\n+    '%m/%d/%Y %H:%M',        # '10/25/2006 14:30'\n+    '%m/%d/%Y',              # '10/25/2006'\n+    '%m/%d/%y %H:%M:%S',     # '10/25/06 14:30:59'\n+    '%m/%d/%y %H:%M:%S.%f',  # '10/25/06 14:30:59.000200'\n+    '%m/%d/%y %H:%M',        # '10/25/06 14:30'\n+    '%m/%d/%y',              # '10/25/06'\n+]\n+\n+# First day of week, to be used on calendars\n+# 0 means Sunday, 1 means Monday...\n+FIRST_DAY_OF_WEEK = 0\n+\n+# Decimal separator symbol\n+DECIMAL_SEPARATOR = '.'\n+\n+# Boolean that sets whether to add thousand separator when formatting numbers\n+USE_THOUSAND_SEPARATOR = False\n+\n+# Number of digits that will be together, when splitting them by\n+# THOUSAND_SEPARATOR. 0 means no grouping, 3 means splitting by thousands...\n+NUMBER_GROUPING = 0\n+\n+# Thousand separator symbol\n+THOUSAND_SEPARATOR = ','\n+\n+# The tablespaces to use for each model when not specified otherwise.\n+DEFAULT_TABLESPACE = ''\n+DEFAULT_INDEX_TABLESPACE = ''\n+\n+# Default X-Frame-Options header value\n+X_FRAME_OPTIONS = 'SAMEORIGIN'\n+\n+USE_X_FORWARDED_HOST = False\n+USE_X_FORWARDED_PORT = False\n+\n+# The Python dotted path to the WSGI application that Django's internal server\n+# (runserver) will use. If `None`, the return value of\n+# 'django.core.wsgi.get_wsgi_application' is used, thus preserving the same\n+# behavior as previous versions of Django. Otherwise this should point to an\n+# actual WSGI application object.\n+WSGI_APPLICATION = None\n+\n+# If your Django app is behind a proxy that sets a header to specify secure\n+# connections, AND that proxy ensures that user-submitted headers with the\n+# same name are ignored (so that people can't spoof it), set this value to\n+# a tuple of (header_name, header_value). For any requests that come in with\n+# that header/value, request.is_secure() will return True.\n+# WARNING! Only set this if you fully understand what you're doing. Otherwise,\n+# you may be opening yourself up to a security risk.\n+SECURE_PROXY_SSL_HEADER = None\n+\n+##############\n+# MIDDLEWARE #\n+##############\n+\n+# List of middleware to use. Order is important; in the request phase, these\n+# middleware will be applied in the order given, and in the response\n+# phase the middleware will be applied in reverse order.\n+MIDDLEWARE = []\n+\n+############\n+# SESSIONS #\n+############\n+\n+# Cache to store session data if using the cache session backend.\n+SESSION_CACHE_ALIAS = 'default'\n+# Cookie name. This can be whatever you want.\n+SESSION_COOKIE_NAME = 'sessionid'\n+# Age of cookie, in seconds (default: 2 weeks).\n+SESSION_COOKIE_AGE = 60 * 60 * 24 * 7 * 2\n+# A string like \"example.com\", or None for standard domain cookie.\n+SESSION_COOKIE_DOMAIN = None\n+# Whether the session cookie should be secure (https:// only).\n+SESSION_COOKIE_SECURE = False\n+# The path of the session cookie.\n+SESSION_COOKIE_PATH = '/'\n+# Whether to use the non-RFC standard httpOnly flag (IE, FF3+, others)\n+SESSION_COOKIE_HTTPONLY = True\n+# Whether to set the flag restricting cookie leaks on cross-site requests.\n+# This can be 'Lax', 'Strict', or None to disable the flag.\n+SESSION_COOKIE_SAMESITE = 'Lax'\n+# Whether to save the session data on every request.\n+SESSION_SAVE_EVERY_REQUEST = False\n+# Whether a user's session cookie expires when the Web browser is closed.\n+SESSION_EXPIRE_AT_BROWSER_CLOSE = False\n+# The module to store session data\n+SESSION_ENGINE = 'django.contrib.sessions.backends.db'\n+# Directory to store session files if using the file session module. If None,\n+# the backend will use a sensible default.\n+SESSION_FILE_PATH = None\n+# class to serialize session data\n+SESSION_SERIALIZER = 'django.contrib.sessions.serializers.JSONSerializer'\n+\n+#########\n+# CACHE #\n+#########\n+\n+# The cache backends to use.\n+CACHES = {\n+    'default': {\n+        'BACKEND': 'django.core.cache.backends.locmem.LocMemCache',\n+    }\n+}\n+CACHE_MIDDLEWARE_KEY_PREFIX = ''\n+CACHE_MIDDLEWARE_SECONDS = 600\n+CACHE_MIDDLEWARE_ALIAS = 'default'\n+\n+##################\n+# AUTHENTICATION #\n+##################\n+\n+AUTH_USER_MODEL = 'auth.User'\n+\n+AUTHENTICATION_BACKENDS = ['django.contrib.auth.backends.ModelBackend']\n+\n+LOGIN_URL = '/accounts/login/'\n+\n+LOGIN_REDIRECT_URL = '/accounts/profile/'\n+\n+LOGOUT_REDIRECT_URL = None\n+\n+# The number of days a password reset link is valid for\n+PASSWORD_RESET_TIMEOUT_DAYS = 3\n+\n+# the first hasher in this list is the preferred algorithm.  any\n+# password using different algorithms will be converted automatically\n+# upon login\n+PASSWORD_HASHERS = [\n+    'django.contrib.auth.hashers.PBKDF2PasswordHasher',\n+    'django.contrib.auth.hashers.PBKDF2SHA1PasswordHasher',\n+    'django.contrib.auth.hashers.Argon2PasswordHasher',\n+    'django.contrib.auth.hashers.BCryptSHA256PasswordHasher',\n+]\n+\n+AUTH_PASSWORD_VALIDATORS = []\n+\n+###########\n+# SIGNING #\n+###########\n+\n+SIGNING_BACKEND = 'django.core.signing.TimestampSigner'\n+\n+########\n+# CSRF #\n+########\n+\n+# Dotted path to callable to be used as view when a request is\n+# rejected by the CSRF middleware.\n+CSRF_FAILURE_VIEW = 'django.views.csrf.csrf_failure'\n+\n+# Settings for CSRF cookie.\n+CSRF_COOKIE_NAME = 'csrftoken'\n+CSRF_COOKIE_AGE = 60 * 60 * 24 * 7 * 52\n+CSRF_COOKIE_DOMAIN = None\n+CSRF_COOKIE_PATH = '/'\n+CSRF_COOKIE_SECURE = False\n+CSRF_COOKIE_HTTPONLY = False\n+CSRF_COOKIE_SAMESITE = 'Lax'\n+CSRF_HEADER_NAME = 'HTTP_X_CSRFTOKEN'\n+CSRF_TRUSTED_ORIGINS = []\n+CSRF_USE_SESSIONS = False\n+\n+############\n+# MESSAGES #\n+############\n+\n+# Class to use as messages backend\n+MESSAGE_STORAGE = 'django.contrib.messages.storage.fallback.FallbackStorage'\n+\n+# Default values of MESSAGE_LEVEL and MESSAGE_TAGS are defined within\n+# django.contrib.messages to avoid imports in this settings file.\n+\n+###########\n+# LOGGING #\n+###########\n+\n+# The callable to use to configure logging\n+LOGGING_CONFIG = 'logging.config.dictConfig'\n+\n+# Custom logging configuration.\n+LOGGING = {}\n+\n+# Default exception reporter filter class used in case none has been\n+# specifically assigned to the HttpRequest instance.\n+DEFAULT_EXCEPTION_REPORTER_FILTER = 'django.views.debug.SafeExceptionReporterFilter'\n+\n+###########\n+# TESTING #\n+###########\n+\n+# The name of the class to use to run the test suite\n+TEST_RUNNER = 'django.test.runner.DiscoverRunner'\n+\n+# Apps that don't need to be serialized at test database creation time\n+# (only apps with migrations are to start with)\n+TEST_NON_SERIALIZED_APPS = []\n+\n+############\n+# FIXTURES #\n+############\n+\n+# The list of directories to search for fixtures\n+FIXTURE_DIRS = []\n+\n+###############\n+# STATICFILES #\n+###############\n+\n+# A list of locations of additional static files\n+STATICFILES_DIRS = []\n+\n+# The default file storage backend used during the build process\n+STATICFILES_STORAGE = 'django.contrib.staticfiles.storage.StaticFilesStorage'\n+\n+# List of finder classes that know how to find static files in\n+# various locations.\n+STATICFILES_FINDERS = [\n+    'django.contrib.staticfiles.finders.FileSystemFinder',\n+    'django.contrib.staticfiles.finders.AppDirectoriesFinder',\n+    # 'django.contrib.staticfiles.finders.DefaultStorageFinder',\n+]\n+\n+##############\n+# MIGRATIONS #\n+##############\n+\n+# Migration module overrides for apps, by app label.\n+MIGRATION_MODULES = {}\n+\n+#################\n+# SYSTEM CHECKS #\n+#################\n+\n+# List of all issues generated by system checks that should be silenced. Light\n+# issues like warnings, infos or debugs will not generate a message. Silencing\n+# serious issues like errors and criticals does not result in hiding the\n+# message, but Django will not stop you from e.g. running server.\n+SILENCED_SYSTEM_CHECKS = []\n+\n+#######################\n+# SECURITY MIDDLEWARE #\n+#######################\n+SECURE_BROWSER_XSS_FILTER = False\n+SECURE_CONTENT_TYPE_NOSNIFF = False\n+SECURE_HSTS_INCLUDE_SUBDOMAINS = False\n+SECURE_HSTS_PRELOAD = False\n+SECURE_HSTS_SECONDS = 0\n+SECURE_REDIRECT_EXEMPT = []\n+SECURE_SSL_HOST = None\n+SECURE_SSL_REDIRECT = False\ndiff --git a/.harness/checkpoints/9c5dfe39b5aa/tests--file_storage--tests.py b/.harness/checkpoints/9c5dfe39b5aa/tests--file_storage--tests.py\nnew file mode 100644\nindex 0000000000..434869554c\n--- /dev/null\n+++ b/.harness/checkpoints/9c5dfe39b5aa/tests--file_storage--tests.py\n@@ -0,0 +1,998 @@\n+import os\n+import shutil\n+import sys\n+import tempfile\n+import threading\n+import time\n+import unittest\n+from datetime import datetime, timedelta\n+from io import StringIO\n+from urllib.request import urlopen\n+\n+from django.core.cache import cache\n+from django.core.exceptions import SuspiciousFileOperation\n+from django.core.files.base import ContentFile, File\n+from django.core.files.storage import FileSystemStorage, get_storage_class\n+from django.core.files.uploadedfile import (\n+    InMemoryUploadedFile, SimpleUploadedFile, TemporaryUploadedFile,\n+)\n+from django.db.models.fields.files import FileDescriptor\n+from django.test import (\n+    LiveServerTestCase, SimpleTestCase, TestCase, override_settings,\n+)\n+from django.test.utils import requires_tz_support\n+from django.urls import NoReverseMatch, reverse_lazy\n+from django.utils import timezone\n+\n+from .models import Storage, temp_storage, temp_storage_location\n+\n+FILE_SUFFIX_REGEX = '[A-Za-z0-9]{7}'\n+\n+\n+class GetStorageClassTests(SimpleTestCase):\n+\n+    def test_get_filesystem_storage(self):\n+        \"\"\"\n+        get_storage_class returns the class for a storage backend name/path.\n+        \"\"\"\n+        self.assertEqual(\n+            get_storage_class('django.core.files.storage.FileSystemStorage'),\n+            FileSystemStorage)\n+\n+    def test_get_invalid_storage_module(self):\n+        \"\"\"\n+        get_storage_class raises an error if the requested import don't exist.\n+        \"\"\"\n+        with self.assertRaisesMessage(ImportError, \"No module named 'storage'\"):\n+            get_storage_class('storage.NonexistentStorage')\n+\n+    def test_get_nonexistent_storage_class(self):\n+        \"\"\"\n+        get_storage_class raises an error if the requested class don't exist.\n+        \"\"\"\n+        with self.assertRaises(ImportError):\n+            get_storage_class('django.core.files.storage.NonexistentStorage')\n+\n+    def test_get_nonexistent_storage_module(self):\n+        \"\"\"\n+        get_storage_class raises an error if the requested module don't exist.\n+        \"\"\"\n+        with self.assertRaisesMessage(ImportError, \"No module named 'django.core.files.nonexistent_storage'\"):\n+            get_storage_class('django.core.files.nonexistent_storage.NonexistentStorage')\n+\n+\n+class FileSystemStorageTests(unittest.TestCase):\n+\n+    def test_deconstruction(self):\n+        path, args, kwargs = temp_storage.deconstruct()\n+        self.assertEqual(path, \"django.core.files.storage.FileSystemStorage\")\n+        self.assertEqual(args, ())\n+        self.assertEqual(kwargs, {'location': temp_storage_location})\n+\n+        kwargs_orig = {\n+            'location': temp_storage_location,\n+            'base_url': 'http://myfiles.example.com/'\n+        }\n+        storage = FileSystemStorage(**kwargs_orig)\n+        path, args, kwargs = storage.deconstruct()\n+        self.assertEqual(kwargs, kwargs_orig)\n+\n+    def test_lazy_base_url_init(self):\n+        \"\"\"\n+        FileSystemStorage.__init__() shouldn't evaluate base_url.\n+        \"\"\"\n+        storage = FileSystemStorage(base_url=reverse_lazy('app:url'))\n+        with self.assertRaises(NoReverseMatch):\n+            storage.url(storage.base_url)\n+\n+\n+class FileStorageTests(SimpleTestCase):\n+    storage_class = FileSystemStorage\n+\n+    def setUp(self):\n+        self.temp_dir = tempfile.mkdtemp()\n+        self.storage = self.storage_class(location=self.temp_dir, base_url='/test_media_url/')\n+        # Set up a second temporary directory which is ensured to have a mixed\n+        # case name.\n+        self.temp_dir2 = tempfile.mkdtemp(suffix='aBc')\n+\n+    def tearDown(self):\n+        shutil.rmtree(self.temp_dir)\n+        shutil.rmtree(self.temp_dir2)\n+\n+    def test_empty_location(self):\n+        \"\"\"\n+        Makes sure an exception is raised if the location is empty\n+        \"\"\"\n+        storage = self.storage_class(location='')\n+        self.assertEqual(storage.base_location, '')\n+        self.assertEqual(storage.location, os.getcwd())\n+\n+    def test_file_access_options(self):\n+        \"\"\"\n+        Standard file access options are available, and work as expected.\n+        \"\"\"\n+        self.assertFalse(self.storage.exists('storage_test'))\n+        f = self.storage.open('storage_test', 'w')\n+        f.write('storage contents')\n+        f.close()\n+        self.assertTrue(self.storage.exists('storage_test'))\n+\n+        f = self.storage.open('storage_test', 'r')\n+        self.assertEqual(f.read(), 'storage contents')\n+        f.close()\n+\n+        self.storage.delete('storage_test')\n+        self.assertFalse(self.storage.exists('storage_test'))\n+\n+    def _test_file_time_getter(self, getter):\n+        # Check for correct behavior under both USE_TZ=True and USE_TZ=False.\n+        # The tests are similar since they both set up a situation where the\n+        # system time zone, Django's TIME_ZONE, and UTC are distinct.\n+        self._test_file_time_getter_tz_handling_on(getter)\n+        self._test_file_time_getter_tz_handling_off(getter)\n+\n+    @override_settings(USE_TZ=True, TIME_ZONE='Africa/Algiers')\n+    def _test_file_time_getter_tz_handling_on(self, getter):\n+        # Django's TZ (and hence the system TZ) is set to Africa/Algiers which\n+        # is UTC+1 and has no DST change. We can set the Django TZ to something\n+        # else so that UTC, Django's TIME_ZONE, and the system timezone are all\n+        # different.\n+        now_in_algiers = timezone.make_aware(datetime.now())\n+\n+        with timezone.override(timezone.get_fixed_timezone(-300)):\n+            # At this point the system TZ is +1 and the Django TZ\n+            # is -5. The following will be aware in UTC.\n+            now = timezone.now()\n+            self.assertFalse(self.storage.exists('test.file.tz.on'))\n+\n+            f = ContentFile('custom contents')\n+            f_name = self.storage.save('test.file.tz.on', f)\n+            self.addCleanup(self.storage.delete, f_name)\n+            dt = getter(f_name)\n+            # dt should be aware, in UTC\n+            self.assertTrue(timezone.is_aware(dt))\n+            self.assertEqual(now.tzname(), dt.tzname())\n+\n+            # The three timezones are indeed distinct.\n+            naive_now = datetime.now()\n+            algiers_offset = now_in_algiers.tzinfo.utcoffset(naive_now)\n+            django_offset = timezone.get_current_timezone().utcoffset(naive_now)\n+            utc_offset = timezone.utc.utcoffset(naive_now)\n+            self.assertGreater(algiers_offset, utc_offset)\n+            self.assertLess(django_offset, utc_offset)\n+\n+            # dt and now should be the same effective time.\n+            self.assertLess(abs(dt - now), timedelta(seconds=2))\n+\n+    @override_settings(USE_TZ=False, TIME_ZONE='Africa/Algiers')\n+    def _test_file_time_getter_tz_handling_off(self, getter):\n+        # Django's TZ (and hence the system TZ) is set to Africa/Algiers which\n+        # is UTC+1 and has no DST change. We can set the Django TZ to something\n+        # else so that UTC, Django's TIME_ZONE, and the system timezone are all\n+        # different.\n+        now_in_algiers = timezone.make_aware(datetime.now())\n+\n+        with timezone.override(timezone.get_fixed_timezone(-300)):\n+            # At this point the system TZ is +1 and the Django TZ\n+            # is -5.\n+            self.assertFalse(self.storage.exists('test.file.tz.off'))\n+\n+            f = ContentFile('custom contents')\n+            f_name = self.storage.save('test.file.tz.off', f)\n+            self.addCleanup(self.storage.delete, f_name)\n+            dt = getter(f_name)\n+            # dt should be naive, in system (+1) TZ\n+            self.assertTrue(timezone.is_naive(dt))\n+\n+            # The three timezones are indeed distinct.\n+            naive_now = datetime.now()\n+            algiers_offset = now_in_algiers.tzinfo.utcoffset(naive_now)\n+            django_offset = timezone.get_current_timezone().utcoffset(naive_now)\n+            utc_offset = timezone.utc.utcoffset(naive_now)\n+            self.assertGreater(algiers_offset, utc_offset)\n+            self.assertLess(django_offset, utc_offset)\n+\n+            # dt and naive_now should be the same effective time.\n+            self.assertLess(abs(dt - naive_now), timedelta(seconds=2))\n+            # If we convert dt to an aware object using the Algiers\n+            # timezone then it should be the same effective time to\n+            # now_in_algiers.\n+            _dt = timezone.make_aware(dt, now_in_algiers.tzinfo)\n+            self.assertLess(abs(_dt - now_in_algiers), timedelta(seconds=2))\n+\n+    def test_file_get_accessed_time(self):\n+        \"\"\"\n+        File storage returns a Datetime object for the last accessed time of\n+        a file.\n+        \"\"\"\n+        self.assertFalse(self.storage.exists('test.file'))\n+\n+        f = ContentFile('custom contents')\n+        f_name = self.storage.save('test.file', f)\n+        self.addCleanup(self.storage.delete, f_name)\n+        atime = self.storage.get_accessed_time(f_name)\n+\n+        self.assertEqual(atime, datetime.fromtimestamp(os.path.getatime(self.storage.path(f_name))))\n+        self.assertLess(timezone.now() - self.storage.get_accessed_time(f_name), timedelta(seconds=2))\n+\n+    @requires_tz_support\n+    def test_file_get_accessed_time_timezone(self):\n+        self._test_file_time_getter(self.storage.get_accessed_time)\n+\n+    def test_file_get_created_time(self):\n+        \"\"\"\n+        File storage returns a datetime for the creation time of a file.\n+        \"\"\"\n+        self.assertFalse(self.storage.exists('test.file'))\n+\n+        f = ContentFile('custom contents')\n+        f_name = self.storage.save('test.file', f)\n+        self.addCleanup(self.storage.delete, f_name)\n+        ctime = self.storage.get_created_time(f_name)\n+\n+        self.assertEqual(ctime, datetime.fromtimestamp(os.path.getctime(self.storage.path(f_name))))\n+        self.assertLess(timezone.now() - self.storage.get_created_time(f_name), timedelta(seconds=2))\n+\n+    @requires_tz_support\n+    def test_file_get_created_time_timezone(self):\n+        self._test_file_time_getter(self.storage.get_created_time)\n+\n+    def test_file_get_modified_time(self):\n+        \"\"\"\n+        File storage returns a datetime for the last modified time of a file.\n+        \"\"\"\n+        self.assertFalse(self.storage.exists('test.file'))\n+\n+        f = ContentFile('custom contents')\n+        f_name = self.storage.save('test.file', f)\n+        self.addCleanup(self.storage.delete, f_name)\n+        mtime = self.storage.get_modified_time(f_name)\n+\n+        self.assertEqual(mtime, datetime.fromtimestamp(os.path.getmtime(self.storage.path(f_name))))\n+        self.assertLess(timezone.now() - self.storage.get_modified_time(f_name), timedelta(seconds=2))\n+\n+    @requires_tz_support\n+    def test_file_get_modified_time_timezone(self):\n+        self._test_file_time_getter(self.storage.get_modified_time)\n+\n+    def test_file_save_without_name(self):\n+        \"\"\"\n+        File storage extracts the filename from the content object if no\n+        name is given explicitly.\n+        \"\"\"\n+        self.assertFalse(self.storage.exists('test.file'))\n+\n+        f = ContentFile('custom contents')\n+        f.name = 'test.file'\n+\n+        storage_f_name = self.storage.save(None, f)\n+\n+        self.assertEqual(storage_f_name, f.name)\n+\n+        self.assertTrue(os.path.exists(os.path.join(self.temp_dir, f.name)))\n+\n+        self.storage.delete(storage_f_name)\n+\n+    def test_file_save_with_path(self):\n+        \"\"\"\n+        Saving a pathname should create intermediate directories as necessary.\n+        \"\"\"\n+        self.assertFalse(self.storage.exists('path/to'))\n+        self.storage.save('path/to/test.file', ContentFile('file saved with path'))\n+\n+        self.assertTrue(self.storage.exists('path/to'))\n+        with self.storage.open('path/to/test.file') as f:\n+            self.assertEqual(f.read(), b'file saved with path')\n+\n+        self.assertTrue(os.path.exists(\n+            os.path.join(self.temp_dir, 'path', 'to', 'test.file')))\n+\n+        self.storage.delete('path/to/test.file')\n+\n+    def test_save_doesnt_close(self):\n+        with TemporaryUploadedFile('test', 'text/plain', 1, 'utf8') as file:\n+            file.write(b'1')\n+            file.seek(0)\n+            self.assertFalse(file.closed)\n+            self.storage.save('path/to/test.file', file)\n+            self.assertFalse(file.closed)\n+            self.assertFalse(file.file.closed)\n+\n+        file = InMemoryUploadedFile(StringIO('1'), '', 'test', 'text/plain', 1, 'utf8')\n+        with file:\n+            self.assertFalse(file.closed)\n+            self.storage.save('path/to/test.file', file)\n+            self.assertFalse(file.closed)\n+            self.assertFalse(file.file.closed)\n+\n+    def test_file_path(self):\n+        \"\"\"\n+        File storage returns the full path of a file\n+        \"\"\"\n+        self.assertFalse(self.storage.exists('test.file'))\n+\n+        f = ContentFile('custom contents')\n+        f_name = self.storage.save('test.file', f)\n+\n+        self.assertEqual(self.storage.path(f_name), os.path.join(self.temp_dir, f_name))\n+\n+        self.storage.delete(f_name)\n+\n+    def test_file_url(self):\n+        \"\"\"\n+        File storage returns a url to access a given file from the Web.\n+        \"\"\"\n+        self.assertEqual(self.storage.url('test.file'), self.storage.base_url + 'test.file')\n+\n+        # should encode special chars except ~!*()'\n+        # like encodeURIComponent() JavaScript function do\n+        self.assertEqual(\n+            self.storage.url(r\"~!*()'@#$%^&*abc`+ =.file\"),\n+            \"/test_media_url/~!*()'%40%23%24%25%5E%26*abc%60%2B%20%3D.file\"\n+        )\n+        self.assertEqual(self.storage.url(\"ab\\0c\"), \"/test_media_url/ab%00c\")\n+\n+        # should translate os path separator(s) to the url path separator\n+        self.assertEqual(self.storage.url(\"\"\"a/b\\\\c.file\"\"\"), \"/test_media_url/a/b/c.file\")\n+\n+        # #25905: remove leading slashes from file names to prevent unsafe url output\n+        self.assertEqual(self.storage.url(\"/evil.com\"), \"/test_media_url/evil.com\")\n+        self.assertEqual(self.storage.url(r\"\\evil.com\"), \"/test_media_url/evil.com\")\n+        self.assertEqual(self.storage.url(\"///evil.com\"), \"/test_media_url/evil.com\")\n+        self.assertEqual(self.storage.url(r\"\\\\\\evil.com\"), \"/test_media_url/evil.com\")\n+\n+        self.assertEqual(self.storage.url(None), \"/test_media_url/\")\n+\n+    def test_base_url(self):\n+        \"\"\"\n+        File storage returns a url even when its base_url is unset or modified.\n+        \"\"\"\n+        self.storage.base_url = None\n+        with self.assertRaises(ValueError):\n+            self.storage.url('test.file')\n+\n+        # #22717: missing ending slash in base_url should be auto-corrected\n+        storage = self.storage_class(location=self.temp_dir, base_url='/no_ending_slash')\n+        self.assertEqual(\n+            storage.url('test.file'),\n+            '%s%s' % (storage.base_url, 'test.file')\n+        )\n+\n+    def test_listdir(self):\n+        \"\"\"\n+        File storage returns a tuple containing directories and files.\n+        \"\"\"\n+        self.assertFalse(self.storage.exists('storage_test_1'))\n+        self.assertFalse(self.storage.exists('storage_test_2'))\n+        self.assertFalse(self.storage.exists('storage_dir_1'))\n+\n+        self.storage.save('storage_test_1', ContentFile('custom content'))\n+        self.storage.save('storage_test_2', ContentFile('custom content'))\n+        os.mkdir(os.path.join(self.temp_dir, 'storage_dir_1'))\n+\n+        dirs, files = self.storage.listdir('')\n+        self.assertEqual(set(dirs), {'storage_dir_1'})\n+        self.assertEqual(set(files), {'storage_test_1', 'storage_test_2'})\n+\n+        self.storage.delete('storage_test_1')\n+        self.storage.delete('storage_test_2')\n+        os.rmdir(os.path.join(self.temp_dir, 'storage_dir_1'))\n+\n+    def test_file_storage_prevents_directory_traversal(self):\n+        \"\"\"\n+        File storage prevents directory traversal (files can only be accessed if\n+        they're below the storage location).\n+        \"\"\"\n+        with self.assertRaises(SuspiciousFileOperation):\n+            self.storage.exists('..')\n+        with self.assertRaises(SuspiciousFileOperation):\n+            self.storage.exists('/etc/passwd')\n+\n+    def test_file_storage_preserves_filename_case(self):\n+        \"\"\"The storage backend should preserve case of filenames.\"\"\"\n+        # Create a storage backend associated with the mixed case name\n+        # directory.\n+        other_temp_storage = self.storage_class(location=self.temp_dir2)\n+        # Ask that storage backend to store a file with a mixed case filename.\n+        mixed_case = 'CaSe_SeNsItIvE'\n+        file = other_temp_storage.open(mixed_case, 'w')\n+        file.write('storage contents')\n+        file.close()\n+        self.assertEqual(os.path.join(self.temp_dir2, mixed_case), other_temp_storage.path(mixed_case))\n+        other_temp_storage.delete(mixed_case)\n+\n+    def test_makedirs_race_handling(self):\n+        \"\"\"\n+        File storage should be robust against directory creation race conditions.\n+        \"\"\"\n+        real_makedirs = os.makedirs\n+\n+        # Monkey-patch os.makedirs, to simulate a normal call, a raced call,\n+        # and an error.\n+        def fake_makedirs(path, mode=0o777, exist_ok=False):\n+            if path == os.path.join(self.temp_dir, 'normal'):\n+                real_makedirs(path, mode, exist_ok)\n+            elif path == os.path.join(self.temp_dir, 'raced'):\n+                real_makedirs(path, mode, exist_ok)\n+                if not exist_ok:\n+                    raise FileExistsError()\n+            elif path == os.path.join(self.temp_dir, 'error'):\n+                raise PermissionError()\n+            else:\n+                self.fail('unexpected argument %r' % path)\n+\n+        try:\n+            os.makedirs = fake_makedirs\n+\n+            self.storage.save('normal/test.file', ContentFile('saved normally'))\n+            with self.storage.open('normal/test.file') as f:\n+                self.assertEqual(f.read(), b'saved normally')\n+\n+            self.storage.save('raced/test.file', ContentFile('saved with race'))\n+            with self.storage.open('raced/test.file') as f:\n+                self.assertEqual(f.read(), b'saved with race')\n+\n+            # Exceptions aside from FileExistsError are raised.\n+            with self.assertRaises(PermissionError):\n+                self.storage.save('error/test.file', ContentFile('not saved'))\n+        finally:\n+            os.makedirs = real_makedirs\n+\n+    def test_remove_race_handling(self):\n+        \"\"\"\n+        File storage should be robust against file removal race conditions.\n+        \"\"\"\n+        real_remove = os.remove\n+\n+        # Monkey-patch os.remove, to simulate a normal call, a raced call,\n+        # and an error.\n+        def fake_remove(path):\n+            if path == os.path.join(self.temp_dir, 'normal.file'):\n+                real_remove(path)\n+            elif path == os.path.join(self.temp_dir, 'raced.file'):\n+                real_remove(path)\n+                raise FileNotFoundError()\n+            elif path == os.path.join(self.temp_dir, 'error.file'):\n+                raise PermissionError()\n+            else:\n+                self.fail('unexpected argument %r' % path)\n+\n+        try:\n+            os.remove = fake_remove\n+\n+            self.storage.save('normal.file', ContentFile('delete normally'))\n+            self.storage.delete('normal.file')\n+            self.assertFalse(self.storage.exists('normal.file'))\n+\n+            self.storage.save('raced.file', ContentFile('delete with race'))\n+            self.storage.delete('raced.file')\n+            self.assertFalse(self.storage.exists('normal.file'))\n+\n+            # Exceptions aside from FileNotFoundError are raised.\n+            self.storage.save('error.file', ContentFile('delete with error'))\n+            with self.assertRaises(PermissionError):\n+                self.storage.delete('error.file')\n+        finally:\n+            os.remove = real_remove\n+\n+    def test_file_chunks_error(self):\n+        \"\"\"\n+        Test behavior when file.chunks() is raising an error\n+        \"\"\"\n+        f1 = ContentFile('chunks fails')\n+\n+        def failing_chunks():\n+            raise OSError\n+        f1.chunks = failing_chunks\n+        with self.assertRaises(OSError):\n+            self.storage.save('error.file', f1)\n+\n+    def test_delete_no_name(self):\n+        \"\"\"\n+        Calling delete with an empty name should not try to remove the base\n+        storage directory, but fail loudly (#20660).\n+        \"\"\"\n+        with self.assertRaises(AssertionError):\n+            self.storage.delete('')\n+\n+    def test_delete_deletes_directories(self):\n+        tmp_dir = tempfile.mkdtemp(dir=self.storage.location)\n+        self.storage.delete(tmp_dir)\n+        self.assertFalse(os.path.exists(tmp_dir))\n+\n+    @override_settings(\n+        MEDIA_ROOT='media_root',\n+        MEDIA_URL='media_url/',\n+        FILE_UPLOAD_PERMISSIONS=0o777,\n+        FILE_UPLOAD_DIRECTORY_PERMISSIONS=0o777,\n+    )\n+    def test_setting_changed(self):\n+        \"\"\"\n+        Properties using settings values as defaults should be updated on\n+        referenced settings change while specified values should be unchanged.\n+        \"\"\"\n+        storage = self.storage_class(\n+            location='explicit_location',\n+            base_url='explicit_base_url/',\n+            file_permissions_mode=0o666,\n+            directory_permissions_mode=0o666,\n+        )\n+        defaults_storage = self.storage_class()\n+        settings = {\n+            'MEDIA_ROOT': 'overriden_media_root',\n+            'MEDIA_URL': 'overriden_media_url/',\n+            'FILE_UPLOAD_PERMISSIONS': 0o333,\n+            'FILE_UPLOAD_DIRECTORY_PERMISSIONS': 0o333,\n+        }\n+        with self.settings(**settings):\n+            self.assertEqual(storage.base_location, 'explicit_location')\n+            self.assertIn('explicit_location', storage.location)\n+            self.assertEqual(storage.base_url, 'explicit_base_url/')\n+            self.assertEqual(storage.file_permissions_mode, 0o666)\n+            self.assertEqual(storage.directory_permissions_mode, 0o666)\n+            self.assertEqual(defaults_storage.base_location, settings['MEDIA_ROOT'])\n+            self.assertIn(settings['MEDIA_ROOT'], defaults_storage.location)\n+            self.assertEqual(defaults_storage.base_url, settings['MEDIA_URL'])\n+            self.assertEqual(defaults_storage.file_permissions_mode, settings['FILE_UPLOAD_PERMISSIONS'])\n+            self.assertEqual(\n+                defaults_storage.directory_permissions_mode, settings['FILE_UPLOAD_DIRECTORY_PERMISSIONS']\n+            )\n+\n+\n+class CustomStorage(FileSystemStorage):\n+    def get_available_name(self, name, max_length=None):\n+        \"\"\"\n+        Append numbers to duplicate files rather than underscores, like Trac.\n+        \"\"\"\n+        basename, *ext = name.split('.')\n+        number = 2\n+        while self.exists(name):\n+            name = '.'.join([basename, str(number)] + ext)\n+            number += 1\n+\n+        return name\n+\n+\n+class CustomStorageTests(FileStorageTests):\n+    storage_class = CustomStorage\n+\n+    def test_custom_get_available_name(self):\n+        first = self.storage.save('custom_storage', ContentFile('custom contents'))\n+        self.assertEqual(first, 'custom_storage')\n+        second = self.storage.save('custom_storage', ContentFile('more contents'))\n+        self.assertEqual(second, 'custom_storage.2')\n+        self.storage.delete(first)\n+        self.storage.delete(second)\n+\n+\n+class OverwritingStorage(FileSystemStorage):\n+    \"\"\"\n+    Overwrite existing files instead of appending a suffix to generate an\n+    unused name.\n+    \"\"\"\n+    # Mask out O_EXCL so os.open() doesn't raise OSError if the file exists.\n+    OS_OPEN_FLAGS = FileSystemStorage.OS_OPEN_FLAGS & ~os.O_EXCL\n+\n+    def get_available_name(self, name, max_length=None):\n+        \"\"\"Override the effort to find an used name.\"\"\"\n+        return name\n+\n+\n+class OverwritingStorageTests(FileStorageTests):\n+    storage_class = OverwritingStorage\n+\n+    def test_save_overwrite_behavior(self):\n+        \"\"\"Saving to same file name twice overwrites the first file.\"\"\"\n+        name = 'test.file'\n+        self.assertFalse(self.storage.exists(name))\n+        content_1 = b'content one'\n+        content_2 = b'second content'\n+        f_1 = ContentFile(content_1)\n+        f_2 = ContentFile(content_2)\n+        stored_name_1 = self.storage.save(name, f_1)\n+        try:\n+            self.assertEqual(stored_name_1, name)\n+            self.assertTrue(self.storage.exists(name))\n+            self.assertTrue(os.path.exists(os.path.join(self.temp_dir, name)))\n+            with self.storage.open(name) as fp:\n+                self.assertEqual(fp.read(), content_1)\n+            stored_name_2 = self.storage.save(name, f_2)\n+            self.assertEqual(stored_name_2, name)\n+            self.assertTrue(self.storage.exists(name))\n+            self.assertTrue(os.path.exists(os.path.join(self.temp_dir, name)))\n+            with self.storage.open(name) as fp:\n+                self.assertEqual(fp.read(), content_2)\n+        finally:\n+            self.storage.delete(name)\n+\n+\n+class DiscardingFalseContentStorage(FileSystemStorage):\n+    def _save(self, name, content):\n+        if content:\n+            return super()._save(name, content)\n+        return ''\n+\n+\n+class DiscardingFalseContentStorageTests(FileStorageTests):\n+    storage_class = DiscardingFalseContentStorage\n+\n+    def test_custom_storage_discarding_empty_content(self):\n+        \"\"\"\n+        When Storage.save() wraps a file-like object in File, it should include\n+        the name argument so that bool(file) evaluates to True (#26495).\n+        \"\"\"\n+        output = StringIO('content')\n+        self.storage.save('tests/stringio', output)\n+        self.assertTrue(self.storage.exists('tests/stringio'))\n+\n+        with self.storage.open('tests/stringio') as f:\n+            self.assertEqual(f.read(), b'content')\n+\n+\n+class FileFieldStorageTests(TestCase):\n+    def tearDown(self):\n+        shutil.rmtree(temp_storage_location)\n+\n+    def _storage_max_filename_length(self, storage):\n+        \"\"\"\n+        Query filesystem for maximum filename length (e.g. AUFS has 242).\n+        \"\"\"\n+        dir_to_test = storage.location\n+        while not os.path.exists(dir_to_test):\n+            dir_to_test = os.path.dirname(dir_to_test)\n+        try:\n+            return os.pathconf(dir_to_test, 'PC_NAME_MAX')\n+        except Exception:\n+            return 255  # Should be safe on most backends\n+\n+    def test_files(self):\n+        self.assertIsInstance(Storage.normal, FileDescriptor)\n+\n+        # An object without a file has limited functionality.\n+        obj1 = Storage()\n+        self.assertEqual(obj1.normal.name, \"\")\n+        with self.assertRaises(ValueError):\n+            obj1.normal.size\n+\n+        # Saving a file enables full functionality.\n+        obj1.normal.save(\"django_test.txt\", ContentFile(\"content\"))\n+        self.assertEqual(obj1.normal.name, \"tests/django_test.txt\")\n+        self.assertEqual(obj1.normal.size, 7)\n+        self.assertEqual(obj1.normal.read(), b\"content\")\n+        obj1.normal.close()\n+\n+        # File objects can be assigned to FileField attributes, but shouldn't\n+        # get committed until the model it's attached to is saved.\n+        obj1.normal = SimpleUploadedFile(\"assignment.txt\", b\"content\")\n+        dirs, files = temp_storage.listdir(\"tests\")\n+        self.assertEqual(dirs, [])\n+        self.assertNotIn(\"assignment.txt\", files)\n+\n+        obj1.save()\n+        dirs, files = temp_storage.listdir(\"tests\")\n+        self.assertEqual(sorted(files), [\"assignment.txt\", \"django_test.txt\"])\n+\n+        # Save another file with the same name.\n+        obj2 = Storage()\n+        obj2.normal.save(\"django_test.txt\", ContentFile(\"more content\"))\n+        obj2_name = obj2.normal.name\n+        self.assertRegex(obj2_name, \"tests/django_test_%s.txt\" % FILE_SUFFIX_REGEX)\n+        self.assertEqual(obj2.normal.size, 12)\n+        obj2.normal.close()\n+\n+        # Deleting an object does not delete the file it uses.\n+        obj2.delete()\n+        obj2.normal.save(\"django_test.txt\", ContentFile(\"more content\"))\n+        self.assertNotEqual(obj2_name, obj2.normal.name)\n+        self.assertRegex(obj2.normal.name, \"tests/django_test_%s.txt\" % FILE_SUFFIX_REGEX)\n+        obj2.normal.close()\n+\n+    def test_filefield_read(self):\n+        # Files can be read in a little at a time, if necessary.\n+        obj = Storage.objects.create(\n+            normal=SimpleUploadedFile(\"assignment.txt\", b\"content\"))\n+        obj.normal.open()\n+        self.assertEqual(obj.normal.read(3), b\"con\")\n+        self.assertEqual(obj.normal.read(), b\"tent\")\n+        self.assertEqual(list(obj.normal.chunks(chunk_size=2)), [b\"co\", b\"nt\", b\"en\", b\"t\"])\n+        obj.normal.close()\n+\n+    def test_filefield_write(self):\n+        # Files can be written to.\n+        obj = Storage.objects.create(normal=SimpleUploadedFile('rewritten.txt', b'content'))\n+        with obj.normal as normal:\n+            normal.open('wb')\n+            normal.write(b'updated')\n+        obj.refresh_from_db()\n+        self.assertEqual(obj.normal.read(), b'updated')\n+        obj.normal.close()\n+\n+    def test_filefield_reopen(self):\n+        obj = Storage.objects.create(normal=SimpleUploadedFile('reopen.txt', b'content'))\n+        with obj.normal as normal:\n+            normal.open()\n+        obj.normal.open()\n+        obj.normal.file.seek(0)\n+        obj.normal.close()\n+\n+    def test_duplicate_filename(self):\n+        # Multiple files with the same name get _(7 random chars) appended to them.\n+        objs = [Storage() for i in range(2)]\n+        for o in objs:\n+            o.normal.save(\"multiple_files.txt\", ContentFile(\"Same Content\"))\n+        try:\n+            names = [o.normal.name for o in objs]\n+            self.assertEqual(names[0], \"tests/multiple_files.txt\")\n+            self.assertRegex(names[1], \"tests/multiple_files_%s.txt\" % FILE_SUFFIX_REGEX)\n+        finally:\n+            for o in objs:\n+                o.delete()\n+\n+    def test_file_truncation(self):\n+        # Given the max_length is limited, when multiple files get uploaded\n+        # under the same name, then the filename get truncated in order to fit\n+        # in _(7 random chars). When most of the max_length is taken by\n+        # dirname + extension and there are not enough  characters in the\n+        # filename to truncate, an exception should be raised.\n+        objs = [Storage() for i in range(2)]\n+        filename = 'filename.ext'\n+\n+        for o in objs:\n+            o.limited_length.save(filename, ContentFile('Same Content'))\n+        try:\n+            # Testing truncation.\n+            names = [o.limited_length.name for o in objs]\n+            self.assertEqual(names[0], 'tests/%s' % filename)\n+            self.assertRegex(names[1], 'tests/fi_%s.ext' % FILE_SUFFIX_REGEX)\n+\n+            # Testing exception is raised when filename is too short to truncate.\n+            filename = 'short.longext'\n+            objs[0].limited_length.save(filename, ContentFile('Same Content'))\n+            with self.assertRaisesMessage(SuspiciousFileOperation, 'Storage can not find an available filename'):\n+                objs[1].limited_length.save(*(filename, ContentFile('Same Content')))\n+        finally:\n+            for o in objs:\n+                o.delete()\n+\n+    @unittest.skipIf(\n+        sys.platform.startswith('win'),\n+        \"Windows supports at most 260 characters in a path.\",\n+    )\n+    def test_extended_length_storage(self):\n+        # Testing FileField with max_length > 255. Most systems have filename\n+        # length limitation of 255. Path takes extra chars.\n+        filename = (self._storage_max_filename_length(temp_storage) - 4) * 'a'  # 4 chars for extension.\n+        obj = Storage()\n+        obj.extended_length.save('%s.txt' % filename, ContentFile('Same Content'))\n+        self.assertEqual(obj.extended_length.name, 'tests/%s.txt' % filename)\n+        self.assertEqual(obj.extended_length.read(), b'Same Content')\n+        obj.extended_length.close()\n+\n+    def test_filefield_default(self):\n+        # Default values allow an object to access a single file.\n+        temp_storage.save('tests/default.txt', ContentFile('default content'))\n+        obj = Storage.objects.create()\n+        self.assertEqual(obj.default.name, \"tests/default.txt\")\n+        self.assertEqual(obj.default.read(), b\"default content\")\n+        obj.default.close()\n+\n+        # But it shouldn't be deleted, even if there are no more objects using\n+        # it.\n+        obj.delete()\n+        obj = Storage()\n+        self.assertEqual(obj.default.read(), b\"default content\")\n+        obj.default.close()\n+\n+    def test_empty_upload_to(self):\n+        # upload_to can be empty, meaning it does not use subdirectory.\n+        obj = Storage()\n+        obj.empty.save('django_test.txt', ContentFile('more content'))\n+        self.assertEqual(obj.empty.name, \"django_test.txt\")\n+        self.assertEqual(obj.empty.read(), b\"more content\")\n+        obj.empty.close()\n+\n+    def test_random_upload_to(self):\n+        # Verify the fix for #5655, making sure the directory is only\n+        # determined once.\n+        obj = Storage()\n+        obj.random.save(\"random_file\", ContentFile(\"random content\"))\n+        self.assertTrue(obj.random.name.endswith(\"/random_file\"))\n+        obj.random.close()\n+\n+    def test_custom_valid_name_callable_upload_to(self):\n+        \"\"\"\n+        Storage.get_valid_name() should be called when upload_to is a callable.\n+        \"\"\"\n+        obj = Storage()\n+        obj.custom_valid_name.save(\"random_file\", ContentFile(\"random content\"))\n+        # CustomValidNameStorage.get_valid_name() appends '_valid' to the name\n+        self.assertTrue(obj.custom_valid_name.name.endswith(\"/random_file_valid\"))\n+        obj.custom_valid_name.close()\n+\n+    def test_filefield_pickling(self):\n+        # Push an object into the cache to make sure it pickles properly\n+        obj = Storage()\n+        obj.normal.save(\"django_test.txt\", ContentFile(\"more content\"))\n+        obj.normal.close()\n+        cache.set(\"obj\", obj)\n+        self.assertEqual(cache.get(\"obj\").normal.name, \"tests/django_test.txt\")\n+\n+    def test_file_object(self):\n+        # Create sample file\n+        temp_storage.save('tests/example.txt', ContentFile('some content'))\n+\n+        # Load it as Python file object\n+        with open(temp_storage.path('tests/example.txt')) as file_obj:\n+            # Save it using storage and read its content\n+            temp_storage.save('tests/file_obj', file_obj)\n+        self.assertTrue(temp_storage.exists('tests/file_obj'))\n+        with temp_storage.open('tests/file_obj') as f:\n+            self.assertEqual(f.read(), b'some content')\n+\n+    def test_stringio(self):\n+        # Test passing StringIO instance as content argument to save\n+        output = StringIO()\n+        output.write('content')\n+        output.seek(0)\n+\n+        # Save it and read written file\n+        temp_storage.save('tests/stringio', output)\n+        self.assertTrue(temp_storage.exists('tests/stringio'))\n+        with temp_storage.open('tests/stringio') as f:\n+            self.assertEqual(f.read(), b'content')\n+\n+\n+# Tests for a race condition on file saving (#4948).\n+# This is written in such a way that it'll always pass on platforms\n+# without threading.\n+\n+class SlowFile(ContentFile):\n+    def chunks(self):\n+        time.sleep(1)\n+        return super().chunks()\n+\n+\n+class FileSaveRaceConditionTest(SimpleTestCase):\n+    def setUp(self):\n+        self.storage_dir = tempfile.mkdtemp()\n+        self.storage = FileSystemStorage(self.storage_dir)\n+        self.thread = threading.Thread(target=self.save_file, args=['conflict'])\n+\n+    def tearDown(self):\n+        shutil.rmtree(self.storage_dir)\n+\n+    def save_file(self, name):\n+        name = self.storage.save(name, SlowFile(b\"Data\"))\n+\n+    def test_race_condition(self):\n+        self.thread.start()\n+        self.save_file('conflict')\n+        self.thread.join()\n+        files = sorted(os.listdir(self.storage_dir))\n+        self.assertEqual(files[0], 'conflict')\n+        self.assertRegex(files[1], 'conflict_%s' % FILE_SUFFIX_REGEX)\n+\n+\n+@unittest.skipIf(sys.platform.startswith('win'), \"Windows only partially supports umasks and chmod.\")\n+class FileStoragePermissions(unittest.TestCase):\n+    def setUp(self):\n+        self.umask = 0o027\n+        self.old_umask = os.umask(self.umask)\n+        self.storage_dir = tempfile.mkdtemp()\n+\n+    def tearDown(self):\n+        shutil.rmtree(self.storage_dir)\n+        os.umask(self.old_umask)\n+\n+    @override_settings(FILE_UPLOAD_PERMISSIONS=0o654)\n+    def test_file_upload_permissions(self):\n+        self.storage = FileSystemStorage(self.storage_dir)\n+        name = self.storage.save(\"the_file\", ContentFile(\"data\"))\n+        actual_mode = os.stat(self.storage.path(name))[0] & 0o777\n+        self.assertEqual(actual_mode, 0o654)\n+\n+    @override_settings(FILE_UPLOAD_PERMISSIONS=None)\n+    def test_file_upload_default_permissions(self):\n+        self.storage = FileSystemStorage(self.storage_dir)\n+        fname = self.storage.save(\"some_file\", ContentFile(\"data\"))\n+        mode = os.stat(self.storage.path(fname))[0] & 0o777\n+        self.assertEqual(mode, 0o666 & ~self.umask)\n+\n+    @override_settings(FILE_UPLOAD_DIRECTORY_PERMISSIONS=0o765)\n+    def test_file_upload_directory_permissions(self):\n+        self.storage = FileSystemStorage(self.storage_dir)\n+        name = self.storage.save(\"the_directory/the_file\", ContentFile(\"data\"))\n+        dir_mode = os.stat(os.path.dirname(self.storage.path(name)))[0] & 0o777\n+        self.assertEqual(dir_mode, 0o765)\n+\n+    @override_settings(FILE_UPLOAD_DIRECTORY_PERMISSIONS=None)\n+    def test_file_upload_directory_default_permissions(self):\n+        self.storage = FileSystemStorage(self.storage_dir)\n+        name = self.storage.save(\"the_directory/the_file\", ContentFile(\"data\"))\n+        dir_mode = os.stat(os.path.dirname(self.storage.path(name)))[0] & 0o777\n+        self.assertEqual(dir_mode, 0o777 & ~self.umask)\n+\n+\n+class FileStoragePathParsing(SimpleTestCase):\n+    def setUp(self):\n+        self.storage_dir = tempfile.mkdtemp()\n+        self.storage = FileSystemStorage(self.storage_dir)\n+\n+    def tearDown(self):\n+        shutil.rmtree(self.storage_dir)\n+\n+    def test_directory_with_dot(self):\n+        \"\"\"Regression test for #9610.\n+\n+        If the directory name contains a dot and the file name doesn't, make\n+        sure we still mangle the file name instead of the directory name.\n+        \"\"\"\n+\n+        self.storage.save('dotted.path/test', ContentFile(\"1\"))\n+        self.storage.save('dotted.path/test', ContentFile(\"2\"))\n+\n+        files = sorted(os.listdir(os.path.join(self.storage_dir, 'dotted.path')))\n+        self.assertFalse(os.path.exists(os.path.join(self.storage_dir, 'dotted_.path')))\n+        self.assertEqual(files[0], 'test')\n+        self.assertRegex(files[1], 'test_%s' % FILE_SUFFIX_REGEX)\n+\n+    def test_first_character_dot(self):\n+        \"\"\"\n+        File names with a dot as their first character don't have an extension,\n+        and the underscore should get added to the end.\n+        \"\"\"\n+        self.storage.save('dotted.path/.test', ContentFile(\"1\"))\n+        self.storage.save('dotted.path/.test', ContentFile(\"2\"))\n+\n+        files = sorted(os.listdir(os.path.join(self.storage_dir, 'dotted.path')))\n+        self.assertFalse(os.path.exists(os.path.join(self.storage_dir, 'dotted_.path')))\n+        self.assertEqual(files[0], '.test')\n+        self.assertRegex(files[1], '.test_%s' % FILE_SUFFIX_REGEX)\n+\n+\n+class ContentFileStorageTestCase(unittest.TestCase):\n+\n+    def setUp(self):\n+        self.storage_dir = tempfile.mkdtemp()\n+        self.storage = FileSystemStorage(self.storage_dir)\n+\n+    def tearDown(self):\n+        shutil.rmtree(self.storage_dir)\n+\n+    def test_content_saving(self):\n+        \"\"\"\n+        ContentFile can be saved correctly with the filesystem storage,\n+        if it was initialized with either bytes or unicode content.\n+        \"\"\"\n+        self.storage.save('bytes.txt', ContentFile(b\"content\"))\n+        self.storage.save('unicode.txt', ContentFile(\"espa\u00f1ol\"))\n+\n+\n+@override_settings(ROOT_URLCONF='file_storage.urls')\n+class FileLikeObjectTestCase(LiveServerTestCase):\n+    \"\"\"\n+    Test file-like objects (#15644).\n+    \"\"\"\n+\n+    available_apps = []\n+\n+    def setUp(self):\n+        self.temp_dir = tempfile.mkdtemp()\n+        self.storage = FileSystemStorage(location=self.temp_dir)\n+\n+    def tearDown(self):\n+        shutil.rmtree(self.temp_dir)\n+\n+    def test_urllib_request_urlopen(self):\n+        \"\"\"\n+        Test the File storage API with a file-like object coming from\n+        urllib.request.urlopen().\n+        \"\"\"\n+        file_like_object = urlopen(self.live_server_url + '/')\n+        f = File(file_like_object)\n+        stored_filename = self.storage.save(\"remote_file.html\", f)\n+\n+        remote_file = urlopen(self.live_server_url + '/')\n+        with self.storage.open(stored_filename) as stored_file:\n+            self.assertEqual(stored_file.read(), remote_file.read())\ndiff --git a/django/conf/global_settings.py b/django/conf/global_settings.py\nindex acee9887d2..bdeec80610 100644\n--- a/django/conf/global_settings.py\n+++ b/django/conf/global_settings.py\n@@ -304,7 +304,7 @@ FILE_UPLOAD_TEMP_DIR = None\n \n # The numeric mode to set newly-uploaded files to. The value should be a mode\n # you'd pass directly to os.chmod; see https://docs.python.org/library/os.html#files-and-directories.\n-FILE_UPLOAD_PERMISSIONS = None\n+FILE_UPLOAD_PERMISSIONS = 0o644\n \n # The numeric mode to assign to newly-created directories, when uploading files.\n # The value should be a mode as you'd pass to os.chmod;\ndiff --git a/tests/file_storage/tests.py b/tests/file_storage/tests.py\nindex 434869554c..ee878df332 100644\n--- a/tests/file_storage/tests.py\n+++ b/tests/file_storage/tests.py\n@@ -892,12 +892,11 @@ class FileStoragePermissions(unittest.TestCase):\n         actual_mode = os.stat(self.storage.path(name))[0] & 0o777\n         self.assertEqual(actual_mode, 0o654)\n \n-    @override_settings(FILE_UPLOAD_PERMISSIONS=None)\n     def test_file_upload_default_permissions(self):\n         self.storage = FileSystemStorage(self.storage_dir)\n         fname = self.storage.save(\"some_file\", ContentFile(\"data\"))\n         mode = os.stat(self.storage.path(fname))[0] & 0o777\n-        self.assertEqual(mode, 0o666 & ~self.umask)\n+        self.assertEqual(mode, 0o644)\n \n     @override_settings(FILE_UPLOAD_DIRECTORY_PERMISSIONS=0o765)\n     def test_file_upload_directory_permissions(self):\ndiff --git a/tests/test_utils/tests.py b/tests/test_utils/tests.py\nindex a1a113a26e..e953eb6609 100644\n--- a/tests/test_utils/tests.py\n+++ b/tests/test_utils/tests.py\n@@ -1099,7 +1099,7 @@ class OverrideSettingsTests(SimpleTestCase):\n         the file_permissions_mode attribute of\n         django.core.files.storage.default_storage.\n         \"\"\"\n-        self.assertIsNone(default_storage.file_permissions_mode)\n+        self.assertEqual(default_storage.file_permissions_mode, 0o644)\n         with self.settings(FILE_UPLOAD_PERMISSIONS=0o777):\n             self.assertEqual(default_storage.file_permissions_mode, 0o777)\n \n"}
{"instance_id": "django__django-10924", "model_name_or_path": "harness/claude-sonnet-4-6", "model_patch": "diff --git a/.harness/checkpoints/3a278b5881b0/--Users--huetuanthi--dev--agents--harness--django--db--models--fields--__init__.py b/.harness/checkpoints/3a278b5881b0/--Users--huetuanthi--dev--agents--harness--django--db--models--fields--__init__.py\nnew file mode 100644\nindex 0000000000..0c64748ccd\n--- /dev/null\n+++ b/.harness/checkpoints/3a278b5881b0/--Users--huetuanthi--dev--agents--harness--django--db--models--fields--__init__.py\n@@ -0,0 +1,2368 @@\n+import collections.abc\n+import copy\n+import datetime\n+import decimal\n+import operator\n+import uuid\n+import warnings\n+from base64 import b64decode, b64encode\n+from functools import partialmethod, total_ordering\n+\n+from django import forms\n+from django.apps import apps\n+from django.conf import settings\n+from django.core import checks, exceptions, validators\n+# When the _meta object was formalized, this exception was moved to\n+# django.core.exceptions. It is retained here for backwards compatibility\n+# purposes.\n+from django.core.exceptions import FieldDoesNotExist  # NOQA\n+from django.db import connection, connections, router\n+from django.db.models.constants import LOOKUP_SEP\n+from django.db.models.query_utils import DeferredAttribute, RegisterLookupMixin\n+from django.utils import timezone\n+from django.utils.datastructures import DictWrapper\n+from django.utils.dateparse import (\n+    parse_date, parse_datetime, parse_duration, parse_time,\n+)\n+from django.utils.duration import duration_microseconds, duration_string\n+from django.utils.functional import Promise, cached_property\n+from django.utils.ipv6 import clean_ipv6_address\n+from django.utils.itercompat import is_iterable\n+from django.utils.text import capfirst\n+from django.utils.translation import gettext_lazy as _\n+\n+__all__ = [\n+    'AutoField', 'BLANK_CHOICE_DASH', 'BigAutoField', 'BigIntegerField',\n+    'BinaryField', 'BooleanField', 'CharField', 'CommaSeparatedIntegerField',\n+    'DateField', 'DateTimeField', 'DecimalField', 'DurationField',\n+    'EmailField', 'Empty', 'Field', 'FieldDoesNotExist', 'FilePathField',\n+    'FloatField', 'GenericIPAddressField', 'IPAddressField', 'IntegerField',\n+    'NOT_PROVIDED', 'NullBooleanField', 'PositiveIntegerField',\n+    'PositiveSmallIntegerField', 'SlugField', 'SmallIntegerField', 'TextField',\n+    'TimeField', 'URLField', 'UUIDField',\n+]\n+\n+\n+class Empty:\n+    pass\n+\n+\n+class NOT_PROVIDED:\n+    pass\n+\n+\n+# The values to use for \"blank\" in SelectFields. Will be appended to the start\n+# of most \"choices\" lists.\n+BLANK_CHOICE_DASH = [(\"\", \"---------\")]\n+\n+\n+def _load_field(app_label, model_name, field_name):\n+    return apps.get_model(app_label, model_name)._meta.get_field(field_name)\n+\n+\n+# A guide to Field parameters:\n+#\n+#   * name:      The name of the field specified in the model.\n+#   * attname:   The attribute to use on the model object. This is the same as\n+#                \"name\", except in the case of ForeignKeys, where \"_id\" is\n+#                appended.\n+#   * db_column: The db_column specified in the model (or None).\n+#   * column:    The database column for this field. This is the same as\n+#                \"attname\", except if db_column is specified.\n+#\n+# Code that introspects values, or does other dynamic things, should use\n+# attname. For example, this gets the primary key value of object \"obj\":\n+#\n+#     getattr(obj, opts.pk.attname)\n+\n+def _empty(of_cls):\n+    new = Empty()\n+    new.__class__ = of_cls\n+    return new\n+\n+\n+def return_None():\n+    return None\n+\n+\n+@total_ordering\n+class Field(RegisterLookupMixin):\n+    \"\"\"Base class for all field types\"\"\"\n+\n+    # Designates whether empty strings fundamentally are allowed at the\n+    # database level.\n+    empty_strings_allowed = True\n+    empty_values = list(validators.EMPTY_VALUES)\n+\n+    # These track each time a Field instance is created. Used to retain order.\n+    # The auto_creation_counter is used for fields that Django implicitly\n+    # creates, creation_counter is used for all user-specified fields.\n+    creation_counter = 0\n+    auto_creation_counter = -1\n+    default_validators = []  # Default set of validators\n+    default_error_messages = {\n+        'invalid_choice': _('Value %(value)r is not a valid choice.'),\n+        'null': _('This field cannot be null.'),\n+        'blank': _('This field cannot be blank.'),\n+        'unique': _('%(model_name)s with this %(field_label)s '\n+                    'already exists.'),\n+        # Translators: The 'lookup_type' is one of 'date', 'year' or 'month'.\n+        # Eg: \"Title must be unique for pub_date year\"\n+        'unique_for_date': _(\"%(field_label)s must be unique for \"\n+                             \"%(date_field_label)s %(lookup_type)s.\"),\n+    }\n+    system_check_deprecated_details = None\n+    system_check_removed_details = None\n+\n+    # Field flags\n+    hidden = False\n+\n+    many_to_many = None\n+    many_to_one = None\n+    one_to_many = None\n+    one_to_one = None\n+    related_model = None\n+\n+    # Generic field type description, usually overridden by subclasses\n+    def _description(self):\n+        return _('Field of type: %(field_type)s') % {\n+            'field_type': self.__class__.__name__\n+        }\n+    description = property(_description)\n+\n+    def __init__(self, verbose_name=None, name=None, primary_key=False,\n+                 max_length=None, unique=False, blank=False, null=False,\n+                 db_index=False, rel=None, default=NOT_PROVIDED, editable=True,\n+                 serialize=True, unique_for_date=None, unique_for_month=None,\n+                 unique_for_year=None, choices=None, help_text='', db_column=None,\n+                 db_tablespace=None, auto_created=False, validators=(),\n+                 error_messages=None):\n+        self.name = name\n+        self.verbose_name = verbose_name  # May be set by set_attributes_from_name\n+        self._verbose_name = verbose_name  # Store original for deconstruction\n+        self.primary_key = primary_key\n+        self.max_length, self._unique = max_length, unique\n+        self.blank, self.null = blank, null\n+        self.remote_field = rel\n+        self.is_relation = self.remote_field is not None\n+        self.default = default\n+        self.editable = editable\n+        self.serialize = serialize\n+        self.unique_for_date = unique_for_date\n+        self.unique_for_month = unique_for_month\n+        self.unique_for_year = unique_for_year\n+        if isinstance(choices, collections.abc.Iterator):\n+            choices = list(choices)\n+        self.choices = choices\n+        self.help_text = help_text\n+        self.db_index = db_index\n+        self.db_column = db_column\n+        self._db_tablespace = db_tablespace\n+        self.auto_created = auto_created\n+\n+        # Adjust the appropriate creation counter, and save our local copy.\n+        if auto_created:\n+            self.creation_counter = Field.auto_creation_counter\n+            Field.auto_creation_counter -= 1\n+        else:\n+            self.creation_counter = Field.creation_counter\n+            Field.creation_counter += 1\n+\n+        self._validators = list(validators)  # Store for deconstruction later\n+\n+        messages = {}\n+        for c in reversed(self.__class__.__mro__):\n+            messages.update(getattr(c, 'default_error_messages', {}))\n+        messages.update(error_messages or {})\n+        self._error_messages = error_messages  # Store for deconstruction later\n+        self.error_messages = messages\n+\n+    def __str__(self):\n+        \"\"\"\n+        Return \"app_label.model_label.field_name\" for fields attached to\n+        models.\n+        \"\"\"\n+        if not hasattr(self, 'model'):\n+            return super().__str__()\n+        model = self.model\n+        app = model._meta.app_label\n+        return '%s.%s.%s' % (app, model._meta.object_name, self.name)\n+\n+    def __repr__(self):\n+        \"\"\"Display the module, class, and name of the field.\"\"\"\n+        path = '%s.%s' % (self.__class__.__module__, self.__class__.__qualname__)\n+        name = getattr(self, 'name', None)\n+        if name is not None:\n+            return '<%s: %s>' % (path, name)\n+        return '<%s>' % path\n+\n+    def check(self, **kwargs):\n+        return [\n+            *self._check_field_name(),\n+            *self._check_choices(),\n+            *self._check_db_index(),\n+            *self._check_null_allowed_for_primary_keys(),\n+            *self._check_backend_specific_checks(**kwargs),\n+            *self._check_validators(),\n+            *self._check_deprecation_details(),\n+        ]\n+\n+    def _check_field_name(self):\n+        \"\"\"\n+        Check if field name is valid, i.e. 1) does not end with an\n+        underscore, 2) does not contain \"__\" and 3) is not \"pk\".\n+        \"\"\"\n+        if self.name.endswith('_'):\n+            return [\n+                checks.Error(\n+                    'Field names must not end with an underscore.',\n+                    obj=self,\n+                    id='fields.E001',\n+                )\n+            ]\n+        elif LOOKUP_SEP in self.name:\n+            return [\n+                checks.Error(\n+                    'Field names must not contain \"%s\".' % (LOOKUP_SEP,),\n+                    obj=self,\n+                    id='fields.E002',\n+                )\n+            ]\n+        elif self.name == 'pk':\n+            return [\n+                checks.Error(\n+                    \"'pk' is a reserved word that cannot be used as a field name.\",\n+                    obj=self,\n+                    id='fields.E003',\n+                )\n+            ]\n+        else:\n+            return []\n+\n+    def _check_choices(self):\n+        if not self.choices:\n+            return []\n+\n+        def is_value(value, accept_promise=True):\n+            return isinstance(value, (str, Promise) if accept_promise else str) or not is_iterable(value)\n+\n+        if is_value(self.choices, accept_promise=False):\n+            return [\n+                checks.Error(\n+                    \"'choices' must be an iterable (e.g., a list or tuple).\",\n+                    obj=self,\n+                    id='fields.E004',\n+                )\n+            ]\n+\n+        # Expect [group_name, [value, display]]\n+        for choices_group in self.choices:\n+            try:\n+                group_name, group_choices = choices_group\n+            except (TypeError, ValueError):\n+                # Containing non-pairs\n+                break\n+            try:\n+                if not all(\n+                    is_value(value) and is_value(human_name)\n+                    for value, human_name in group_choices\n+                ):\n+                    break\n+            except (TypeError, ValueError):\n+                # No groups, choices in the form [value, display]\n+                value, human_name = group_name, group_choices\n+                if not is_value(value) or not is_value(human_name):\n+                    break\n+\n+            # Special case: choices=['ab']\n+            if isinstance(choices_group, str):\n+                break\n+        else:\n+            return []\n+\n+        return [\n+            checks.Error(\n+                \"'choices' must be an iterable containing \"\n+                \"(actual value, human readable name) tuples.\",\n+                obj=self,\n+                id='fields.E005',\n+            )\n+        ]\n+\n+    def _check_db_index(self):\n+        if self.db_index not in (None, True, False):\n+            return [\n+                checks.Error(\n+                    \"'db_index' must be None, True or False.\",\n+                    obj=self,\n+                    id='fields.E006',\n+                )\n+            ]\n+        else:\n+            return []\n+\n+    def _check_null_allowed_for_primary_keys(self):\n+        if (self.primary_key and self.null and\n+                not connection.features.interprets_empty_strings_as_nulls):\n+            # We cannot reliably check this for backends like Oracle which\n+            # consider NULL and '' to be equal (and thus set up\n+            # character-based fields a little differently).\n+            return [\n+                checks.Error(\n+                    'Primary keys must not have null=True.',\n+                    hint=('Set null=False on the field, or '\n+                          'remove primary_key=True argument.'),\n+                    obj=self,\n+                    id='fields.E007',\n+                )\n+            ]\n+        else:\n+            return []\n+\n+    def _check_backend_specific_checks(self, **kwargs):\n+        app_label = self.model._meta.app_label\n+        for db in connections:\n+            if router.allow_migrate(db, app_label, model_name=self.model._meta.model_name):\n+                return connections[db].validation.check_field(self, **kwargs)\n+        return []\n+\n+    def _check_validators(self):\n+        errors = []\n+        for i, validator in enumerate(self.validators):\n+            if not callable(validator):\n+                errors.append(\n+                    checks.Error(\n+                        \"All 'validators' must be callable.\",\n+                        hint=(\n+                            \"validators[{i}] ({repr}) isn't a function or \"\n+                            \"instance of a validator class.\".format(\n+                                i=i, repr=repr(validator),\n+                            )\n+                        ),\n+                        obj=self,\n+                        id='fields.E008',\n+                    )\n+                )\n+        return errors\n+\n+    def _check_deprecation_details(self):\n+        if self.system_check_removed_details is not None:\n+            return [\n+                checks.Error(\n+                    self.system_check_removed_details.get(\n+                        'msg',\n+                        '%s has been removed except for support in historical '\n+                        'migrations.' % self.__class__.__name__\n+                    ),\n+                    hint=self.system_check_removed_details.get('hint'),\n+                    obj=self,\n+                    id=self.system_check_removed_details.get('id', 'fields.EXXX'),\n+                )\n+            ]\n+        elif self.system_check_deprecated_details is not None:\n+            return [\n+                checks.Warning(\n+                    self.system_check_deprecated_details.get(\n+                        'msg',\n+                        '%s has been deprecated.' % self.__class__.__name__\n+                    ),\n+                    hint=self.system_check_deprecated_details.get('hint'),\n+                    obj=self,\n+                    id=self.system_check_deprecated_details.get('id', 'fields.WXXX'),\n+                )\n+            ]\n+        return []\n+\n+    def get_col(self, alias, output_field=None):\n+        if output_field is None:\n+            output_field = self\n+        if alias != self.model._meta.db_table or output_field != self:\n+            from django.db.models.expressions import Col\n+            return Col(alias, self, output_field)\n+        else:\n+            return self.cached_col\n+\n+    @cached_property\n+    def cached_col(self):\n+        from django.db.models.expressions import Col\n+        return Col(self.model._meta.db_table, self)\n+\n+    def select_format(self, compiler, sql, params):\n+        \"\"\"\n+        Custom format for select clauses. For example, GIS columns need to be\n+        selected as AsText(table.col) on MySQL as the table.col data can't be\n+        used by Django.\n+        \"\"\"\n+        return sql, params\n+\n+    def deconstruct(self):\n+        \"\"\"\n+        Return enough information to recreate the field as a 4-tuple:\n+\n+         * The name of the field on the model, if contribute_to_class() has\n+           been run.\n+         * The import path of the field, including the class:e.g.\n+           django.db.models.IntegerField This should be the most portable\n+           version, so less specific may be better.\n+         * A list of positional arguments.\n+         * A dict of keyword arguments.\n+\n+        Note that the positional or keyword arguments must contain values of\n+        the following types (including inner values of collection types):\n+\n+         * None, bool, str, int, float, complex, set, frozenset, list, tuple,\n+           dict\n+         * UUID\n+         * datetime.datetime (naive), datetime.date\n+         * top-level classes, top-level functions - will be referenced by their\n+           full import path\n+         * Storage instances - these have their own deconstruct() method\n+\n+        This is because the values here must be serialized into a text format\n+        (possibly new Python code, possibly JSON) and these are the only types\n+        with encoding handlers defined.\n+\n+        There's no need to return the exact way the field was instantiated this\n+        time, just ensure that the resulting field is the same - prefer keyword\n+        arguments over positional ones, and omit parameters with their default\n+        values.\n+        \"\"\"\n+        # Short-form way of fetching all the default parameters\n+        keywords = {}\n+        possibles = {\n+            \"verbose_name\": None,\n+            \"primary_key\": False,\n+            \"max_length\": None,\n+            \"unique\": False,\n+            \"blank\": False,\n+            \"null\": False,\n+            \"db_index\": False,\n+            \"default\": NOT_PROVIDED,\n+            \"editable\": True,\n+            \"serialize\": True,\n+            \"unique_for_date\": None,\n+            \"unique_for_month\": None,\n+            \"unique_for_year\": None,\n+            \"choices\": None,\n+            \"help_text\": '',\n+            \"db_column\": None,\n+            \"db_tablespace\": None,\n+            \"auto_created\": False,\n+            \"validators\": [],\n+            \"error_messages\": None,\n+        }\n+        attr_overrides = {\n+            \"unique\": \"_unique\",\n+            \"error_messages\": \"_error_messages\",\n+            \"validators\": \"_validators\",\n+            \"verbose_name\": \"_verbose_name\",\n+            \"db_tablespace\": \"_db_tablespace\",\n+        }\n+        equals_comparison = {\"choices\", \"validators\"}\n+        for name, default in possibles.items():\n+            value = getattr(self, attr_overrides.get(name, name))\n+            # Unroll anything iterable for choices into a concrete list\n+            if name == \"choices\" and isinstance(value, collections.abc.Iterable):\n+                value = list(value)\n+            # Do correct kind of comparison\n+            if name in equals_comparison:\n+                if value != default:\n+                    keywords[name] = value\n+            else:\n+                if value is not default:\n+                    keywords[name] = value\n+        # Work out path - we shorten it for known Django core fields\n+        path = \"%s.%s\" % (self.__class__.__module__, self.__class__.__qualname__)\n+        if path.startswith(\"django.db.models.fields.related\"):\n+            path = path.replace(\"django.db.models.fields.related\", \"django.db.models\")\n+        if path.startswith(\"django.db.models.fields.files\"):\n+            path = path.replace(\"django.db.models.fields.files\", \"django.db.models\")\n+        if path.startswith(\"django.db.models.fields.proxy\"):\n+            path = path.replace(\"django.db.models.fields.proxy\", \"django.db.models\")\n+        if path.startswith(\"django.db.models.fields\"):\n+            path = path.replace(\"django.db.models.fields\", \"django.db.models\")\n+        # Return basic info - other fields should override this.\n+        return (self.name, path, [], keywords)\n+\n+    def clone(self):\n+        \"\"\"\n+        Uses deconstruct() to clone a new copy of this Field.\n+        Will not preserve any class attachments/attribute names.\n+        \"\"\"\n+        name, path, args, kwargs = self.deconstruct()\n+        return self.__class__(*args, **kwargs)\n+\n+    def __eq__(self, other):\n+        # Needed for @total_ordering\n+        if isinstance(other, Field):\n+            return self.creation_counter == other.creation_counter\n+        return NotImplemented\n+\n+    def __lt__(self, other):\n+        # This is needed because bisect does not take a comparison function.\n+        if isinstance(other, Field):\n+            return self.creation_counter < other.creation_counter\n+        return NotImplemented\n+\n+    def __hash__(self):\n+        return hash(self.creation_counter)\n+\n+    def __deepcopy__(self, memodict):\n+        # We don't have to deepcopy very much here, since most things are not\n+        # intended to be altered after initial creation.\n+        obj = copy.copy(self)\n+        if self.remote_field:\n+            obj.remote_field = copy.copy(self.remote_field)\n+            if hasattr(self.remote_field, 'field') and self.remote_field.field is self:\n+                obj.remote_field.field = obj\n+        memodict[id(self)] = obj\n+        return obj\n+\n+    def __copy__(self):\n+        # We need to avoid hitting __reduce__, so define this\n+        # slightly weird copy construct.\n+        obj = Empty()\n+        obj.__class__ = self.__class__\n+        obj.__dict__ = self.__dict__.copy()\n+        return obj\n+\n+    def __reduce__(self):\n+        \"\"\"\n+        Pickling should return the model._meta.fields instance of the field,\n+        not a new copy of that field. So, use the app registry to load the\n+        model and then the field back.\n+        \"\"\"\n+        if not hasattr(self, 'model'):\n+            # Fields are sometimes used without attaching them to models (for\n+            # example in aggregation). In this case give back a plain field\n+            # instance. The code below will create a new empty instance of\n+            # class self.__class__, then update its dict with self.__dict__\n+            # values - so, this is very close to normal pickle.\n+            state = self.__dict__.copy()\n+            # The _get_default cached_property can't be pickled due to lambda\n+            # usage.\n+            state.pop('_get_default', None)\n+            return _empty, (self.__class__,), state\n+        return _load_field, (self.model._meta.app_label, self.model._meta.object_name,\n+                             self.name)\n+\n+    def get_pk_value_on_save(self, instance):\n+        \"\"\"\n+        Hook to generate new PK values on save. This method is called when\n+        saving instances with no primary key value set. If this method returns\n+        something else than None, then the returned value is used when saving\n+        the new instance.\n+        \"\"\"\n+        if self.default:\n+            return self.get_default()\n+        return None\n+\n+    def to_python(self, value):\n+        \"\"\"\n+        Convert the input value into the expected Python data type, raising\n+        django.core.exceptions.ValidationError if the data can't be converted.\n+        Return the converted value. Subclasses should override this.\n+        \"\"\"\n+        return value\n+\n+    @cached_property\n+    def validators(self):\n+        \"\"\"\n+        Some validators can't be created at field initialization time.\n+        This method provides a way to delay their creation until required.\n+        \"\"\"\n+        return [*self.default_validators, *self._validators]\n+\n+    def run_validators(self, value):\n+        if value in self.empty_values:\n+            return\n+\n+        errors = []\n+        for v in self.validators:\n+            try:\n+                v(value)\n+            except exceptions.ValidationError as e:\n+                if hasattr(e, 'code') and e.code in self.error_messages:\n+                    e.message = self.error_messages[e.code]\n+                errors.extend(e.error_list)\n+\n+        if errors:\n+            raise exceptions.ValidationError(errors)\n+\n+    def validate(self, value, model_instance):\n+        \"\"\"\n+        Validate value and raise ValidationError if necessary. Subclasses\n+        should override this to provide validation logic.\n+        \"\"\"\n+        if not self.editable:\n+            # Skip validation for non-editable fields.\n+            return\n+\n+        if self.choices is not None and value not in self.empty_values:\n+            for option_key, option_value in self.choices:\n+                if isinstance(option_value, (list, tuple)):\n+                    # This is an optgroup, so look inside the group for\n+                    # options.\n+                    for optgroup_key, optgroup_value in option_value:\n+                        if value == optgroup_key:\n+                            return\n+                elif value == option_key:\n+                    return\n+            raise exceptions.ValidationError(\n+                self.error_messages['invalid_choice'],\n+                code='invalid_choice',\n+                params={'value': value},\n+            )\n+\n+        if value is None and not self.null:\n+            raise exceptions.ValidationError(self.error_messages['null'], code='null')\n+\n+        if not self.blank and value in self.empty_values:\n+            raise exceptions.ValidationError(self.error_messages['blank'], code='blank')\n+\n+    def clean(self, value, model_instance):\n+        \"\"\"\n+        Convert the value's type and run validation. Validation errors\n+        from to_python() and validate() are propagated. Return the correct\n+        value if no error is raised.\n+        \"\"\"\n+        value = self.to_python(value)\n+        self.validate(value, model_instance)\n+        self.run_validators(value)\n+        return value\n+\n+    def db_type_parameters(self, connection):\n+        return DictWrapper(self.__dict__, connection.ops.quote_name, 'qn_')\n+\n+    def db_check(self, connection):\n+        \"\"\"\n+        Return the database column check constraint for this field, for the\n+        provided connection. Works the same way as db_type() for the case that\n+        get_internal_type() does not map to a preexisting model field.\n+        \"\"\"\n+        data = self.db_type_parameters(connection)\n+        try:\n+            return connection.data_type_check_constraints[self.get_internal_type()] % data\n+        except KeyError:\n+            return None\n+\n+    def db_type(self, connection):\n+        \"\"\"\n+        Return the database column data type for this field, for the provided\n+        connection.\n+        \"\"\"\n+        # The default implementation of this method looks at the\n+        # backend-specific data_types dictionary, looking up the field by its\n+        # \"internal type\".\n+        #\n+        # A Field class can implement the get_internal_type() method to specify\n+        # which *preexisting* Django Field class it's most similar to -- i.e.,\n+        # a custom field might be represented by a TEXT column type, which is\n+        # the same as the TextField Django field type, which means the custom\n+        # field's get_internal_type() returns 'TextField'.\n+        #\n+        # But the limitation of the get_internal_type() / data_types approach\n+        # is that it cannot handle database column types that aren't already\n+        # mapped to one of the built-in Django field types. In this case, you\n+        # can implement db_type() instead of get_internal_type() to specify\n+        # exactly which wacky database column type you want to use.\n+        data = self.db_type_parameters(connection)\n+        try:\n+            return connection.data_types[self.get_internal_type()] % data\n+        except KeyError:\n+            return None\n+\n+    def rel_db_type(self, connection):\n+        \"\"\"\n+        Return the data type that a related field pointing to this field should\n+        use. For example, this method is called by ForeignKey and OneToOneField\n+        to determine its data type.\n+        \"\"\"\n+        return self.db_type(connection)\n+\n+    def cast_db_type(self, connection):\n+        \"\"\"Return the data type to use in the Cast() function.\"\"\"\n+        db_type = connection.ops.cast_data_types.get(self.get_internal_type())\n+        if db_type:\n+            return db_type % self.db_type_parameters(connection)\n+        return self.db_type(connection)\n+\n+    def db_parameters(self, connection):\n+        \"\"\"\n+        Extension of db_type(), providing a range of different return values\n+        (type, checks). This will look at db_type(), allowing custom model\n+        fields to override it.\n+        \"\"\"\n+        type_string = self.db_type(connection)\n+        check_string = self.db_check(connection)\n+        return {\n+            \"type\": type_string,\n+            \"check\": check_string,\n+        }\n+\n+    def db_type_suffix(self, connection):\n+        return connection.data_types_suffix.get(self.get_internal_type())\n+\n+    def get_db_converters(self, connection):\n+        if hasattr(self, 'from_db_value'):\n+            return [self.from_db_value]\n+        return []\n+\n+    @property\n+    def unique(self):\n+        return self._unique or self.primary_key\n+\n+    @property\n+    def db_tablespace(self):\n+        return self._db_tablespace or settings.DEFAULT_INDEX_TABLESPACE\n+\n+    def set_attributes_from_name(self, name):\n+        self.name = self.name or name\n+        self.attname, self.column = self.get_attname_column()\n+        self.concrete = self.column is not None\n+        if self.verbose_name is None and self.name:\n+            self.verbose_name = self.name.replace('_', ' ')\n+\n+    def contribute_to_class(self, cls, name, private_only=False):\n+        \"\"\"\n+        Register the field with the model class it belongs to.\n+\n+        If private_only is True, create a separate instance of this field\n+        for every subclass of cls, even if cls is not an abstract model.\n+        \"\"\"\n+        self.set_attributes_from_name(name)\n+        self.model = cls\n+        cls._meta.add_field(self, private=private_only)\n+        if self.column:\n+            # Don't override classmethods with the descriptor. This means that\n+            # if you have a classmethod and a field with the same name, then\n+            # such fields can't be deferred (we don't have a check for this).\n+            if not getattr(cls, self.attname, None):\n+                setattr(cls, self.attname, DeferredAttribute(self.attname))\n+        if self.choices is not None:\n+            setattr(cls, 'get_%s_display' % self.name,\n+                    partialmethod(cls._get_FIELD_display, field=self))\n+\n+    def get_filter_kwargs_for_object(self, obj):\n+        \"\"\"\n+        Return a dict that when passed as kwargs to self.model.filter(), would\n+        yield all instances having the same value for this field as obj has.\n+        \"\"\"\n+        return {self.name: getattr(obj, self.attname)}\n+\n+    def get_attname(self):\n+        return self.name\n+\n+    def get_attname_column(self):\n+        attname = self.get_attname()\n+        column = self.db_column or attname\n+        return attname, column\n+\n+    def get_internal_type(self):\n+        return self.__class__.__name__\n+\n+    def pre_save(self, model_instance, add):\n+        \"\"\"Return field's value just before saving.\"\"\"\n+        return getattr(model_instance, self.attname)\n+\n+    def get_prep_value(self, value):\n+        \"\"\"Perform preliminary non-db specific value checks and conversions.\"\"\"\n+        if isinstance(value, Promise):\n+            value = value._proxy____cast()\n+        return value\n+\n+    def get_db_prep_value(self, value, connection, prepared=False):\n+        \"\"\"\n+        Return field's value prepared for interacting with the database backend.\n+\n+        Used by the default implementations of get_db_prep_save().\n+        \"\"\"\n+        if not prepared:\n+            value = self.get_prep_value(value)\n+        return value\n+\n+    def get_db_prep_save(self, value, connection):\n+        \"\"\"Return field's value prepared for saving into a database.\"\"\"\n+        return self.get_db_prep_value(value, connection=connection, prepared=False)\n+\n+    def has_default(self):\n+        \"\"\"Return a boolean of whether this field has a default value.\"\"\"\n+        return self.default is not NOT_PROVIDED\n+\n+    def get_default(self):\n+        \"\"\"Return the default value for this field.\"\"\"\n+        return self._get_default()\n+\n+    @cached_property\n+    def _get_default(self):\n+        if self.has_default():\n+            if callable(self.default):\n+                return self.default\n+            return lambda: self.default\n+\n+        if not self.empty_strings_allowed or self.null and not connection.features.interprets_empty_strings_as_nulls:\n+            return return_None\n+        return str  # return empty string\n+\n+    def get_choices(self, include_blank=True, blank_choice=BLANK_CHOICE_DASH, limit_choices_to=None, ordering=()):\n+        \"\"\"\n+        Return choices with a default blank choices included, for use\n+        as <select> choices for this field.\n+        \"\"\"\n+        if self.choices is not None:\n+            choices = list(self.choices)\n+            if include_blank:\n+                blank_defined = any(choice in ('', None) for choice, _ in self.flatchoices)\n+                if not blank_defined:\n+                    choices = blank_choice + choices\n+            return choices\n+        rel_model = self.remote_field.model\n+        limit_choices_to = limit_choices_to or self.get_limit_choices_to()\n+        choice_func = operator.attrgetter(\n+            self.remote_field.get_related_field().attname\n+            if hasattr(self.remote_field, 'get_related_field')\n+            else 'pk'\n+        )\n+        return (blank_choice if include_blank else []) + [\n+            (choice_func(x), str(x))\n+            for x in rel_model._default_manager.complex_filter(limit_choices_to).order_by(*ordering)\n+        ]\n+\n+    def value_to_string(self, obj):\n+        \"\"\"\n+        Return a string value of this field from the passed obj.\n+        This is used by the serialization framework.\n+        \"\"\"\n+        return str(self.value_from_object(obj))\n+\n+    def _get_flatchoices(self):\n+        \"\"\"Flattened version of choices tuple.\"\"\"\n+        if self.choices is None:\n+            return []\n+        flat = []\n+        for choice, value in self.choices:\n+            if isinstance(value, (list, tuple)):\n+                flat.extend(value)\n+            else:\n+                flat.append((choice, value))\n+        return flat\n+    flatchoices = property(_get_flatchoices)\n+\n+    def save_form_data(self, instance, data):\n+        setattr(instance, self.name, data)\n+\n+    def formfield(self, form_class=None, choices_form_class=None, **kwargs):\n+        \"\"\"Return a django.forms.Field instance for this field.\"\"\"\n+        defaults = {\n+            'required': not self.blank,\n+            'label': capfirst(self.verbose_name),\n+            'help_text': self.help_text,\n+        }\n+        if self.has_default():\n+            if callable(self.default):\n+                defaults['initial'] = self.default\n+                defaults['show_hidden_initial'] = True\n+            else:\n+                defaults['initial'] = self.get_default()\n+        if self.choices is not None:\n+            # Fields with choices get special treatment.\n+            include_blank = (self.blank or\n+                             not (self.has_default() or 'initial' in kwargs))\n+            defaults['choices'] = self.get_choices(include_blank=include_blank)\n+            defaults['coerce'] = self.to_python\n+            if self.null:\n+                defaults['empty_value'] = None\n+            if choices_form_class is not None:\n+                form_class = choices_form_class\n+            else:\n+                form_class = forms.TypedChoiceField\n+            # Many of the subclass-specific formfield arguments (min_value,\n+            # max_value) don't apply for choice fields, so be sure to only pass\n+            # the values that TypedChoiceField will understand.\n+            for k in list(kwargs):\n+                if k not in ('coerce', 'empty_value', 'choices', 'required',\n+                             'widget', 'label', 'initial', 'help_text',\n+                             'error_messages', 'show_hidden_initial', 'disabled'):\n+                    del kwargs[k]\n+        defaults.update(kwargs)\n+        if form_class is None:\n+            form_class = forms.CharField\n+        return form_class(**defaults)\n+\n+    def value_from_object(self, obj):\n+        \"\"\"Return the value of this field in the given model instance.\"\"\"\n+        return getattr(obj, self.attname)\n+\n+\n+class AutoField(Field):\n+    description = _(\"Integer\")\n+\n+    empty_strings_allowed = False\n+    default_error_messages = {\n+        'invalid': _(\"'%(value)s' value must be an integer.\"),\n+    }\n+\n+    def __init__(self, *args, **kwargs):\n+        kwargs['blank'] = True\n+        super().__init__(*args, **kwargs)\n+\n+    def check(self, **kwargs):\n+        return [\n+            *super().check(**kwargs),\n+            *self._check_primary_key(),\n+        ]\n+\n+    def _check_primary_key(self):\n+        if not self.primary_key:\n+            return [\n+                checks.Error(\n+                    'AutoFields must set primary_key=True.',\n+                    obj=self,\n+                    id='fields.E100',\n+                ),\n+            ]\n+        else:\n+            return []\n+\n+    def deconstruct(self):\n+        name, path, args, kwargs = super().deconstruct()\n+        del kwargs['blank']\n+        kwargs['primary_key'] = True\n+        return name, path, args, kwargs\n+\n+    def get_internal_type(self):\n+        return \"AutoField\"\n+\n+    def to_python(self, value):\n+        if value is None:\n+            return value\n+        try:\n+            return int(value)\n+        except (TypeError, ValueError):\n+            raise exceptions.ValidationError(\n+                self.error_messages['invalid'],\n+                code='invalid',\n+                params={'value': value},\n+            )\n+\n+    def rel_db_type(self, connection):\n+        return IntegerField().db_type(connection=connection)\n+\n+    def validate(self, value, model_instance):\n+        pass\n+\n+    def get_db_prep_value(self, value, connection, prepared=False):\n+        if not prepared:\n+            value = self.get_prep_value(value)\n+            value = connection.ops.validate_autopk_value(value)\n+        return value\n+\n+    def get_prep_value(self, value):\n+        from django.db.models.expressions import OuterRef\n+        value = super().get_prep_value(value)\n+        if value is None or isinstance(value, OuterRef):\n+            return value\n+        return int(value)\n+\n+    def contribute_to_class(self, cls, name, **kwargs):\n+        assert not cls._meta.auto_field, \"Model %s can't have more than one AutoField.\" % cls._meta.label\n+        super().contribute_to_class(cls, name, **kwargs)\n+        cls._meta.auto_field = self\n+\n+    def formfield(self, **kwargs):\n+        return None\n+\n+\n+class BigAutoField(AutoField):\n+    description = _(\"Big (8 byte) integer\")\n+\n+    def get_internal_type(self):\n+        return \"BigAutoField\"\n+\n+    def rel_db_type(self, connection):\n+        return BigIntegerField().db_type(connection=connection)\n+\n+\n+class BooleanField(Field):\n+    empty_strings_allowed = False\n+    default_error_messages = {\n+        'invalid': _(\"'%(value)s' value must be either True or False.\"),\n+        'invalid_nullable': _(\"'%(value)s' value must be either True, False, or None.\"),\n+    }\n+    description = _(\"Boolean (Either True or False)\")\n+\n+    def get_internal_type(self):\n+        return \"BooleanField\"\n+\n+    def to_python(self, value):\n+        if self.null and value in self.empty_values:\n+            return None\n+        if value in (True, False):\n+            # 1/0 are equal to True/False. bool() converts former to latter.\n+            return bool(value)\n+        if value in ('t', 'True', '1'):\n+            return True\n+        if value in ('f', 'False', '0'):\n+            return False\n+        raise exceptions.ValidationError(\n+            self.error_messages['invalid_nullable' if self.null else 'invalid'],\n+            code='invalid',\n+            params={'value': value},\n+        )\n+\n+    def get_prep_value(self, value):\n+        value = super().get_prep_value(value)\n+        if value is None:\n+            return None\n+        return self.to_python(value)\n+\n+    def formfield(self, **kwargs):\n+        if self.choices is not None:\n+            include_blank = not (self.has_default() or 'initial' in kwargs)\n+            defaults = {'choices': self.get_choices(include_blank=include_blank)}\n+        else:\n+            form_class = forms.NullBooleanField if self.null else forms.BooleanField\n+            # In HTML checkboxes, 'required' means \"must be checked\" which is\n+            # different from the choices case (\"must select some value\").\n+            # required=False allows unchecked checkboxes.\n+            defaults = {'form_class': form_class, 'required': False}\n+        return super().formfield(**{**defaults, **kwargs})\n+\n+\n+class CharField(Field):\n+    description = _(\"String (up to %(max_length)s)\")\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.validators.append(validators.MaxLengthValidator(self.max_length))\n+\n+    def check(self, **kwargs):\n+        return [\n+            *super().check(**kwargs),\n+            *self._check_max_length_attribute(**kwargs),\n+        ]\n+\n+    def _check_max_length_attribute(self, **kwargs):\n+        if self.max_length is None:\n+            return [\n+                checks.Error(\n+                    \"CharFields must define a 'max_length' attribute.\",\n+                    obj=self,\n+                    id='fields.E120',\n+                )\n+            ]\n+        elif (not isinstance(self.max_length, int) or isinstance(self.max_length, bool) or\n+                self.max_length <= 0):\n+            return [\n+                checks.Error(\n+                    \"'max_length' must be a positive integer.\",\n+                    obj=self,\n+                    id='fields.E121',\n+                )\n+            ]\n+        else:\n+            return []\n+\n+    def cast_db_type(self, connection):\n+        if self.max_length is None:\n+            return connection.ops.cast_char_field_without_max_length\n+        return super().cast_db_type(connection)\n+\n+    def get_internal_type(self):\n+        return \"CharField\"\n+\n+    def to_python(self, value):\n+        if isinstance(value, str) or value is None:\n+            return value\n+        return str(value)\n+\n+    def get_prep_value(self, value):\n+        value = super().get_prep_value(value)\n+        return self.to_python(value)\n+\n+    def formfield(self, **kwargs):\n+        # Passing max_length to forms.CharField means that the value's length\n+        # will be validated twice. This is considered acceptable since we want\n+        # the value in the form field (to pass into widget for example).\n+        defaults = {'max_length': self.max_length}\n+        # TODO: Handle multiple backends with different feature flags.\n+        if self.null and not connection.features.interprets_empty_strings_as_nulls:\n+            defaults['empty_value'] = None\n+        defaults.update(kwargs)\n+        return super().formfield(**defaults)\n+\n+\n+class CommaSeparatedIntegerField(CharField):\n+    default_validators = [validators.validate_comma_separated_integer_list]\n+    description = _(\"Comma-separated integers\")\n+    system_check_removed_details = {\n+        'msg': (\n+            'CommaSeparatedIntegerField is removed except for support in '\n+            'historical migrations.'\n+        ),\n+        'hint': (\n+            'Use CharField(validators=[validate_comma_separated_integer_list]) '\n+            'instead.'\n+        ),\n+        'id': 'fields.E901',\n+    }\n+\n+\n+class DateTimeCheckMixin:\n+\n+    def check(self, **kwargs):\n+        return [\n+            *super().check(**kwargs),\n+            *self._check_mutually_exclusive_options(),\n+            *self._check_fix_default_value(),\n+        ]\n+\n+    def _check_mutually_exclusive_options(self):\n+        # auto_now, auto_now_add, and default are mutually exclusive\n+        # options. The use of more than one of these options together\n+        # will trigger an Error\n+        mutually_exclusive_options = [self.auto_now_add, self.auto_now, self.has_default()]\n+        enabled_options = [option not in (None, False) for option in mutually_exclusive_options].count(True)\n+        if enabled_options > 1:\n+            return [\n+                checks.Error(\n+                    \"The options auto_now, auto_now_add, and default \"\n+                    \"are mutually exclusive. Only one of these options \"\n+                    \"may be present.\",\n+                    obj=self,\n+                    id='fields.E160',\n+                )\n+            ]\n+        else:\n+            return []\n+\n+    def _check_fix_default_value(self):\n+        return []\n+\n+\n+class DateField(DateTimeCheckMixin, Field):\n+    empty_strings_allowed = False\n+    default_error_messages = {\n+        'invalid': _(\"'%(value)s' value has an invalid date format. It must be \"\n+                     \"in YYYY-MM-DD format.\"),\n+        'invalid_date': _(\"'%(value)s' value has the correct format (YYYY-MM-DD) \"\n+                          \"but it is an invalid date.\"),\n+    }\n+    description = _(\"Date (without time)\")\n+\n+    def __init__(self, verbose_name=None, name=None, auto_now=False,\n+                 auto_now_add=False, **kwargs):\n+        self.auto_now, self.auto_now_add = auto_now, auto_now_add\n+        if auto_now or auto_now_add:\n+            kwargs['editable'] = False\n+            kwargs['blank'] = True\n+        super().__init__(verbose_name, name, **kwargs)\n+\n+    def _check_fix_default_value(self):\n+        \"\"\"\n+        Warn that using an actual date or datetime value is probably wrong;\n+        it's only evaluated on server startup.\n+        \"\"\"\n+        if not self.has_default():\n+            return []\n+\n+        now = timezone.now()\n+        if not timezone.is_naive(now):\n+            now = timezone.make_naive(now, timezone.utc)\n+        value = self.default\n+        if isinstance(value, datetime.datetime):\n+            if not timezone.is_naive(value):\n+                value = timezone.make_naive(value, timezone.utc)\n+            value = value.date()\n+        elif isinstance(value, datetime.date):\n+            # Nothing to do, as dates don't have tz information\n+            pass\n+        else:\n+            # No explicit date / datetime value -- no checks necessary\n+            return []\n+        offset = datetime.timedelta(days=1)\n+        lower = (now - offset).date()\n+        upper = (now + offset).date()\n+        if lower <= value <= upper:\n+            return [\n+                checks.Warning(\n+                    'Fixed default value provided.',\n+                    hint='It seems you set a fixed date / time / datetime '\n+                         'value as default for this field. This may not be '\n+                         'what you want. If you want to have the current date '\n+                         'as default, use `django.utils.timezone.now`',\n+                    obj=self,\n+                    id='fields.W161',\n+                )\n+            ]\n+\n+        return []\n+\n+    def deconstruct(self):\n+        name, path, args, kwargs = super().deconstruct()\n+        if self.auto_now:\n+            kwargs['auto_now'] = True\n+        if self.auto_now_add:\n+            kwargs['auto_now_add'] = True\n+        if self.auto_now or self.auto_now_add:\n+            del kwargs['editable']\n+            del kwargs['blank']\n+        return name, path, args, kwargs\n+\n+    def get_internal_type(self):\n+        return \"DateField\"\n+\n+    def to_python(self, value):\n+        if value is None:\n+            return value\n+        if isinstance(value, datetime.datetime):\n+            if settings.USE_TZ and timezone.is_aware(value):\n+                # Convert aware datetimes to the default time zone\n+                # before casting them to dates (#17742).\n+                default_timezone = timezone.get_default_timezone()\n+                value = timezone.make_naive(value, default_timezone)\n+            return value.date()\n+        if isinstance(value, datetime.date):\n+            return value\n+\n+        try:\n+            parsed = parse_date(value)\n+            if parsed is not None:\n+                return parsed\n+        except ValueError:\n+            raise exceptions.ValidationError(\n+                self.error_messages['invalid_date'],\n+                code='invalid_date',\n+                params={'value': value},\n+            )\n+\n+        raise exceptions.ValidationError(\n+            self.error_messages['invalid'],\n+            code='invalid',\n+            params={'value': value},\n+        )\n+\n+    def pre_save(self, model_instance, add):\n+        if self.auto_now or (self.auto_now_add and add):\n+            value = datetime.date.today()\n+            setattr(model_instance, self.attname, value)\n+            return value\n+        else:\n+            return super().pre_save(model_instance, add)\n+\n+    def contribute_to_class(self, cls, name, **kwargs):\n+        super().contribute_to_class(cls, name, **kwargs)\n+        if not self.null:\n+            setattr(\n+                cls, 'get_next_by_%s' % self.name,\n+                partialmethod(cls._get_next_or_previous_by_FIELD, field=self, is_next=True)\n+            )\n+            setattr(\n+                cls, 'get_previous_by_%s' % self.name,\n+                partialmethod(cls._get_next_or_previous_by_FIELD, field=self, is_next=False)\n+            )\n+\n+    def get_prep_value(self, value):\n+        value = super().get_prep_value(value)\n+        return self.to_python(value)\n+\n+    def get_db_prep_value(self, value, connection, prepared=False):\n+        # Casts dates into the format expected by the backend\n+        if not prepared:\n+            value = self.get_prep_value(value)\n+        return connection.ops.adapt_datefield_value(value)\n+\n+    def value_to_string(self, obj):\n+        val = self.value_from_object(obj)\n+        return '' if val is None else val.isoformat()\n+\n+    def formfield(self, **kwargs):\n+        return super().formfield(**{\n+            'form_class': forms.DateField,\n+            **kwargs,\n+        })\n+\n+\n+class DateTimeField(DateField):\n+    empty_strings_allowed = False\n+    default_error_messages = {\n+        'invalid': _(\"'%(value)s' value has an invalid format. It must be in \"\n+                     \"YYYY-MM-DD HH:MM[:ss[.uuuuuu]][TZ] format.\"),\n+        'invalid_date': _(\"'%(value)s' value has the correct format \"\n+                          \"(YYYY-MM-DD) but it is an invalid date.\"),\n+        'invalid_datetime': _(\"'%(value)s' value has the correct format \"\n+                              \"(YYYY-MM-DD HH:MM[:ss[.uuuuuu]][TZ]) \"\n+                              \"but it is an invalid date/time.\"),\n+    }\n+    description = _(\"Date (with time)\")\n+\n+    # __init__ is inherited from DateField\n+\n+    def _check_fix_default_value(self):\n+        \"\"\"\n+        Warn that using an actual date or datetime value is probably wrong;\n+        it's only evaluated on server startup.\n+        \"\"\"\n+        if not self.has_default():\n+            return []\n+\n+        now = timezone.now()\n+        if not timezone.is_naive(now):\n+            now = timezone.make_naive(now, timezone.utc)\n+        value = self.default\n+        if isinstance(value, datetime.datetime):\n+            second_offset = datetime.timedelta(seconds=10)\n+            lower = now - second_offset\n+            upper = now + second_offset\n+            if timezone.is_aware(value):\n+                value = timezone.make_naive(value, timezone.utc)\n+        elif isinstance(value, datetime.date):\n+            second_offset = datetime.timedelta(seconds=10)\n+            lower = now - second_offset\n+            lower = datetime.datetime(lower.year, lower.month, lower.day)\n+            upper = now + second_offset\n+            upper = datetime.datetime(upper.year, upper.month, upper.day)\n+            value = datetime.datetime(value.year, value.month, value.day)\n+        else:\n+            # No explicit date / datetime value -- no checks necessary\n+            return []\n+        if lower <= value <= upper:\n+            return [\n+                checks.Warning(\n+                    'Fixed default value provided.',\n+                    hint='It seems you set a fixed date / time / datetime '\n+                         'value as default for this field. This may not be '\n+                         'what you want. If you want to have the current date '\n+                         'as default, use `django.utils.timezone.now`',\n+                    obj=self,\n+                    id='fields.W161',\n+                )\n+            ]\n+\n+        return []\n+\n+    def get_internal_type(self):\n+        return \"DateTimeField\"\n+\n+    def to_python(self, value):\n+        if value is None:\n+            return value\n+        if isinstance(value, datetime.datetime):\n+            return value\n+        if isinstance(value, datetime.date):\n+            value = datetime.datetime(value.year, value.month, value.day)\n+            if settings.USE_TZ:\n+                # For backwards compatibility, interpret naive datetimes in\n+                # local time. This won't work during DST change, but we can't\n+                # do much about it, so we let the exceptions percolate up the\n+                # call stack.\n+                warnings.warn(\"DateTimeField %s.%s received a naive datetime \"\n+                              \"(%s) while time zone support is active.\" %\n+                              (self.model.__name__, self.name, value),\n+                              RuntimeWarning)\n+                default_timezone = timezone.get_default_timezone()\n+                value = timezone.make_aware(value, default_timezone)\n+            return value\n+\n+        try:\n+            parsed = parse_datetime(value)\n+            if parsed is not None:\n+                return parsed\n+        except ValueError:\n+            raise exceptions.ValidationError(\n+                self.error_messages['invalid_datetime'],\n+                code='invalid_datetime',\n+                params={'value': value},\n+            )\n+\n+        try:\n+            parsed = parse_date(value)\n+            if parsed is not None:\n+                return datetime.datetime(parsed.year, parsed.month, parsed.day)\n+        except ValueError:\n+            raise exceptions.ValidationError(\n+                self.error_messages['invalid_date'],\n+                code='invalid_date',\n+                params={'value': value},\n+            )\n+\n+        raise exceptions.ValidationError(\n+            self.error_messages['invalid'],\n+            code='invalid',\n+            params={'value': value},\n+        )\n+\n+    def pre_save(self, model_instance, add):\n+        if self.auto_now or (self.auto_now_add and add):\n+            value = timezone.now()\n+            setattr(model_instance, self.attname, value)\n+            return value\n+        else:\n+            return super().pre_save(model_instance, add)\n+\n+    # contribute_to_class is inherited from DateField, it registers\n+    # get_next_by_FOO and get_prev_by_FOO\n+\n+    def get_prep_value(self, value):\n+        value = super().get_prep_value(value)\n+        value = self.to_python(value)\n+        if value is not None and settings.USE_TZ and timezone.is_naive(value):\n+            # For backwards compatibility, interpret naive datetimes in local\n+            # time. This won't work during DST change, but we can't do much\n+            # about it, so we let the exceptions percolate up the call stack.\n+            try:\n+                name = '%s.%s' % (self.model.__name__, self.name)\n+            except AttributeError:\n+                name = '(unbound)'\n+            warnings.warn(\"DateTimeField %s received a naive datetime (%s)\"\n+                          \" while time zone support is active.\" %\n+                          (name, value),\n+                          RuntimeWarning)\n+            default_timezone = timezone.get_default_timezone()\n+            value = timezone.make_aware(value, default_timezone)\n+        return value\n+\n+    def get_db_prep_value(self, value, connection, prepared=False):\n+        # Casts datetimes into the format expected by the backend\n+        if not prepared:\n+            value = self.get_prep_value(value)\n+        return connection.ops.adapt_datetimefield_value(value)\n+\n+    def value_to_string(self, obj):\n+        val = self.value_from_object(obj)\n+        return '' if val is None else val.isoformat()\n+\n+    def formfield(self, **kwargs):\n+        return super().formfield(**{\n+            'form_class': forms.DateTimeField,\n+            **kwargs,\n+        })\n+\n+\n+class DecimalField(Field):\n+    empty_strings_allowed = False\n+    default_error_messages = {\n+        'invalid': _(\"'%(value)s' value must be a decimal number.\"),\n+    }\n+    description = _(\"Decimal number\")\n+\n+    def __init__(self, verbose_name=None, name=None, max_digits=None,\n+                 decimal_places=None, **kwargs):\n+        self.max_digits, self.decimal_places = max_digits, decimal_places\n+        super().__init__(verbose_name, name, **kwargs)\n+\n+    def check(self, **kwargs):\n+        errors = super().check(**kwargs)\n+\n+        digits_errors = [\n+            *self._check_decimal_places(),\n+            *self._check_max_digits(),\n+        ]\n+        if not digits_errors:\n+            errors.extend(self._check_decimal_places_and_max_digits(**kwargs))\n+        else:\n+            errors.extend(digits_errors)\n+        return errors\n+\n+    def _check_decimal_places(self):\n+        try:\n+            decimal_places = int(self.decimal_places)\n+            if decimal_places < 0:\n+                raise ValueError()\n+        except TypeError:\n+            return [\n+                checks.Error(\n+                    \"DecimalFields must define a 'decimal_places' attribute.\",\n+                    obj=self,\n+                    id='fields.E130',\n+                )\n+            ]\n+        except ValueError:\n+            return [\n+                checks.Error(\n+                    \"'decimal_places' must be a non-negative integer.\",\n+                    obj=self,\n+                    id='fields.E131',\n+                )\n+            ]\n+        else:\n+            return []\n+\n+    def _check_max_digits(self):\n+        try:\n+            max_digits = int(self.max_digits)\n+            if max_digits <= 0:\n+                raise ValueError()\n+        except TypeError:\n+            return [\n+                checks.Error(\n+                    \"DecimalFields must define a 'max_digits' attribute.\",\n+                    obj=self,\n+                    id='fields.E132',\n+                )\n+            ]\n+        except ValueError:\n+            return [\n+                checks.Error(\n+                    \"'max_digits' must be a positive integer.\",\n+                    obj=self,\n+                    id='fields.E133',\n+                )\n+            ]\n+        else:\n+            return []\n+\n+    def _check_decimal_places_and_max_digits(self, **kwargs):\n+        if int(self.decimal_places) > int(self.max_digits):\n+            return [\n+                checks.Error(\n+                    \"'max_digits' must be greater or equal to 'decimal_places'.\",\n+                    obj=self,\n+                    id='fields.E134',\n+                )\n+            ]\n+        return []\n+\n+    @cached_property\n+    def validators(self):\n+        return super().validators + [\n+            validators.DecimalValidator(self.max_digits, self.decimal_places)\n+        ]\n+\n+    @cached_property\n+    def context(self):\n+        return decimal.Context(prec=self.max_digits)\n+\n+    def deconstruct(self):\n+        name, path, args, kwargs = super().deconstruct()\n+        if self.max_digits is not None:\n+            kwargs['max_digits'] = self.max_digits\n+        if self.decimal_places is not None:\n+            kwargs['decimal_places'] = self.decimal_places\n+        return name, path, args, kwargs\n+\n+    def get_internal_type(self):\n+        return \"DecimalField\"\n+\n+    def to_python(self, value):\n+        if value is None:\n+            return value\n+        if isinstance(value, float):\n+            return self.context.create_decimal_from_float(value)\n+        try:\n+            return decimal.Decimal(value)\n+        except decimal.InvalidOperation:\n+            raise exceptions.ValidationError(\n+                self.error_messages['invalid'],\n+                code='invalid',\n+                params={'value': value},\n+            )\n+\n+    def get_db_prep_save(self, value, connection):\n+        return connection.ops.adapt_decimalfield_value(self.to_python(value), self.max_digits, self.decimal_places)\n+\n+    def get_prep_value(self, value):\n+        value = super().get_prep_value(value)\n+        return self.to_python(value)\n+\n+    def formfield(self, **kwargs):\n+        return super().formfield(**{\n+            'max_digits': self.max_digits,\n+            'decimal_places': self.decimal_places,\n+            'form_class': forms.DecimalField,\n+            **kwargs,\n+        })\n+\n+\n+class DurationField(Field):\n+    \"\"\"\n+    Store timedelta objects.\n+\n+    Use interval on PostgreSQL, INTERVAL DAY TO SECOND on Oracle, and bigint\n+    of microseconds on other databases.\n+    \"\"\"\n+    empty_strings_allowed = False\n+    default_error_messages = {\n+        'invalid': _(\"'%(value)s' value has an invalid format. It must be in \"\n+                     \"[DD] [[HH:]MM:]ss[.uuuuuu] format.\")\n+    }\n+    description = _(\"Duration\")\n+\n+    def get_internal_type(self):\n+        return \"DurationField\"\n+\n+    def to_python(self, value):\n+        if value is None:\n+            return value\n+        if isinstance(value, datetime.timedelta):\n+            return value\n+        try:\n+            parsed = parse_duration(value)\n+        except ValueError:\n+            pass\n+        else:\n+            if parsed is not None:\n+                return parsed\n+\n+        raise exceptions.ValidationError(\n+            self.error_messages['invalid'],\n+            code='invalid',\n+            params={'value': value},\n+        )\n+\n+    def get_db_prep_value(self, value, connection, prepared=False):\n+        if connection.features.has_native_duration_field:\n+            return value\n+        if value is None:\n+            return None\n+        return duration_microseconds(value)\n+\n+    def get_db_converters(self, connection):\n+        converters = []\n+        if not connection.features.has_native_duration_field:\n+            converters.append(connection.ops.convert_durationfield_value)\n+        return converters + super().get_db_converters(connection)\n+\n+    def value_to_string(self, obj):\n+        val = self.value_from_object(obj)\n+        return '' if val is None else duration_string(val)\n+\n+    def formfield(self, **kwargs):\n+        return super().formfield(**{\n+            'form_class': forms.DurationField,\n+            **kwargs,\n+        })\n+\n+\n+class EmailField(CharField):\n+    default_validators = [validators.validate_email]\n+    description = _(\"Email address\")\n+\n+    def __init__(self, *args, **kwargs):\n+        # max_length=254 to be compliant with RFCs 3696 and 5321\n+        kwargs.setdefault('max_length', 254)\n+        super().__init__(*args, **kwargs)\n+\n+    def deconstruct(self):\n+        name, path, args, kwargs = super().deconstruct()\n+        # We do not exclude max_length if it matches default as we want to change\n+        # the default in future.\n+        return name, path, args, kwargs\n+\n+    def formfield(self, **kwargs):\n+        # As with CharField, this will cause email validation to be performed\n+        # twice.\n+        return super().formfield(**{\n+            'form_class': forms.EmailField,\n+            **kwargs,\n+        })\n+\n+\n+class FilePathField(Field):\n+    description = _(\"File path\")\n+\n+    def __init__(self, verbose_name=None, name=None, path='', match=None,\n+                 recursive=False, allow_files=True, allow_folders=False, **kwargs):\n+        self.path, self.match, self.recursive = path, match, recursive\n+        self.allow_files, self.allow_folders = allow_files, allow_folders\n+        kwargs.setdefault('max_length', 100)\n+        super().__init__(verbose_name, name, **kwargs)\n+\n+    def check(self, **kwargs):\n+        return [\n+            *super().check(**kwargs),\n+            *self._check_allowing_files_or_folders(**kwargs),\n+        ]\n+\n+    def _check_allowing_files_or_folders(self, **kwargs):\n+        if not self.allow_files and not self.allow_folders:\n+            return [\n+                checks.Error(\n+                    \"FilePathFields must have either 'allow_files' or 'allow_folders' set to True.\",\n+                    obj=self,\n+                    id='fields.E140',\n+                )\n+            ]\n+        return []\n+\n+    def deconstruct(self):\n+        name, path, args, kwargs = super().deconstruct()\n+        if self.path != '':\n+            kwargs['path'] = self.path\n+        if self.match is not None:\n+            kwargs['match'] = self.match\n+        if self.recursive is not False:\n+            kwargs['recursive'] = self.recursive\n+        if self.allow_files is not True:\n+            kwargs['allow_files'] = self.allow_files\n+        if self.allow_folders is not False:\n+            kwargs['allow_folders'] = self.allow_folders\n+        if kwargs.get(\"max_length\") == 100:\n+            del kwargs[\"max_length\"]\n+        return name, path, args, kwargs\n+\n+    def get_prep_value(self, value):\n+        value = super().get_prep_value(value)\n+        if value is None:\n+            return None\n+        return str(value)\n+\n+    def formfield(self, **kwargs):\n+        return super().formfield(**{\n+            'path': self.path,\n+            'match': self.match,\n+            'recursive': self.recursive,\n+            'form_class': forms.FilePathField,\n+            'allow_files': self.allow_files,\n+            'allow_folders': self.allow_folders,\n+            **kwargs,\n+        })\n+\n+    def get_internal_type(self):\n+        return \"FilePathField\"\n+\n+\n+class FloatField(Field):\n+    empty_strings_allowed = False\n+    default_error_messages = {\n+        'invalid': _(\"'%(value)s' value must be a float.\"),\n+    }\n+    description = _(\"Floating point number\")\n+\n+    def get_prep_value(self, value):\n+        value = super().get_prep_value(value)\n+        if value is None:\n+            return None\n+        return float(value)\n+\n+    def get_internal_type(self):\n+        return \"FloatField\"\n+\n+    def to_python(self, value):\n+        if value is None:\n+            return value\n+        try:\n+            return float(value)\n+        except (TypeError, ValueError):\n+            raise exceptions.ValidationError(\n+                self.error_messages['invalid'],\n+                code='invalid',\n+                params={'value': value},\n+            )\n+\n+    def formfield(self, **kwargs):\n+        return super().formfield(**{\n+            'form_class': forms.FloatField,\n+            **kwargs,\n+        })\n+\n+\n+class IntegerField(Field):\n+    empty_strings_allowed = False\n+    default_error_messages = {\n+        'invalid': _(\"'%(value)s' value must be an integer.\"),\n+    }\n+    description = _(\"Integer\")\n+\n+    def check(self, **kwargs):\n+        return [\n+            *super().check(**kwargs),\n+            *self._check_max_length_warning(),\n+        ]\n+\n+    def _check_max_length_warning(self):\n+        if self.max_length is not None:\n+            return [\n+                checks.Warning(\n+                    \"'max_length' is ignored when used with %s.\" % self.__class__.__name__,\n+                    hint=\"Remove 'max_length' from field\",\n+                    obj=self,\n+                    id='fields.W122',\n+                )\n+            ]\n+        return []\n+\n+    @cached_property\n+    def validators(self):\n+        # These validators can't be added at field initialization time since\n+        # they're based on values retrieved from `connection`.\n+        validators_ = super().validators\n+        internal_type = self.get_internal_type()\n+        min_value, max_value = connection.ops.integer_field_range(internal_type)\n+        if min_value is not None and not any(\n+            (\n+                isinstance(validator, validators.MinValueValidator) and (\n+                    validator.limit_value()\n+                    if callable(validator.limit_value)\n+                    else validator.limit_value\n+                ) >= min_value\n+            ) for validator in validators_\n+        ):\n+            validators_.append(validators.MinValueValidator(min_value))\n+        if max_value is not None and not any(\n+            (\n+                isinstance(validator, validators.MaxValueValidator) and (\n+                    validator.limit_value()\n+                    if callable(validator.limit_value)\n+                    else validator.limit_value\n+                ) <= max_value\n+            ) for validator in validators_\n+        ):\n+            validators_.append(validators.MaxValueValidator(max_value))\n+        return validators_\n+\n+    def get_prep_value(self, value):\n+        value = super().get_prep_value(value)\n+        if value is None:\n+            return None\n+        return int(value)\n+\n+    def get_internal_type(self):\n+        return \"IntegerField\"\n+\n+    def to_python(self, value):\n+        if value is None:\n+            return value\n+        try:\n+            return int(value)\n+        except (TypeError, ValueError):\n+            raise exceptions.ValidationError(\n+                self.error_messages['invalid'],\n+                code='invalid',\n+                params={'value': value},\n+            )\n+\n+    def formfield(self, **kwargs):\n+        return super().formfield(**{\n+            'form_class': forms.IntegerField,\n+            **kwargs,\n+        })\n+\n+\n+class BigIntegerField(IntegerField):\n+    description = _(\"Big (8 byte) integer\")\n+    MAX_BIGINT = 9223372036854775807\n+\n+    def get_internal_type(self):\n+        return \"BigIntegerField\"\n+\n+    def formfield(self, **kwargs):\n+        return super().formfield(**{\n+            'min_value': -BigIntegerField.MAX_BIGINT - 1,\n+            'max_value': BigIntegerField.MAX_BIGINT,\n+            **kwargs,\n+        })\n+\n+\n+class IPAddressField(Field):\n+    empty_strings_allowed = False\n+    description = _(\"IPv4 address\")\n+    system_check_removed_details = {\n+        'msg': (\n+            'IPAddressField has been removed except for support in '\n+            'historical migrations.'\n+        ),\n+        'hint': 'Use GenericIPAddressField instead.',\n+        'id': 'fields.E900',\n+    }\n+\n+    def __init__(self, *args, **kwargs):\n+        kwargs['max_length'] = 15\n+        super().__init__(*args, **kwargs)\n+\n+    def deconstruct(self):\n+        name, path, args, kwargs = super().deconstruct()\n+        del kwargs['max_length']\n+        return name, path, args, kwargs\n+\n+    def get_prep_value(self, value):\n+        value = super().get_prep_value(value)\n+        if value is None:\n+            return None\n+        return str(value)\n+\n+    def get_internal_type(self):\n+        return \"IPAddressField\"\n+\n+\n+class GenericIPAddressField(Field):\n+    empty_strings_allowed = False\n+    description = _(\"IP address\")\n+    default_error_messages = {}\n+\n+    def __init__(self, verbose_name=None, name=None, protocol='both',\n+                 unpack_ipv4=False, *args, **kwargs):\n+        self.unpack_ipv4 = unpack_ipv4\n+        self.protocol = protocol\n+        self.default_validators, invalid_error_message = \\\n+            validators.ip_address_validators(protocol, unpack_ipv4)\n+        self.default_error_messages['invalid'] = invalid_error_message\n+        kwargs['max_length'] = 39\n+        super().__init__(verbose_name, name, *args, **kwargs)\n+\n+    def check(self, **kwargs):\n+        return [\n+            *super().check(**kwargs),\n+            *self._check_blank_and_null_values(**kwargs),\n+        ]\n+\n+    def _check_blank_and_null_values(self, **kwargs):\n+        if not getattr(self, 'null', False) and getattr(self, 'blank', False):\n+            return [\n+                checks.Error(\n+                    'GenericIPAddressFields cannot have blank=True if null=False, '\n+                    'as blank values are stored as nulls.',\n+                    obj=self,\n+                    id='fields.E150',\n+                )\n+            ]\n+        return []\n+\n+    def deconstruct(self):\n+        name, path, args, kwargs = super().deconstruct()\n+        if self.unpack_ipv4 is not False:\n+            kwargs['unpack_ipv4'] = self.unpack_ipv4\n+        if self.protocol != \"both\":\n+            kwargs['protocol'] = self.protocol\n+        if kwargs.get(\"max_length\") == 39:\n+            del kwargs['max_length']\n+        return name, path, args, kwargs\n+\n+    def get_internal_type(self):\n+        return \"GenericIPAddressField\"\n+\n+    def to_python(self, value):\n+        if value is None:\n+            return None\n+        if not isinstance(value, str):\n+            value = str(value)\n+        value = value.strip()\n+        if ':' in value:\n+            return clean_ipv6_address(value, self.unpack_ipv4, self.error_messages['invalid'])\n+        return value\n+\n+    def get_db_prep_value(self, value, connection, prepared=False):\n+        if not prepared:\n+            value = self.get_prep_value(value)\n+        return connection.ops.adapt_ipaddressfield_value(value)\n+\n+    def get_prep_value(self, value):\n+        value = super().get_prep_value(value)\n+        if value is None:\n+            return None\n+        if value and ':' in value:\n+            try:\n+                return clean_ipv6_address(value, self.unpack_ipv4)\n+            except exceptions.ValidationError:\n+                pass\n+        return str(value)\n+\n+    def formfield(self, **kwargs):\n+        return super().formfield(**{\n+            'protocol': self.protocol,\n+            'form_class': forms.GenericIPAddressField,\n+            **kwargs,\n+        })\n+\n+\n+class NullBooleanField(BooleanField):\n+    default_error_messages = {\n+        'invalid': _(\"'%(value)s' value must be either None, True or False.\"),\n+        'invalid_nullable': _(\"'%(value)s' value must be either None, True or False.\"),\n+    }\n+    description = _(\"Boolean (Either True, False or None)\")\n+\n+    def __init__(self, *args, **kwargs):\n+        kwargs['null'] = True\n+        kwargs['blank'] = True\n+        super().__init__(*args, **kwargs)\n+\n+    def deconstruct(self):\n+        name, path, args, kwargs = super().deconstruct()\n+        del kwargs['null']\n+        del kwargs['blank']\n+        return name, path, args, kwargs\n+\n+    def get_internal_type(self):\n+        return \"NullBooleanField\"\n+\n+\n+class PositiveIntegerRelDbTypeMixin:\n+\n+    def rel_db_type(self, connection):\n+        \"\"\"\n+        Return the data type that a related field pointing to this field should\n+        use. In most cases, a foreign key pointing to a positive integer\n+        primary key will have an integer column data type but some databases\n+        (e.g. MySQL) have an unsigned integer type. In that case\n+        (related_fields_match_type=True), the primary key should return its\n+        db_type.\n+        \"\"\"\n+        if connection.features.related_fields_match_type:\n+            return self.db_type(connection)\n+        else:\n+            return IntegerField().db_type(connection=connection)\n+\n+\n+class PositiveIntegerField(PositiveIntegerRelDbTypeMixin, IntegerField):\n+    description = _(\"Positive integer\")\n+\n+    def get_internal_type(self):\n+        return \"PositiveIntegerField\"\n+\n+    def formfield(self, **kwargs):\n+        return super().formfield(**{\n+            'min_value': 0,\n+            **kwargs,\n+        })\n+\n+\n+class PositiveSmallIntegerField(PositiveIntegerRelDbTypeMixin, IntegerField):\n+    description = _(\"Positive small integer\")\n+\n+    def get_internal_type(self):\n+        return \"PositiveSmallIntegerField\"\n+\n+    def formfield(self, **kwargs):\n+        return super().formfield(**{\n+            'min_value': 0,\n+            **kwargs,\n+        })\n+\n+\n+class SlugField(CharField):\n+    default_validators = [validators.validate_slug]\n+    description = _(\"Slug (up to %(max_length)s)\")\n+\n+    def __init__(self, *args, max_length=50, db_index=True, allow_unicode=False, **kwargs):\n+        self.allow_unicode = allow_unicode\n+        if self.allow_unicode:\n+            self.default_validators = [validators.validate_unicode_slug]\n+        super().__init__(*args, max_length=max_length, db_index=db_index, **kwargs)\n+\n+    def deconstruct(self):\n+        name, path, args, kwargs = super().deconstruct()\n+        if kwargs.get(\"max_length\") == 50:\n+            del kwargs['max_length']\n+        if self.db_index is False:\n+            kwargs['db_index'] = False\n+        else:\n+            del kwargs['db_index']\n+        if self.allow_unicode is not False:\n+            kwargs['allow_unicode'] = self.allow_unicode\n+        return name, path, args, kwargs\n+\n+    def get_internal_type(self):\n+        return \"SlugField\"\n+\n+    def formfield(self, **kwargs):\n+        return super().formfield(**{\n+            'form_class': forms.SlugField,\n+            'allow_unicode': self.allow_unicode,\n+            **kwargs,\n+        })\n+\n+\n+class SmallIntegerField(IntegerField):\n+    description = _(\"Small integer\")\n+\n+    def get_internal_type(self):\n+        return \"SmallIntegerField\"\n+\n+\n+class TextField(Field):\n+    description = _(\"Text\")\n+\n+    def get_internal_type(self):\n+        return \"TextField\"\n+\n+    def to_python(self, value):\n+        if isinstance(value, str) or value is None:\n+            return value\n+        return str(value)\n+\n+    def get_prep_value(self, value):\n+        value = super().get_prep_value(value)\n+        return self.to_python(value)\n+\n+    def formfield(self, **kwargs):\n+        # Passing max_length to forms.CharField means that the value's length\n+        # will be validated twice. This is considered acceptable since we want\n+        # the value in the form field (to pass into widget for example).\n+        return super().formfield(**{\n+            'max_length': self.max_length,\n+            **({} if self.choices is not None else {'widget': forms.Textarea}),\n+            **kwargs,\n+        })\n+\n+\n+class TimeField(DateTimeCheckMixin, Field):\n+    empty_strings_allowed = False\n+    default_error_messages = {\n+        'invalid': _(\"'%(value)s' value has an invalid format. It must be in \"\n+                     \"HH:MM[:ss[.uuuuuu]] format.\"),\n+        'invalid_time': _(\"'%(value)s' value has the correct format \"\n+                          \"(HH:MM[:ss[.uuuuuu]]) but it is an invalid time.\"),\n+    }\n+    description = _(\"Time\")\n+\n+    def __init__(self, verbose_name=None, name=None, auto_now=False,\n+                 auto_now_add=False, **kwargs):\n+        self.auto_now, self.auto_now_add = auto_now, auto_now_add\n+        if auto_now or auto_now_add:\n+            kwargs['editable'] = False\n+            kwargs['blank'] = True\n+        super().__init__(verbose_name, name, **kwargs)\n+\n+    def _check_fix_default_value(self):\n+        \"\"\"\n+        Warn that using an actual date or datetime value is probably wrong;\n+        it's only evaluated on server startup.\n+        \"\"\"\n+        if not self.has_default():\n+            return []\n+\n+        now = timezone.now()\n+        if not timezone.is_naive(now):\n+            now = timezone.make_naive(now, timezone.utc)\n+        value = self.default\n+        if isinstance(value, datetime.datetime):\n+            second_offset = datetime.timedelta(seconds=10)\n+            lower = now - second_offset\n+            upper = now + second_offset\n+            if timezone.is_aware(value):\n+                value = timezone.make_naive(value, timezone.utc)\n+        elif isinstance(value, datetime.time):\n+            second_offset = datetime.timedelta(seconds=10)\n+            lower = now - second_offset\n+            upper = now + second_offset\n+            value = datetime.datetime.combine(now.date(), value)\n+            if timezone.is_aware(value):\n+                value = timezone.make_naive(value, timezone.utc).time()\n+        else:\n+            # No explicit time / datetime value -- no checks necessary\n+            return []\n+        if lower <= value <= upper:\n+            return [\n+                checks.Warning(\n+                    'Fixed default value provided.',\n+                    hint='It seems you set a fixed date / time / datetime '\n+                         'value as default for this field. This may not be '\n+                         'what you want. If you want to have the current date '\n+                         'as default, use `django.utils.timezone.now`',\n+                    obj=self,\n+                    id='fields.W161',\n+                )\n+            ]\n+\n+        return []\n+\n+    def deconstruct(self):\n+        name, path, args, kwargs = super().deconstruct()\n+        if self.auto_now is not False:\n+            kwargs[\"auto_now\"] = self.auto_now\n+        if self.auto_now_add is not False:\n+            kwargs[\"auto_now_add\"] = self.auto_now_add\n+        if self.auto_now or self.auto_now_add:\n+            del kwargs['blank']\n+            del kwargs['editable']\n+        return name, path, args, kwargs\n+\n+    def get_internal_type(self):\n+        return \"TimeField\"\n+\n+    def to_python(self, value):\n+        if value is None:\n+            return None\n+        if isinstance(value, datetime.time):\n+            return value\n+        if isinstance(value, datetime.datetime):\n+            # Not usually a good idea to pass in a datetime here (it loses\n+            # information), but this can be a side-effect of interacting with a\n+            # database backend (e.g. Oracle), so we'll be accommodating.\n+            return value.time()\n+\n+        try:\n+            parsed = parse_time(value)\n+            if parsed is not None:\n+                return parsed\n+        except ValueError:\n+            raise exceptions.ValidationError(\n+                self.error_messages['invalid_time'],\n+                code='invalid_time',\n+                params={'value': value},\n+            )\n+\n+        raise exceptions.ValidationError(\n+            self.error_messages['invalid'],\n+            code='invalid',\n+            params={'value': value},\n+        )\n+\n+    def pre_save(self, model_instance, add):\n+        if self.auto_now or (self.auto_now_add and add):\n+            value = datetime.datetime.now().time()\n+            setattr(model_instance, self.attname, value)\n+            return value\n+        else:\n+            return super().pre_save(model_instance, add)\n+\n+    def get_prep_value(self, value):\n+        value = super().get_prep_value(value)\n+        return self.to_python(value)\n+\n+    def get_db_prep_value(self, value, connection, prepared=False):\n+        # Casts times into the format expected by the backend\n+        if not prepared:\n+            value = self.get_prep_value(value)\n+        return connection.ops.adapt_timefield_value(value)\n+\n+    def value_to_string(self, obj):\n+        val = self.value_from_object(obj)\n+        return '' if val is None else val.isoformat()\n+\n+    def formfield(self, **kwargs):\n+        return super().formfield(**{\n+            'form_class': forms.TimeField,\n+            **kwargs,\n+        })\n+\n+\n+class URLField(CharField):\n+    default_validators = [validators.URLValidator()]\n+    description = _(\"URL\")\n+\n+    def __init__(self, verbose_name=None, name=None, **kwargs):\n+        kwargs.setdefault('max_length', 200)\n+        super().__init__(verbose_name, name, **kwargs)\n+\n+    def deconstruct(self):\n+        name, path, args, kwargs = super().deconstruct()\n+        if kwargs.get(\"max_length\") == 200:\n+            del kwargs['max_length']\n+        return name, path, args, kwargs\n+\n+    def formfield(self, **kwargs):\n+        # As with CharField, this will cause URL validation to be performed\n+        # twice.\n+        return super().formfield(**{\n+            'form_class': forms.URLField,\n+            **kwargs,\n+        })\n+\n+\n+class BinaryField(Field):\n+    description = _(\"Raw binary data\")\n+    empty_values = [None, b'']\n+\n+    def __init__(self, *args, **kwargs):\n+        kwargs.setdefault('editable', False)\n+        super().__init__(*args, **kwargs)\n+        if self.max_length is not None:\n+            self.validators.append(validators.MaxLengthValidator(self.max_length))\n+\n+    def check(self, **kwargs):\n+        return [*super().check(**kwargs), *self._check_str_default_value()]\n+\n+    def _check_str_default_value(self):\n+        if self.has_default() and isinstance(self.default, str):\n+            return [\n+                checks.Error(\n+                    \"BinaryField's default cannot be a string. Use bytes \"\n+                    \"content instead.\",\n+                    obj=self,\n+                    id='fields.E170',\n+                )\n+            ]\n+        return []\n+\n+    def deconstruct(self):\n+        name, path, args, kwargs = super().deconstruct()\n+        if self.editable:\n+            kwargs['editable'] = True\n+        else:\n+            del kwargs['editable']\n+        return name, path, args, kwargs\n+\n+    def get_internal_type(self):\n+        return \"BinaryField\"\n+\n+    def get_placeholder(self, value, compiler, connection):\n+        return connection.ops.binary_placeholder_sql(value)\n+\n+    def get_default(self):\n+        if self.has_default() and not callable(self.default):\n+            return self.default\n+        default = super().get_default()\n+        if default == '':\n+            return b''\n+        return default\n+\n+    def get_db_prep_value(self, value, connection, prepared=False):\n+        value = super().get_db_prep_value(value, connection, prepared)\n+        if value is not None:\n+            return connection.Database.Binary(value)\n+        return value\n+\n+    def value_to_string(self, obj):\n+        \"\"\"Binary data is serialized as base64\"\"\"\n+        return b64encode(self.value_from_object(obj)).decode('ascii')\n+\n+    def to_python(self, value):\n+        # If it's a string, it should be base64-encoded data\n+        if isinstance(value, str):\n+            return memoryview(b64decode(value.encode('ascii')))\n+        return value\n+\n+\n+class UUIDField(Field):\n+    default_error_messages = {\n+        'invalid': _(\"'%(value)s' is not a valid UUID.\"),\n+    }\n+    description = _('Universally unique identifier')\n+    empty_strings_allowed = False\n+\n+    def __init__(self, verbose_name=None, **kwargs):\n+        kwargs['max_length'] = 32\n+        super().__init__(verbose_name, **kwargs)\n+\n+    def deconstruct(self):\n+        name, path, args, kwargs = super().deconstruct()\n+        del kwargs['max_length']\n+        return name, path, args, kwargs\n+\n+    def get_internal_type(self):\n+        return \"UUIDField\"\n+\n+    def get_prep_value(self, value):\n+        value = super().get_prep_value(value)\n+        return self.to_python(value)\n+\n+    def get_db_prep_value(self, value, connection, prepared=False):\n+        if value is None:\n+            return None\n+        if not isinstance(value, uuid.UUID):\n+            value = self.to_python(value)\n+\n+        if connection.features.has_native_uuid_field:\n+            return value\n+        return value.hex\n+\n+    def to_python(self, value):\n+        if value is not None and not isinstance(value, uuid.UUID):\n+            input_form = 'int' if isinstance(value, int) else 'hex'\n+            try:\n+                return uuid.UUID(**{input_form: value})\n+            except (AttributeError, ValueError):\n+                raise exceptions.ValidationError(\n+                    self.error_messages['invalid'],\n+                    code='invalid',\n+                    params={'value': value},\n+                )\n+        return value\n+\n+    def formfield(self, **kwargs):\n+        return super().formfield(**{\n+            'form_class': forms.UUIDField,\n+            **kwargs,\n+        })\ndiff --git a/.harness/checkpoints/3a278b5881b0/--Users--huetuanthi--dev--agents--harness--django--forms--fields.py b/.harness/checkpoints/3a278b5881b0/--Users--huetuanthi--dev--agents--harness--django--forms--fields.py\nnew file mode 100644\nindex 0000000000..a977256525\n--- /dev/null\n+++ b/.harness/checkpoints/3a278b5881b0/--Users--huetuanthi--dev--agents--harness--django--forms--fields.py\n@@ -0,0 +1,1202 @@\n+\"\"\"\n+Field classes.\n+\"\"\"\n+\n+import copy\n+import datetime\n+import math\n+import operator\n+import os\n+import re\n+import uuid\n+from decimal import Decimal, DecimalException\n+from io import BytesIO\n+from urllib.parse import urlsplit, urlunsplit\n+\n+from django.core import validators\n+from django.core.exceptions import ValidationError\n+# Provide this import for backwards compatibility.\n+from django.core.validators import EMPTY_VALUES  # NOQA\n+from django.forms.boundfield import BoundField\n+from django.forms.utils import from_current_timezone, to_current_timezone\n+from django.forms.widgets import (\n+    FILE_INPUT_CONTRADICTION, CheckboxInput, ClearableFileInput, DateInput,\n+    DateTimeInput, EmailInput, FileInput, HiddenInput, MultipleHiddenInput,\n+    NullBooleanSelect, NumberInput, Select, SelectMultiple,\n+    SplitDateTimeWidget, SplitHiddenDateTimeWidget, TextInput, TimeInput,\n+    URLInput,\n+)\n+from django.utils import formats\n+from django.utils.dateparse import parse_duration\n+from django.utils.duration import duration_string\n+from django.utils.ipv6 import clean_ipv6_address\n+from django.utils.translation import gettext_lazy as _, ngettext_lazy\n+\n+__all__ = (\n+    'Field', 'CharField', 'IntegerField',\n+    'DateField', 'TimeField', 'DateTimeField', 'DurationField',\n+    'RegexField', 'EmailField', 'FileField', 'ImageField', 'URLField',\n+    'BooleanField', 'NullBooleanField', 'ChoiceField', 'MultipleChoiceField',\n+    'ComboField', 'MultiValueField', 'FloatField', 'DecimalField',\n+    'SplitDateTimeField', 'GenericIPAddressField', 'FilePathField',\n+    'SlugField', 'TypedChoiceField', 'TypedMultipleChoiceField', 'UUIDField',\n+)\n+\n+\n+class Field:\n+    widget = TextInput  # Default widget to use when rendering this type of Field.\n+    hidden_widget = HiddenInput  # Default widget to use when rendering this as \"hidden\".\n+    default_validators = []  # Default set of validators\n+    # Add an 'invalid' entry to default_error_message if you want a specific\n+    # field error message not raised by the field validators.\n+    default_error_messages = {\n+        'required': _('This field is required.'),\n+    }\n+    empty_values = list(validators.EMPTY_VALUES)\n+\n+    def __init__(self, *, required=True, widget=None, label=None, initial=None,\n+                 help_text='', error_messages=None, show_hidden_initial=False,\n+                 validators=(), localize=False, disabled=False, label_suffix=None):\n+        # required -- Boolean that specifies whether the field is required.\n+        #             True by default.\n+        # widget -- A Widget class, or instance of a Widget class, that should\n+        #           be used for this Field when displaying it. Each Field has a\n+        #           default Widget that it'll use if you don't specify this. In\n+        #           most cases, the default widget is TextInput.\n+        # label -- A verbose name for this field, for use in displaying this\n+        #          field in a form. By default, Django will use a \"pretty\"\n+        #          version of the form field name, if the Field is part of a\n+        #          Form.\n+        # initial -- A value to use in this Field's initial display. This value\n+        #            is *not* used as a fallback if data isn't given.\n+        # help_text -- An optional string to use as \"help text\" for this Field.\n+        # error_messages -- An optional dictionary to override the default\n+        #                   messages that the field will raise.\n+        # show_hidden_initial -- Boolean that specifies if it is needed to render a\n+        #                        hidden widget with initial value after widget.\n+        # validators -- List of additional validators to use\n+        # localize -- Boolean that specifies if the field should be localized.\n+        # disabled -- Boolean that specifies whether the field is disabled, that\n+        #             is its widget is shown in the form but not editable.\n+        # label_suffix -- Suffix to be added to the label. Overrides\n+        #                 form's label_suffix.\n+        self.required, self.label, self.initial = required, label, initial\n+        self.show_hidden_initial = show_hidden_initial\n+        self.help_text = help_text\n+        self.disabled = disabled\n+        self.label_suffix = label_suffix\n+        widget = widget or self.widget\n+        if isinstance(widget, type):\n+            widget = widget()\n+        else:\n+            widget = copy.deepcopy(widget)\n+\n+        # Trigger the localization machinery if needed.\n+        self.localize = localize\n+        if self.localize:\n+            widget.is_localized = True\n+\n+        # Let the widget know whether it should display as required.\n+        widget.is_required = self.required\n+\n+        # Hook into self.widget_attrs() for any Field-specific HTML attributes.\n+        extra_attrs = self.widget_attrs(widget)\n+        if extra_attrs:\n+            widget.attrs.update(extra_attrs)\n+\n+        self.widget = widget\n+\n+        messages = {}\n+        for c in reversed(self.__class__.__mro__):\n+            messages.update(getattr(c, 'default_error_messages', {}))\n+        messages.update(error_messages or {})\n+        self.error_messages = messages\n+\n+        self.validators = [*self.default_validators, *validators]\n+\n+        super().__init__()\n+\n+    def prepare_value(self, value):\n+        return value\n+\n+    def to_python(self, value):\n+        return value\n+\n+    def validate(self, value):\n+        if value in self.empty_values and self.required:\n+            raise ValidationError(self.error_messages['required'], code='required')\n+\n+    def run_validators(self, value):\n+        if value in self.empty_values:\n+            return\n+        errors = []\n+        for v in self.validators:\n+            try:\n+                v(value)\n+            except ValidationError as e:\n+                if hasattr(e, 'code') and e.code in self.error_messages:\n+                    e.message = self.error_messages[e.code]\n+                errors.extend(e.error_list)\n+        if errors:\n+            raise ValidationError(errors)\n+\n+    def clean(self, value):\n+        \"\"\"\n+        Validate the given value and return its \"cleaned\" value as an\n+        appropriate Python object. Raise ValidationError for any errors.\n+        \"\"\"\n+        value = self.to_python(value)\n+        self.validate(value)\n+        self.run_validators(value)\n+        return value\n+\n+    def bound_data(self, data, initial):\n+        \"\"\"\n+        Return the value that should be shown for this field on render of a\n+        bound form, given the submitted POST data for the field and the initial\n+        data, if any.\n+\n+        For most fields, this will simply be data; FileFields need to handle it\n+        a bit differently.\n+        \"\"\"\n+        if self.disabled:\n+            return initial\n+        return data\n+\n+    def widget_attrs(self, widget):\n+        \"\"\"\n+        Given a Widget instance (*not* a Widget class), return a dictionary of\n+        any HTML attributes that should be added to the Widget, based on this\n+        Field.\n+        \"\"\"\n+        return {}\n+\n+    def has_changed(self, initial, data):\n+        \"\"\"Return True if data differs from initial.\"\"\"\n+        # Always return False if the field is disabled since self.bound_data\n+        # always uses the initial value in this case.\n+        if self.disabled:\n+            return False\n+        try:\n+            data = self.to_python(data)\n+            if hasattr(self, '_coerce'):\n+                return self._coerce(data) != self._coerce(initial)\n+        except ValidationError:\n+            return True\n+        # For purposes of seeing whether something has changed, None is\n+        # the same as an empty string, if the data or initial value we get\n+        # is None, replace it with ''.\n+        initial_value = initial if initial is not None else ''\n+        data_value = data if data is not None else ''\n+        return initial_value != data_value\n+\n+    def get_bound_field(self, form, field_name):\n+        \"\"\"\n+        Return a BoundField instance that will be used when accessing the form\n+        field in a template.\n+        \"\"\"\n+        return BoundField(form, self, field_name)\n+\n+    def __deepcopy__(self, memo):\n+        result = copy.copy(self)\n+        memo[id(self)] = result\n+        result.widget = copy.deepcopy(self.widget, memo)\n+        result.validators = self.validators[:]\n+        return result\n+\n+\n+class CharField(Field):\n+    def __init__(self, *, max_length=None, min_length=None, strip=True, empty_value='', **kwargs):\n+        self.max_length = max_length\n+        self.min_length = min_length\n+        self.strip = strip\n+        self.empty_value = empty_value\n+        super().__init__(**kwargs)\n+        if min_length is not None:\n+            self.validators.append(validators.MinLengthValidator(int(min_length)))\n+        if max_length is not None:\n+            self.validators.append(validators.MaxLengthValidator(int(max_length)))\n+        self.validators.append(validators.ProhibitNullCharactersValidator())\n+\n+    def to_python(self, value):\n+        \"\"\"Return a string.\"\"\"\n+        if value not in self.empty_values:\n+            value = str(value)\n+            if self.strip:\n+                value = value.strip()\n+        if value in self.empty_values:\n+            return self.empty_value\n+        return value\n+\n+    def widget_attrs(self, widget):\n+        attrs = super().widget_attrs(widget)\n+        if self.max_length is not None and not widget.is_hidden:\n+            # The HTML attribute is maxlength, not max_length.\n+            attrs['maxlength'] = str(self.max_length)\n+        if self.min_length is not None and not widget.is_hidden:\n+            # The HTML attribute is minlength, not min_length.\n+            attrs['minlength'] = str(self.min_length)\n+        return attrs\n+\n+\n+class IntegerField(Field):\n+    widget = NumberInput\n+    default_error_messages = {\n+        'invalid': _('Enter a whole number.'),\n+    }\n+    re_decimal = re.compile(r'\\.0*\\s*$')\n+\n+    def __init__(self, *, max_value=None, min_value=None, **kwargs):\n+        self.max_value, self.min_value = max_value, min_value\n+        if kwargs.get('localize') and self.widget == NumberInput:\n+            # Localized number input is not well supported on most browsers\n+            kwargs.setdefault('widget', super().widget)\n+        super().__init__(**kwargs)\n+\n+        if max_value is not None:\n+            self.validators.append(validators.MaxValueValidator(max_value))\n+        if min_value is not None:\n+            self.validators.append(validators.MinValueValidator(min_value))\n+\n+    def to_python(self, value):\n+        \"\"\"\n+        Validate that int() can be called on the input. Return the result\n+        of int() or None for empty values.\n+        \"\"\"\n+        value = super().to_python(value)\n+        if value in self.empty_values:\n+            return None\n+        if self.localize:\n+            value = formats.sanitize_separators(value)\n+        # Strip trailing decimal and zeros.\n+        try:\n+            value = int(self.re_decimal.sub('', str(value)))\n+        except (ValueError, TypeError):\n+            raise ValidationError(self.error_messages['invalid'], code='invalid')\n+        return value\n+\n+    def widget_attrs(self, widget):\n+        attrs = super().widget_attrs(widget)\n+        if isinstance(widget, NumberInput):\n+            if self.min_value is not None:\n+                attrs['min'] = self.min_value\n+            if self.max_value is not None:\n+                attrs['max'] = self.max_value\n+        return attrs\n+\n+\n+class FloatField(IntegerField):\n+    default_error_messages = {\n+        'invalid': _('Enter a number.'),\n+    }\n+\n+    def to_python(self, value):\n+        \"\"\"\n+        Validate that float() can be called on the input. Return the result\n+        of float() or None for empty values.\n+        \"\"\"\n+        value = super(IntegerField, self).to_python(value)\n+        if value in self.empty_values:\n+            return None\n+        if self.localize:\n+            value = formats.sanitize_separators(value)\n+        try:\n+            value = float(value)\n+        except (ValueError, TypeError):\n+            raise ValidationError(self.error_messages['invalid'], code='invalid')\n+        return value\n+\n+    def validate(self, value):\n+        super().validate(value)\n+        if value in self.empty_values:\n+            return\n+        if not math.isfinite(value):\n+            raise ValidationError(self.error_messages['invalid'], code='invalid')\n+\n+    def widget_attrs(self, widget):\n+        attrs = super().widget_attrs(widget)\n+        if isinstance(widget, NumberInput) and 'step' not in widget.attrs:\n+            attrs.setdefault('step', 'any')\n+        return attrs\n+\n+\n+class DecimalField(IntegerField):\n+    default_error_messages = {\n+        'invalid': _('Enter a number.'),\n+    }\n+\n+    def __init__(self, *, max_value=None, min_value=None, max_digits=None, decimal_places=None, **kwargs):\n+        self.max_digits, self.decimal_places = max_digits, decimal_places\n+        super().__init__(max_value=max_value, min_value=min_value, **kwargs)\n+        self.validators.append(validators.DecimalValidator(max_digits, decimal_places))\n+\n+    def to_python(self, value):\n+        \"\"\"\n+        Validate that the input is a decimal number. Return a Decimal\n+        instance or None for empty values. Ensure that there are no more\n+        than max_digits in the number and no more than decimal_places digits\n+        after the decimal point.\n+        \"\"\"\n+        if value in self.empty_values:\n+            return None\n+        if self.localize:\n+            value = formats.sanitize_separators(value)\n+        value = str(value).strip()\n+        try:\n+            value = Decimal(value)\n+        except DecimalException:\n+            raise ValidationError(self.error_messages['invalid'], code='invalid')\n+        return value\n+\n+    def validate(self, value):\n+        super().validate(value)\n+        if value in self.empty_values:\n+            return\n+        if not value.is_finite():\n+            raise ValidationError(self.error_messages['invalid'], code='invalid')\n+\n+    def widget_attrs(self, widget):\n+        attrs = super().widget_attrs(widget)\n+        if isinstance(widget, NumberInput) and 'step' not in widget.attrs:\n+            if self.decimal_places is not None:\n+                # Use exponential notation for small values since they might\n+                # be parsed as 0 otherwise. ref #20765\n+                step = str(Decimal(1).scaleb(-self.decimal_places)).lower()\n+            else:\n+                step = 'any'\n+            attrs.setdefault('step', step)\n+        return attrs\n+\n+\n+class BaseTemporalField(Field):\n+\n+    def __init__(self, *, input_formats=None, **kwargs):\n+        super().__init__(**kwargs)\n+        if input_formats is not None:\n+            self.input_formats = input_formats\n+\n+    def to_python(self, value):\n+        value = value.strip()\n+        # Try to strptime against each input format.\n+        for format in self.input_formats:\n+            try:\n+                return self.strptime(value, format)\n+            except (ValueError, TypeError):\n+                continue\n+        raise ValidationError(self.error_messages['invalid'], code='invalid')\n+\n+    def strptime(self, value, format):\n+        raise NotImplementedError('Subclasses must define this method.')\n+\n+\n+class DateField(BaseTemporalField):\n+    widget = DateInput\n+    input_formats = formats.get_format_lazy('DATE_INPUT_FORMATS')\n+    default_error_messages = {\n+        'invalid': _('Enter a valid date.'),\n+    }\n+\n+    def to_python(self, value):\n+        \"\"\"\n+        Validate that the input can be converted to a date. Return a Python\n+        datetime.date object.\n+        \"\"\"\n+        if value in self.empty_values:\n+            return None\n+        if isinstance(value, datetime.datetime):\n+            return value.date()\n+        if isinstance(value, datetime.date):\n+            return value\n+        return super().to_python(value)\n+\n+    def strptime(self, value, format):\n+        return datetime.datetime.strptime(value, format).date()\n+\n+\n+class TimeField(BaseTemporalField):\n+    widget = TimeInput\n+    input_formats = formats.get_format_lazy('TIME_INPUT_FORMATS')\n+    default_error_messages = {\n+        'invalid': _('Enter a valid time.')\n+    }\n+\n+    def to_python(self, value):\n+        \"\"\"\n+        Validate that the input can be converted to a time. Return a Python\n+        datetime.time object.\n+        \"\"\"\n+        if value in self.empty_values:\n+            return None\n+        if isinstance(value, datetime.time):\n+            return value\n+        return super().to_python(value)\n+\n+    def strptime(self, value, format):\n+        return datetime.datetime.strptime(value, format).time()\n+\n+\n+class DateTimeField(BaseTemporalField):\n+    widget = DateTimeInput\n+    input_formats = formats.get_format_lazy('DATETIME_INPUT_FORMATS')\n+    default_error_messages = {\n+        'invalid': _('Enter a valid date/time.'),\n+    }\n+\n+    def prepare_value(self, value):\n+        if isinstance(value, datetime.datetime):\n+            value = to_current_timezone(value)\n+        return value\n+\n+    def to_python(self, value):\n+        \"\"\"\n+        Validate that the input can be converted to a datetime. Return a\n+        Python datetime.datetime object.\n+        \"\"\"\n+        if value in self.empty_values:\n+            return None\n+        if isinstance(value, datetime.datetime):\n+            return from_current_timezone(value)\n+        if isinstance(value, datetime.date):\n+            result = datetime.datetime(value.year, value.month, value.day)\n+            return from_current_timezone(result)\n+        result = super().to_python(value)\n+        return from_current_timezone(result)\n+\n+    def strptime(self, value, format):\n+        return datetime.datetime.strptime(value, format)\n+\n+\n+class DurationField(Field):\n+    default_error_messages = {\n+        'invalid': _('Enter a valid duration.'),\n+        'overflow': _('The number of days must be between {min_days} and {max_days}.')\n+    }\n+\n+    def prepare_value(self, value):\n+        if isinstance(value, datetime.timedelta):\n+            return duration_string(value)\n+        return value\n+\n+    def to_python(self, value):\n+        if value in self.empty_values:\n+            return None\n+        if isinstance(value, datetime.timedelta):\n+            return value\n+        try:\n+            value = parse_duration(str(value))\n+        except OverflowError:\n+            raise ValidationError(self.error_messages['overflow'].format(\n+                min_days=datetime.timedelta.min.days,\n+                max_days=datetime.timedelta.max.days,\n+            ), code='overflow')\n+        if value is None:\n+            raise ValidationError(self.error_messages['invalid'], code='invalid')\n+        return value\n+\n+\n+class RegexField(CharField):\n+    def __init__(self, regex, **kwargs):\n+        \"\"\"\n+        regex can be either a string or a compiled regular expression object.\n+        \"\"\"\n+        kwargs.setdefault('strip', False)\n+        super().__init__(**kwargs)\n+        self._set_regex(regex)\n+\n+    def _get_regex(self):\n+        return self._regex\n+\n+    def _set_regex(self, regex):\n+        if isinstance(regex, str):\n+            regex = re.compile(regex)\n+        self._regex = regex\n+        if hasattr(self, '_regex_validator') and self._regex_validator in self.validators:\n+            self.validators.remove(self._regex_validator)\n+        self._regex_validator = validators.RegexValidator(regex=regex)\n+        self.validators.append(self._regex_validator)\n+\n+    regex = property(_get_regex, _set_regex)\n+\n+\n+class EmailField(CharField):\n+    widget = EmailInput\n+    default_validators = [validators.validate_email]\n+\n+    def __init__(self, **kwargs):\n+        super().__init__(strip=True, **kwargs)\n+\n+\n+class FileField(Field):\n+    widget = ClearableFileInput\n+    default_error_messages = {\n+        'invalid': _(\"No file was submitted. Check the encoding type on the form.\"),\n+        'missing': _(\"No file was submitted.\"),\n+        'empty': _(\"The submitted file is empty.\"),\n+        'max_length': ngettext_lazy(\n+            'Ensure this filename has at most %(max)d character (it has %(length)d).',\n+            'Ensure this filename has at most %(max)d characters (it has %(length)d).',\n+            'max'),\n+        'contradiction': _('Please either submit a file or check the clear checkbox, not both.')\n+    }\n+\n+    def __init__(self, *, max_length=None, allow_empty_file=False, **kwargs):\n+        self.max_length = max_length\n+        self.allow_empty_file = allow_empty_file\n+        super().__init__(**kwargs)\n+\n+    def to_python(self, data):\n+        if data in self.empty_values:\n+            return None\n+\n+        # UploadedFile objects should have name and size attributes.\n+        try:\n+            file_name = data.name\n+            file_size = data.size\n+        except AttributeError:\n+            raise ValidationError(self.error_messages['invalid'], code='invalid')\n+\n+        if self.max_length is not None and len(file_name) > self.max_length:\n+            params = {'max': self.max_length, 'length': len(file_name)}\n+            raise ValidationError(self.error_messages['max_length'], code='max_length', params=params)\n+        if not file_name:\n+            raise ValidationError(self.error_messages['invalid'], code='invalid')\n+        if not self.allow_empty_file and not file_size:\n+            raise ValidationError(self.error_messages['empty'], code='empty')\n+\n+        return data\n+\n+    def clean(self, data, initial=None):\n+        # If the widget got contradictory inputs, we raise a validation error\n+        if data is FILE_INPUT_CONTRADICTION:\n+            raise ValidationError(self.error_messages['contradiction'], code='contradiction')\n+        # False means the field value should be cleared; further validation is\n+        # not needed.\n+        if data is False:\n+            if not self.required:\n+                return False\n+            # If the field is required, clearing is not possible (the widget\n+            # shouldn't return False data in that case anyway). False is not\n+            # in self.empty_value; if a False value makes it this far\n+            # it should be validated from here on out as None (so it will be\n+            # caught by the required check).\n+            data = None\n+        if not data and initial:\n+            return initial\n+        return super().clean(data)\n+\n+    def bound_data(self, data, initial):\n+        if data in (None, FILE_INPUT_CONTRADICTION):\n+            return initial\n+        return data\n+\n+    def has_changed(self, initial, data):\n+        return not self.disabled and data is not None\n+\n+\n+class ImageField(FileField):\n+    default_validators = [validators.validate_image_file_extension]\n+    default_error_messages = {\n+        'invalid_image': _(\n+            \"Upload a valid image. The file you uploaded was either not an \"\n+            \"image or a corrupted image.\"\n+        ),\n+    }\n+\n+    def to_python(self, data):\n+        \"\"\"\n+        Check that the file-upload field data contains a valid image (GIF, JPG,\n+        PNG, etc. -- whatever Pillow supports).\n+        \"\"\"\n+        f = super().to_python(data)\n+        if f is None:\n+            return None\n+\n+        from PIL import Image\n+\n+        # We need to get a file object for Pillow. We might have a path or we might\n+        # have to read the data into memory.\n+        if hasattr(data, 'temporary_file_path'):\n+            file = data.temporary_file_path()\n+        else:\n+            if hasattr(data, 'read'):\n+                file = BytesIO(data.read())\n+            else:\n+                file = BytesIO(data['content'])\n+\n+        try:\n+            # load() could spot a truncated JPEG, but it loads the entire\n+            # image in memory, which is a DoS vector. See #3848 and #18520.\n+            image = Image.open(file)\n+            # verify() must be called immediately after the constructor.\n+            image.verify()\n+\n+            # Annotating so subclasses can reuse it for their own validation\n+            f.image = image\n+            # Pillow doesn't detect the MIME type of all formats. In those\n+            # cases, content_type will be None.\n+            f.content_type = Image.MIME.get(image.format)\n+        except Exception as exc:\n+            # Pillow doesn't recognize it as an image.\n+            raise ValidationError(\n+                self.error_messages['invalid_image'],\n+                code='invalid_image',\n+            ) from exc\n+        if hasattr(f, 'seek') and callable(f.seek):\n+            f.seek(0)\n+        return f\n+\n+    def widget_attrs(self, widget):\n+        attrs = super().widget_attrs(widget)\n+        if isinstance(widget, FileInput) and 'accept' not in widget.attrs:\n+            attrs.setdefault('accept', 'image/*')\n+        return attrs\n+\n+\n+class URLField(CharField):\n+    widget = URLInput\n+    default_error_messages = {\n+        'invalid': _('Enter a valid URL.'),\n+    }\n+    default_validators = [validators.URLValidator()]\n+\n+    def __init__(self, **kwargs):\n+        super().__init__(strip=True, **kwargs)\n+\n+    def to_python(self, value):\n+\n+        def split_url(url):\n+            \"\"\"\n+            Return a list of url parts via urlparse.urlsplit(), or raise\n+            ValidationError for some malformed URLs.\n+            \"\"\"\n+            try:\n+                return list(urlsplit(url))\n+            except ValueError:\n+                # urlparse.urlsplit can raise a ValueError with some\n+                # misformatted URLs.\n+                raise ValidationError(self.error_messages['invalid'], code='invalid')\n+\n+        value = super().to_python(value)\n+        if value:\n+            url_fields = split_url(value)\n+            if not url_fields[0]:\n+                # If no URL scheme given, assume http://\n+                url_fields[0] = 'http'\n+            if not url_fields[1]:\n+                # Assume that if no domain is provided, that the path segment\n+                # contains the domain.\n+                url_fields[1] = url_fields[2]\n+                url_fields[2] = ''\n+                # Rebuild the url_fields list, since the domain segment may now\n+                # contain the path too.\n+                url_fields = split_url(urlunsplit(url_fields))\n+            value = urlunsplit(url_fields)\n+        return value\n+\n+\n+class BooleanField(Field):\n+    widget = CheckboxInput\n+\n+    def to_python(self, value):\n+        \"\"\"Return a Python boolean object.\"\"\"\n+        # Explicitly check for the string 'False', which is what a hidden field\n+        # will submit for False. Also check for '0', since this is what\n+        # RadioSelect will provide. Because bool(\"True\") == bool('1') == True,\n+        # we don't need to handle that explicitly.\n+        if isinstance(value, str) and value.lower() in ('false', '0'):\n+            value = False\n+        else:\n+            value = bool(value)\n+        return super().to_python(value)\n+\n+    def validate(self, value):\n+        if not value and self.required:\n+            raise ValidationError(self.error_messages['required'], code='required')\n+\n+    def has_changed(self, initial, data):\n+        if self.disabled:\n+            return False\n+        # Sometimes data or initial may be a string equivalent of a boolean\n+        # so we should run it through to_python first to get a boolean value\n+        return self.to_python(initial) != self.to_python(data)\n+\n+\n+class NullBooleanField(BooleanField):\n+    \"\"\"\n+    A field whose valid values are None, True, and False. Clean invalid values\n+    to None.\n+    \"\"\"\n+    widget = NullBooleanSelect\n+\n+    def to_python(self, value):\n+        \"\"\"\n+        Explicitly check for the string 'True' and 'False', which is what a\n+        hidden field will submit for True and False, for 'true' and 'false',\n+        which are likely to be returned by JavaScript serializations of forms,\n+        and for '1' and '0', which is what a RadioField will submit. Unlike\n+        the Booleanfield, this field must check for True because it doesn't\n+        use the bool() function.\n+        \"\"\"\n+        if value in (True, 'True', 'true', '1'):\n+            return True\n+        elif value in (False, 'False', 'false', '0'):\n+            return False\n+        else:\n+            return None\n+\n+    def validate(self, value):\n+        pass\n+\n+\n+class CallableChoiceIterator:\n+    def __init__(self, choices_func):\n+        self.choices_func = choices_func\n+\n+    def __iter__(self):\n+        yield from self.choices_func()\n+\n+\n+class ChoiceField(Field):\n+    widget = Select\n+    default_error_messages = {\n+        'invalid_choice': _('Select a valid choice. %(value)s is not one of the available choices.'),\n+    }\n+\n+    def __init__(self, *, choices=(), **kwargs):\n+        super().__init__(**kwargs)\n+        self.choices = choices\n+\n+    def __deepcopy__(self, memo):\n+        result = super().__deepcopy__(memo)\n+        result._choices = copy.deepcopy(self._choices, memo)\n+        return result\n+\n+    def _get_choices(self):\n+        return self._choices\n+\n+    def _set_choices(self, value):\n+        # Setting choices also sets the choices on the widget.\n+        # choices can be any iterable, but we call list() on it because\n+        # it will be consumed more than once.\n+        if callable(value):\n+            value = CallableChoiceIterator(value)\n+        else:\n+            value = list(value)\n+\n+        self._choices = self.widget.choices = value\n+\n+    choices = property(_get_choices, _set_choices)\n+\n+    def to_python(self, value):\n+        \"\"\"Return a string.\"\"\"\n+        if value in self.empty_values:\n+            return ''\n+        return str(value)\n+\n+    def validate(self, value):\n+        \"\"\"Validate that the input is in self.choices.\"\"\"\n+        super().validate(value)\n+        if value and not self.valid_value(value):\n+            raise ValidationError(\n+                self.error_messages['invalid_choice'],\n+                code='invalid_choice',\n+                params={'value': value},\n+            )\n+\n+    def valid_value(self, value):\n+        \"\"\"Check to see if the provided value is a valid choice.\"\"\"\n+        text_value = str(value)\n+        for k, v in self.choices:\n+            if isinstance(v, (list, tuple)):\n+                # This is an optgroup, so look inside the group for options\n+                for k2, v2 in v:\n+                    if value == k2 or text_value == str(k2):\n+                        return True\n+            else:\n+                if value == k or text_value == str(k):\n+                    return True\n+        return False\n+\n+\n+class TypedChoiceField(ChoiceField):\n+    def __init__(self, *, coerce=lambda val: val, empty_value='', **kwargs):\n+        self.coerce = coerce\n+        self.empty_value = empty_value\n+        super().__init__(**kwargs)\n+\n+    def _coerce(self, value):\n+        \"\"\"\n+        Validate that the value can be coerced to the right type (if not empty).\n+        \"\"\"\n+        if value == self.empty_value or value in self.empty_values:\n+            return self.empty_value\n+        try:\n+            value = self.coerce(value)\n+        except (ValueError, TypeError, ValidationError):\n+            raise ValidationError(\n+                self.error_messages['invalid_choice'],\n+                code='invalid_choice',\n+                params={'value': value},\n+            )\n+        return value\n+\n+    def clean(self, value):\n+        value = super().clean(value)\n+        return self._coerce(value)\n+\n+\n+class MultipleChoiceField(ChoiceField):\n+    hidden_widget = MultipleHiddenInput\n+    widget = SelectMultiple\n+    default_error_messages = {\n+        'invalid_choice': _('Select a valid choice. %(value)s is not one of the available choices.'),\n+        'invalid_list': _('Enter a list of values.'),\n+    }\n+\n+    def to_python(self, value):\n+        if not value:\n+            return []\n+        elif not isinstance(value, (list, tuple)):\n+            raise ValidationError(self.error_messages['invalid_list'], code='invalid_list')\n+        return [str(val) for val in value]\n+\n+    def validate(self, value):\n+        \"\"\"Validate that the input is a list or tuple.\"\"\"\n+        if self.required and not value:\n+            raise ValidationError(self.error_messages['required'], code='required')\n+        # Validate that each value in the value list is in self.choices.\n+        for val in value:\n+            if not self.valid_value(val):\n+                raise ValidationError(\n+                    self.error_messages['invalid_choice'],\n+                    code='invalid_choice',\n+                    params={'value': val},\n+                )\n+\n+    def has_changed(self, initial, data):\n+        if self.disabled:\n+            return False\n+        if initial is None:\n+            initial = []\n+        if data is None:\n+            data = []\n+        if len(initial) != len(data):\n+            return True\n+        initial_set = {str(value) for value in initial}\n+        data_set = {str(value) for value in data}\n+        return data_set != initial_set\n+\n+\n+class TypedMultipleChoiceField(MultipleChoiceField):\n+    def __init__(self, *, coerce=lambda val: val, **kwargs):\n+        self.coerce = coerce\n+        self.empty_value = kwargs.pop('empty_value', [])\n+        super().__init__(**kwargs)\n+\n+    def _coerce(self, value):\n+        \"\"\"\n+        Validate that the values are in self.choices and can be coerced to the\n+        right type.\n+        \"\"\"\n+        if value == self.empty_value or value in self.empty_values:\n+            return self.empty_value\n+        new_value = []\n+        for choice in value:\n+            try:\n+                new_value.append(self.coerce(choice))\n+            except (ValueError, TypeError, ValidationError):\n+                raise ValidationError(\n+                    self.error_messages['invalid_choice'],\n+                    code='invalid_choice',\n+                    params={'value': choice},\n+                )\n+        return new_value\n+\n+    def clean(self, value):\n+        value = super().clean(value)\n+        return self._coerce(value)\n+\n+    def validate(self, value):\n+        if value != self.empty_value:\n+            super().validate(value)\n+        elif self.required:\n+            raise ValidationError(self.error_messages['required'], code='required')\n+\n+\n+class ComboField(Field):\n+    \"\"\"\n+    A Field whose clean() method calls multiple Field clean() methods.\n+    \"\"\"\n+    def __init__(self, fields, **kwargs):\n+        super().__init__(**kwargs)\n+        # Set 'required' to False on the individual fields, because the\n+        # required validation will be handled by ComboField, not by those\n+        # individual fields.\n+        for f in fields:\n+            f.required = False\n+        self.fields = fields\n+\n+    def clean(self, value):\n+        \"\"\"\n+        Validate the given value against all of self.fields, which is a\n+        list of Field instances.\n+        \"\"\"\n+        super().clean(value)\n+        for field in self.fields:\n+            value = field.clean(value)\n+        return value\n+\n+\n+class MultiValueField(Field):\n+    \"\"\"\n+    Aggregate the logic of multiple Fields.\n+\n+    Its clean() method takes a \"decompressed\" list of values, which are then\n+    cleaned into a single value according to self.fields. Each value in\n+    this list is cleaned by the corresponding field -- the first value is\n+    cleaned by the first field, the second value is cleaned by the second\n+    field, etc. Once all fields are cleaned, the list of clean values is\n+    \"compressed\" into a single value.\n+\n+    Subclasses should not have to implement clean(). Instead, they must\n+    implement compress(), which takes a list of valid values and returns a\n+    \"compressed\" version of those values -- a single value.\n+\n+    You'll probably want to use this with MultiWidget.\n+    \"\"\"\n+    default_error_messages = {\n+        'invalid': _('Enter a list of values.'),\n+        'incomplete': _('Enter a complete value.'),\n+    }\n+\n+    def __init__(self, fields, *, require_all_fields=True, **kwargs):\n+        self.require_all_fields = require_all_fields\n+        super().__init__(**kwargs)\n+        for f in fields:\n+            f.error_messages.setdefault('incomplete',\n+                                        self.error_messages['incomplete'])\n+            if self.disabled:\n+                f.disabled = True\n+            if self.require_all_fields:\n+                # Set 'required' to False on the individual fields, because the\n+                # required validation will be handled by MultiValueField, not\n+                # by those individual fields.\n+                f.required = False\n+        self.fields = fields\n+\n+    def __deepcopy__(self, memo):\n+        result = super().__deepcopy__(memo)\n+        result.fields = tuple(x.__deepcopy__(memo) for x in self.fields)\n+        return result\n+\n+    def validate(self, value):\n+        pass\n+\n+    def clean(self, value):\n+        \"\"\"\n+        Validate every value in the given list. A value is validated against\n+        the corresponding Field in self.fields.\n+\n+        For example, if this MultiValueField was instantiated with\n+        fields=(DateField(), TimeField()), clean() would call\n+        DateField.clean(value[0]) and TimeField.clean(value[1]).\n+        \"\"\"\n+        clean_data = []\n+        errors = []\n+        if self.disabled and not isinstance(value, list):\n+            value = self.widget.decompress(value)\n+        if not value or isinstance(value, (list, tuple)):\n+            if not value or not [v for v in value if v not in self.empty_values]:\n+                if self.required:\n+                    raise ValidationError(self.error_messages['required'], code='required')\n+                else:\n+                    return self.compress([])\n+        else:\n+            raise ValidationError(self.error_messages['invalid'], code='invalid')\n+        for i, field in enumerate(self.fields):\n+            try:\n+                field_value = value[i]\n+            except IndexError:\n+                field_value = None\n+            if field_value in self.empty_values:\n+                if self.require_all_fields:\n+                    # Raise a 'required' error if the MultiValueField is\n+                    # required and any field is empty.\n+                    if self.required:\n+                        raise ValidationError(self.error_messages['required'], code='required')\n+                elif field.required:\n+                    # Otherwise, add an 'incomplete' error to the list of\n+                    # collected errors and skip field cleaning, if a required\n+                    # field is empty.\n+                    if field.error_messages['incomplete'] not in errors:\n+                        errors.append(field.error_messages['incomplete'])\n+                    continue\n+            try:\n+                clean_data.append(field.clean(field_value))\n+            except ValidationError as e:\n+                # Collect all validation errors in a single list, which we'll\n+                # raise at the end of clean(), rather than raising a single\n+                # exception for the first error we encounter. Skip duplicates.\n+                errors.extend(m for m in e.error_list if m not in errors)\n+        if errors:\n+            raise ValidationError(errors)\n+\n+        out = self.compress(clean_data)\n+        self.validate(out)\n+        self.run_validators(out)\n+        return out\n+\n+    def compress(self, data_list):\n+        \"\"\"\n+        Return a single value for the given list of values. The values can be\n+        assumed to be valid.\n+\n+        For example, if this MultiValueField was instantiated with\n+        fields=(DateField(), TimeField()), this might return a datetime\n+        object created by combining the date and time in data_list.\n+        \"\"\"\n+        raise NotImplementedError('Subclasses must implement this method.')\n+\n+    def has_changed(self, initial, data):\n+        if self.disabled:\n+            return False\n+        if initial is None:\n+            initial = ['' for x in range(0, len(data))]\n+        else:\n+            if not isinstance(initial, list):\n+                initial = self.widget.decompress(initial)\n+        for field, initial, data in zip(self.fields, initial, data):\n+            try:\n+                initial = field.to_python(initial)\n+            except ValidationError:\n+                return True\n+            if field.has_changed(initial, data):\n+                return True\n+        return False\n+\n+\n+class FilePathField(ChoiceField):\n+    def __init__(self, path, *, match=None, recursive=False, allow_files=True,\n+                 allow_folders=False, **kwargs):\n+        self.path, self.match, self.recursive = path, match, recursive\n+        self.allow_files, self.allow_folders = allow_files, allow_folders\n+        super().__init__(choices=(), **kwargs)\n+\n+        if self.required:\n+            self.choices = []\n+        else:\n+            self.choices = [(\"\", \"---------\")]\n+\n+        if self.match is not None:\n+            self.match_re = re.compile(self.match)\n+\n+        if recursive:\n+            for root, dirs, files in sorted(os.walk(self.path)):\n+                if self.allow_files:\n+                    for f in sorted(files):\n+                        if self.match is None or self.match_re.search(f):\n+                            f = os.path.join(root, f)\n+                            self.choices.append((f, f.replace(path, \"\", 1)))\n+                if self.allow_folders:\n+                    for f in sorted(dirs):\n+                        if f == '__pycache__':\n+                            continue\n+                        if self.match is None or self.match_re.search(f):\n+                            f = os.path.join(root, f)\n+                            self.choices.append((f, f.replace(path, \"\", 1)))\n+        else:\n+            choices = []\n+            for f in os.scandir(self.path):\n+                if f.name == '__pycache__':\n+                    continue\n+                if (((self.allow_files and f.is_file()) or\n+                        (self.allow_folders and f.is_dir())) and\n+                        (self.match is None or self.match_re.search(f.name))):\n+                    choices.append((f.path, f.name))\n+            choices.sort(key=operator.itemgetter(1))\n+            self.choices.extend(choices)\n+\n+        self.widget.choices = self.choices\n+\n+\n+class SplitDateTimeField(MultiValueField):\n+    widget = SplitDateTimeWidget\n+    hidden_widget = SplitHiddenDateTimeWidget\n+    default_error_messages = {\n+        'invalid_date': _('Enter a valid date.'),\n+        'invalid_time': _('Enter a valid time.'),\n+    }\n+\n+    def __init__(self, *, input_date_formats=None, input_time_formats=None, **kwargs):\n+        errors = self.default_error_messages.copy()\n+        if 'error_messages' in kwargs:\n+            errors.update(kwargs['error_messages'])\n+        localize = kwargs.get('localize', False)\n+        fields = (\n+            DateField(input_formats=input_date_formats,\n+                      error_messages={'invalid': errors['invalid_date']},\n+                      localize=localize),\n+            TimeField(input_formats=input_time_formats,\n+                      error_messages={'invalid': errors['invalid_time']},\n+                      localize=localize),\n+        )\n+        super().__init__(fields, **kwargs)\n+\n+    def compress(self, data_list):\n+        if data_list:\n+            # Raise a validation error if time or date is empty\n+            # (possible if SplitDateTimeField has required=False).\n+            if data_list[0] in self.empty_values:\n+                raise ValidationError(self.error_messages['invalid_date'], code='invalid_date')\n+            if data_list[1] in self.empty_values:\n+                raise ValidationError(self.error_messages['invalid_time'], code='invalid_time')\n+            result = datetime.datetime.combine(*data_list)\n+            return from_current_timezone(result)\n+        return None\n+\n+\n+class GenericIPAddressField(CharField):\n+    def __init__(self, *, protocol='both', unpack_ipv4=False, **kwargs):\n+        self.unpack_ipv4 = unpack_ipv4\n+        self.default_validators = validators.ip_address_validators(protocol, unpack_ipv4)[0]\n+        super().__init__(**kwargs)\n+\n+    def to_python(self, value):\n+        if value in self.empty_values:\n+            return ''\n+        value = value.strip()\n+        if value and ':' in value:\n+            return clean_ipv6_address(value, self.unpack_ipv4)\n+        return value\n+\n+\n+class SlugField(CharField):\n+    default_validators = [validators.validate_slug]\n+\n+    def __init__(self, *, allow_unicode=False, **kwargs):\n+        self.allow_unicode = allow_unicode\n+        if self.allow_unicode:\n+            self.default_validators = [validators.validate_unicode_slug]\n+        super().__init__(**kwargs)\n+\n+\n+class UUIDField(CharField):\n+    default_error_messages = {\n+        'invalid': _('Enter a valid UUID.'),\n+    }\n+\n+    def prepare_value(self, value):\n+        if isinstance(value, uuid.UUID):\n+            return str(value)\n+        return value\n+\n+    def to_python(self, value):\n+        value = super().to_python(value)\n+        if value in self.empty_values:\n+            return None\n+        if not isinstance(value, uuid.UUID):\n+            try:\n+                value = uuid.UUID(value)\n+            except ValueError:\n+                raise ValidationError(self.error_messages['invalid'], code='invalid')\n+        return value\ndiff --git a/django/db/models/fields/__init__.py b/django/db/models/fields/__init__.py\nindex 0c64748ccd..8abd317cd7 100644\n--- a/django/db/models/fields/__init__.py\n+++ b/django/db/models/fields/__init__.py\n@@ -1663,7 +1663,8 @@ class FilePathField(Field):\n \n     def __init__(self, verbose_name=None, name=None, path='', match=None,\n                  recursive=False, allow_files=True, allow_folders=False, **kwargs):\n-        self.path, self.match, self.recursive = path, match, recursive\n+        self.path = path\n+        self.match, self.recursive = match, recursive\n         self.allow_files, self.allow_folders = allow_files, allow_folders\n         kwargs.setdefault('max_length', 100)\n         super().__init__(verbose_name, name, **kwargs)\n@@ -1709,7 +1710,7 @@ class FilePathField(Field):\n \n     def formfield(self, **kwargs):\n         return super().formfield(**{\n-            'path': self.path,\n+            'path': self.path() if callable(self.path) else self.path,\n             'match': self.match,\n             'recursive': self.recursive,\n             'form_class': forms.FilePathField,\ndiff --git a/django/forms/fields.py b/django/forms/fields.py\nindex a977256525..841c57d5a8 100644\n--- a/django/forms/fields.py\n+++ b/django/forms/fields.py\n@@ -1090,8 +1090,9 @@ class FilePathField(ChoiceField):\n         if self.match is not None:\n             self.match_re = re.compile(self.match)\n \n+        path = self.path() if callable(self.path) else self.path\n         if recursive:\n-            for root, dirs, files in sorted(os.walk(self.path)):\n+            for root, dirs, files in sorted(os.walk(path)):\n                 if self.allow_files:\n                     for f in sorted(files):\n                         if self.match is None or self.match_re.search(f):\n@@ -1106,7 +1107,7 @@ class FilePathField(ChoiceField):\n                             self.choices.append((f, f.replace(path, \"\", 1)))\n         else:\n             choices = []\n-            for f in os.scandir(self.path):\n+            for f in os.scandir(path):\n                 if f.name == '__pycache__':\n                     continue\n                 if (((self.allow_files and f.is_file()) or\ndiff --git a/tests/field_deconstruction/tests.py b/tests/field_deconstruction/tests.py\nindex 2cf1f93e22..d5876baef0 100644\n--- a/tests/field_deconstruction/tests.py\n+++ b/tests/field_deconstruction/tests.py\n@@ -186,6 +186,14 @@ class FieldDeconstructionTests(SimpleTestCase):\n         self.assertEqual(args, [])\n         self.assertEqual(kwargs, {\"recursive\": True, \"allow_folders\": True, \"max_length\": 123})\n \n+    def test_file_path_field_path_callable(self):\n+        path_callable = lambda: '/path/to/files/'\n+        field = models.FilePathField(path=path_callable)\n+        name, path, args, kwargs = field.deconstruct()\n+        self.assertEqual(path, \"django.db.models.FilePathField\")\n+        self.assertEqual(args, [])\n+        self.assertEqual(kwargs, {\"path\": path_callable})\n+\n     def test_float_field(self):\n         field = models.FloatField()\n         name, path, args, kwargs = field.deconstruct()\ndiff --git a/tests/forms_tests/field_tests/test_filepathfield.py b/tests/forms_tests/field_tests/test_filepathfield.py\nindex 44f6aff521..af93a46595 100644\n--- a/tests/forms_tests/field_tests/test_filepathfield.py\n+++ b/tests/forms_tests/field_tests/test_filepathfield.py\n@@ -97,3 +97,21 @@ class FilePathFieldTest(SimpleTestCase):\n             ('/filepathfield_test_dir/j', 'j'),\n             ('/filepathfield_test_dir/c/f', 'c/f'),\n         ])\n+\n+    def test_callable_path(self):\n+        f = FilePathField(path=lambda: self.path)\n+        expected = [\n+            ('/filepathfield_test_dir/README', 'README'),\n+        ] + self.expected_choices[:4]\n+        self.assertChoices(f, expected)\n+\n+    def test_callable_path_is_invoked_at_each_instantiation(self):\n+        path_values = [self.path, self.path]\n+        path_iter = iter(path_values)\n+        f1 = FilePathField(path=lambda: next(path_iter))\n+        f2 = FilePathField(path=lambda: self.path)\n+        expected = [\n+            ('/filepathfield_test_dir/README', 'README'),\n+        ] + self.expected_choices[:4]\n+        self.assertChoices(f1, expected)\n+        self.assertChoices(f2, expected)\n"}
{"instance_id": "django__django-11001", "model_name_or_path": "harness/claude-sonnet-4-6", "model_patch": "diff --git a/.harness/checkpoints/78299ed1b49b/--Users--huetuanthi--dev--agents--harness--django--db--models--sql--compiler.py b/.harness/checkpoints/78299ed1b49b/--Users--huetuanthi--dev--agents--harness--django--db--models--sql--compiler.py\nnew file mode 100644\nindex 0000000000..7649c39262\n--- /dev/null\n+++ b/.harness/checkpoints/78299ed1b49b/--Users--huetuanthi--dev--agents--harness--django--db--models--sql--compiler.py\n@@ -0,0 +1,1507 @@\n+import collections\n+import re\n+import warnings\n+from itertools import chain\n+\n+from django.core.exceptions import EmptyResultSet, FieldError\n+from django.db.models.constants import LOOKUP_SEP\n+from django.db.models.expressions import OrderBy, Random, RawSQL, Ref\n+from django.db.models.query_utils import QueryWrapper, select_related_descend\n+from django.db.models.sql.constants import (\n+    CURSOR, GET_ITERATOR_CHUNK_SIZE, MULTI, NO_RESULTS, ORDER_DIR, SINGLE,\n+)\n+from django.db.models.sql.query import Query, get_order_dir\n+from django.db.transaction import TransactionManagementError\n+from django.db.utils import DatabaseError, NotSupportedError\n+from django.utils.deprecation import RemovedInDjango31Warning\n+from django.utils.hashable import make_hashable\n+\n+FORCE = object()\n+\n+\n+class SQLCompiler:\n+    def __init__(self, query, connection, using):\n+        self.query = query\n+        self.connection = connection\n+        self.using = using\n+        self.quote_cache = {'*': '*'}\n+        # The select, klass_info, and annotations are needed by QuerySet.iterator()\n+        # these are set as a side-effect of executing the query. Note that we calculate\n+        # separately a list of extra select columns needed for grammatical correctness\n+        # of the query, but these columns are not included in self.select.\n+        self.select = None\n+        self.annotation_col_map = None\n+        self.klass_info = None\n+        self.ordering_parts = re.compile(r'(.*)\\s(ASC|DESC)(.*)')\n+        self._meta_ordering = None\n+\n+    def setup_query(self):\n+        if all(self.query.alias_refcount[a] == 0 for a in self.query.alias_map):\n+            self.query.get_initial_alias()\n+        self.select, self.klass_info, self.annotation_col_map = self.get_select()\n+        self.col_count = len(self.select)\n+\n+    def pre_sql_setup(self):\n+        \"\"\"\n+        Do any necessary class setup immediately prior to producing SQL. This\n+        is for things that can't necessarily be done in __init__ because we\n+        might not have all the pieces in place at that time.\n+        \"\"\"\n+        self.setup_query()\n+        order_by = self.get_order_by()\n+        self.where, self.having = self.query.where.split_having()\n+        extra_select = self.get_extra_select(order_by, self.select)\n+        self.has_extra_select = bool(extra_select)\n+        group_by = self.get_group_by(self.select + extra_select, order_by)\n+        return extra_select, order_by, group_by\n+\n+    def get_group_by(self, select, order_by):\n+        \"\"\"\n+        Return a list of 2-tuples of form (sql, params).\n+\n+        The logic of what exactly the GROUP BY clause contains is hard\n+        to describe in other words than \"if it passes the test suite,\n+        then it is correct\".\n+        \"\"\"\n+        # Some examples:\n+        #     SomeModel.objects.annotate(Count('somecol'))\n+        #     GROUP BY: all fields of the model\n+        #\n+        #    SomeModel.objects.values('name').annotate(Count('somecol'))\n+        #    GROUP BY: name\n+        #\n+        #    SomeModel.objects.annotate(Count('somecol')).values('name')\n+        #    GROUP BY: all cols of the model\n+        #\n+        #    SomeModel.objects.values('name', 'pk').annotate(Count('somecol')).values('pk')\n+        #    GROUP BY: name, pk\n+        #\n+        #    SomeModel.objects.values('name').annotate(Count('somecol')).values('pk')\n+        #    GROUP BY: name, pk\n+        #\n+        # In fact, the self.query.group_by is the minimal set to GROUP BY. It\n+        # can't be ever restricted to a smaller set, but additional columns in\n+        # HAVING, ORDER BY, and SELECT clauses are added to it. Unfortunately\n+        # the end result is that it is impossible to force the query to have\n+        # a chosen GROUP BY clause - you can almost do this by using the form:\n+        #     .values(*wanted_cols).annotate(AnAggregate())\n+        # but any later annotations, extra selects, values calls that\n+        # refer some column outside of the wanted_cols, order_by, or even\n+        # filter calls can alter the GROUP BY clause.\n+\n+        # The query.group_by is either None (no GROUP BY at all), True\n+        # (group by select fields), or a list of expressions to be added\n+        # to the group by.\n+        if self.query.group_by is None:\n+            return []\n+        expressions = []\n+        if self.query.group_by is not True:\n+            # If the group by is set to a list (by .values() call most likely),\n+            # then we need to add everything in it to the GROUP BY clause.\n+            # Backwards compatibility hack for setting query.group_by. Remove\n+            # when  we have public API way of forcing the GROUP BY clause.\n+            # Converts string references to expressions.\n+            for expr in self.query.group_by:\n+                if not hasattr(expr, 'as_sql'):\n+                    expressions.append(self.query.resolve_ref(expr))\n+                else:\n+                    expressions.append(expr)\n+        # Note that even if the group_by is set, it is only the minimal\n+        # set to group by. So, we need to add cols in select, order_by, and\n+        # having into the select in any case.\n+        for expr, _, _ in select:\n+            cols = expr.get_group_by_cols()\n+            for col in cols:\n+                expressions.append(col)\n+        for expr, (sql, params, is_ref) in order_by:\n+            # Skip References to the select clause, as all expressions in the\n+            # select clause are already part of the group by.\n+            if not expr.contains_aggregate and not is_ref:\n+                expressions.extend(expr.get_source_expressions())\n+        having_group_by = self.having.get_group_by_cols() if self.having else ()\n+        for expr in having_group_by:\n+            expressions.append(expr)\n+        result = []\n+        seen = set()\n+        expressions = self.collapse_group_by(expressions, having_group_by)\n+\n+        for expr in expressions:\n+            sql, params = self.compile(expr)\n+            params_hash = make_hashable(params)\n+            if (sql, params_hash) not in seen:\n+                result.append((sql, params))\n+                seen.add((sql, params_hash))\n+        return result\n+\n+    def collapse_group_by(self, expressions, having):\n+        # If the DB can group by primary key, then group by the primary key of\n+        # query's main model. Note that for PostgreSQL the GROUP BY clause must\n+        # include the primary key of every table, but for MySQL it is enough to\n+        # have the main table's primary key.\n+        if self.connection.features.allows_group_by_pk:\n+            # Determine if the main model's primary key is in the query.\n+            pk = None\n+            for expr in expressions:\n+                # Is this a reference to query's base table primary key? If the\n+                # expression isn't a Col-like, then skip the expression.\n+                if (getattr(expr, 'target', None) == self.query.model._meta.pk and\n+                        getattr(expr, 'alias', None) == self.query.base_table):\n+                    pk = expr\n+                    break\n+            # If the main model's primary key is in the query, group by that\n+            # field, HAVING expressions, and expressions associated with tables\n+            # that don't have a primary key included in the grouped columns.\n+            if pk:\n+                pk_aliases = {\n+                    expr.alias for expr in expressions\n+                    if hasattr(expr, 'target') and expr.target.primary_key\n+                }\n+                expressions = [pk] + [\n+                    expr for expr in expressions\n+                    if expr in having or (\n+                        getattr(expr, 'alias', None) is not None and expr.alias not in pk_aliases\n+                    )\n+                ]\n+        elif self.connection.features.allows_group_by_selected_pks:\n+            # Filter out all expressions associated with a table's primary key\n+            # present in the grouped columns. This is done by identifying all\n+            # tables that have their primary key included in the grouped\n+            # columns and removing non-primary key columns referring to them.\n+            # Unmanaged models are excluded because they could be representing\n+            # database views on which the optimization might not be allowed.\n+            pks = {\n+                expr for expr in expressions\n+                if hasattr(expr, 'target') and expr.target.primary_key and expr.target.model._meta.managed\n+            }\n+            aliases = {expr.alias for expr in pks}\n+            expressions = [\n+                expr for expr in expressions if expr in pks or getattr(expr, 'alias', None) not in aliases\n+            ]\n+        return expressions\n+\n+    def get_select(self):\n+        \"\"\"\n+        Return three values:\n+        - a list of 3-tuples of (expression, (sql, params), alias)\n+        - a klass_info structure,\n+        - a dictionary of annotations\n+\n+        The (sql, params) is what the expression will produce, and alias is the\n+        \"AS alias\" for the column (possibly None).\n+\n+        The klass_info structure contains the following information:\n+        - The base model of the query.\n+        - Which columns for that model are present in the query (by\n+          position of the select clause).\n+        - related_klass_infos: [f, klass_info] to descent into\n+\n+        The annotations is a dictionary of {'attname': column position} values.\n+        \"\"\"\n+        select = []\n+        klass_info = None\n+        annotations = {}\n+        select_idx = 0\n+        for alias, (sql, params) in self.query.extra_select.items():\n+            annotations[alias] = select_idx\n+            select.append((RawSQL(sql, params), alias))\n+            select_idx += 1\n+        assert not (self.query.select and self.query.default_cols)\n+        if self.query.default_cols:\n+            cols = self.get_default_columns()\n+        else:\n+            # self.query.select is a special case. These columns never go to\n+            # any model.\n+            cols = self.query.select\n+        if cols:\n+            select_list = []\n+            for col in cols:\n+                select_list.append(select_idx)\n+                select.append((col, None))\n+                select_idx += 1\n+            klass_info = {\n+                'model': self.query.model,\n+                'select_fields': select_list,\n+            }\n+        for alias, annotation in self.query.annotation_select.items():\n+            annotations[alias] = select_idx\n+            select.append((annotation, alias))\n+            select_idx += 1\n+\n+        if self.query.select_related:\n+            related_klass_infos = self.get_related_selections(select)\n+            klass_info['related_klass_infos'] = related_klass_infos\n+\n+            def get_select_from_parent(klass_info):\n+                for ki in klass_info['related_klass_infos']:\n+                    if ki['from_parent']:\n+                        ki['select_fields'] = (klass_info['select_fields'] +\n+                                               ki['select_fields'])\n+                    get_select_from_parent(ki)\n+            get_select_from_parent(klass_info)\n+\n+        ret = []\n+        for col, alias in select:\n+            try:\n+                sql, params = self.compile(col, select_format=True)\n+            except EmptyResultSet:\n+                # Select a predicate that's always False.\n+                sql, params = '0', ()\n+            ret.append((col, (sql, params), alias))\n+        return ret, klass_info, annotations\n+\n+    def get_order_by(self):\n+        \"\"\"\n+        Return a list of 2-tuples of form (expr, (sql, params, is_ref)) for the\n+        ORDER BY clause.\n+\n+        The order_by clause can alter the select clause (for example it\n+        can add aliases to clauses that do not yet have one, or it can\n+        add totally new select clauses).\n+        \"\"\"\n+        if self.query.extra_order_by:\n+            ordering = self.query.extra_order_by\n+        elif not self.query.default_ordering:\n+            ordering = self.query.order_by\n+        elif self.query.order_by:\n+            ordering = self.query.order_by\n+        elif self.query.get_meta().ordering:\n+            ordering = self.query.get_meta().ordering\n+            self._meta_ordering = ordering\n+        else:\n+            ordering = []\n+        if self.query.standard_ordering:\n+            asc, desc = ORDER_DIR['ASC']\n+        else:\n+            asc, desc = ORDER_DIR['DESC']\n+\n+        order_by = []\n+        for field in ordering:\n+            if hasattr(field, 'resolve_expression'):\n+                if not isinstance(field, OrderBy):\n+                    field = field.asc()\n+                if not self.query.standard_ordering:\n+                    field.reverse_ordering()\n+                order_by.append((field, False))\n+                continue\n+            if field == '?':  # random\n+                order_by.append((OrderBy(Random()), False))\n+                continue\n+\n+            col, order = get_order_dir(field, asc)\n+            descending = order == 'DESC'\n+\n+            if col in self.query.annotation_select:\n+                # Reference to expression in SELECT clause\n+                order_by.append((\n+                    OrderBy(Ref(col, self.query.annotation_select[col]), descending=descending),\n+                    True))\n+                continue\n+            if col in self.query.annotations:\n+                # References to an expression which is masked out of the SELECT clause\n+                order_by.append((\n+                    OrderBy(self.query.annotations[col], descending=descending),\n+                    False))\n+                continue\n+\n+            if '.' in field:\n+                # This came in through an extra(order_by=...) addition. Pass it\n+                # on verbatim.\n+                table, col = col.split('.', 1)\n+                order_by.append((\n+                    OrderBy(\n+                        RawSQL('%s.%s' % (self.quote_name_unless_alias(table), col), []),\n+                        descending=descending\n+                    ), False))\n+                continue\n+\n+            if not self.query.extra or col not in self.query.extra:\n+                # 'col' is of the form 'field' or 'field1__field2' or\n+                # '-field1__field2__field', etc.\n+                order_by.extend(self.find_ordering_name(\n+                    field, self.query.get_meta(), default_order=asc))\n+            else:\n+                if col not in self.query.extra_select:\n+                    order_by.append((\n+                        OrderBy(RawSQL(*self.query.extra[col]), descending=descending),\n+                        False))\n+                else:\n+                    order_by.append((\n+                        OrderBy(Ref(col, RawSQL(*self.query.extra[col])), descending=descending),\n+                        True))\n+        result = []\n+        seen = set()\n+\n+        for expr, is_ref in order_by:\n+            resolved = expr.resolve_expression(self.query, allow_joins=True, reuse=None)\n+            if self.query.combinator:\n+                src = resolved.get_source_expressions()[0]\n+                # Relabel order by columns to raw numbers if this is a combined\n+                # query; necessary since the columns can't be referenced by the\n+                # fully qualified name and the simple column names may collide.\n+                for idx, (sel_expr, _, col_alias) in enumerate(self.select):\n+                    if is_ref and col_alias == src.refs:\n+                        src = src.source\n+                    elif col_alias:\n+                        continue\n+                    if src == sel_expr:\n+                        resolved.set_source_expressions([RawSQL('%d' % (idx + 1), ())])\n+                        break\n+                else:\n+                    raise DatabaseError('ORDER BY term does not match any column in the result set.')\n+            sql, params = self.compile(resolved)\n+            # Don't add the same column twice, but the order direction is\n+            # not taken into account so we strip it. When this entire method\n+            # is refactored into expressions, then we can check each part as we\n+            # generate it.\n+            without_ordering = self.ordering_parts.search(sql).group(1)\n+            params_hash = make_hashable(params)\n+            if (without_ordering, params_hash) in seen:\n+                continue\n+            seen.add((without_ordering, params_hash))\n+            result.append((resolved, (sql, params, is_ref)))\n+        return result\n+\n+    def get_extra_select(self, order_by, select):\n+        extra_select = []\n+        if self.query.distinct and not self.query.distinct_fields:\n+            select_sql = [t[1] for t in select]\n+            for expr, (sql, params, is_ref) in order_by:\n+                without_ordering = self.ordering_parts.search(sql).group(1)\n+                if not is_ref and (without_ordering, params) not in select_sql:\n+                    extra_select.append((expr, (without_ordering, params), None))\n+        return extra_select\n+\n+    def quote_name_unless_alias(self, name):\n+        \"\"\"\n+        A wrapper around connection.ops.quote_name that doesn't quote aliases\n+        for table names. This avoids problems with some SQL dialects that treat\n+        quoted strings specially (e.g. PostgreSQL).\n+        \"\"\"\n+        if name in self.quote_cache:\n+            return self.quote_cache[name]\n+        if ((name in self.query.alias_map and name not in self.query.table_map) or\n+                name in self.query.extra_select or (\n+                    name in self.query.external_aliases and name not in self.query.table_map)):\n+            self.quote_cache[name] = name\n+            return name\n+        r = self.connection.ops.quote_name(name)\n+        self.quote_cache[name] = r\n+        return r\n+\n+    def compile(self, node, select_format=False):\n+        vendor_impl = getattr(node, 'as_' + self.connection.vendor, None)\n+        if vendor_impl:\n+            sql, params = vendor_impl(self, self.connection)\n+        else:\n+            sql, params = node.as_sql(self, self.connection)\n+        if select_format is FORCE or (select_format and not self.query.subquery):\n+            return node.output_field.select_format(self, sql, params)\n+        return sql, params\n+\n+    def get_combinator_sql(self, combinator, all):\n+        features = self.connection.features\n+        compilers = [\n+            query.get_compiler(self.using, self.connection)\n+            for query in self.query.combined_queries if not query.is_empty()\n+        ]\n+        if not features.supports_slicing_ordering_in_compound:\n+            for query, compiler in zip(self.query.combined_queries, compilers):\n+                if query.low_mark or query.high_mark:\n+                    raise DatabaseError('LIMIT/OFFSET not allowed in subqueries of compound statements.')\n+                if compiler.get_order_by():\n+                    raise DatabaseError('ORDER BY not allowed in subqueries of compound statements.')\n+        parts = ()\n+        for compiler in compilers:\n+            try:\n+                # If the columns list is limited, then all combined queries\n+                # must have the same columns list. Set the selects defined on\n+                # the query on all combined queries, if not already set.\n+                if not compiler.query.values_select and self.query.values_select:\n+                    compiler.query.set_values((\n+                        *self.query.extra_select,\n+                        *self.query.values_select,\n+                        *self.query.annotation_select,\n+                    ))\n+                part_sql, part_args = compiler.as_sql()\n+                if compiler.query.combinator:\n+                    # Wrap in a subquery if wrapping in parentheses isn't\n+                    # supported.\n+                    if not features.supports_parentheses_in_compound:\n+                        part_sql = 'SELECT * FROM ({})'.format(part_sql)\n+                    # Add parentheses when combining with compound query if not\n+                    # already added for all compound queries.\n+                    elif not features.supports_slicing_ordering_in_compound:\n+                        part_sql = '({})'.format(part_sql)\n+                parts += ((part_sql, part_args),)\n+            except EmptyResultSet:\n+                # Omit the empty queryset with UNION and with DIFFERENCE if the\n+                # first queryset is nonempty.\n+                if combinator == 'union' or (combinator == 'difference' and parts):\n+                    continue\n+                raise\n+        if not parts:\n+            raise EmptyResultSet\n+        combinator_sql = self.connection.ops.set_operators[combinator]\n+        if all and combinator == 'union':\n+            combinator_sql += ' ALL'\n+        braces = '({})' if features.supports_slicing_ordering_in_compound else '{}'\n+        sql_parts, args_parts = zip(*((braces.format(sql), args) for sql, args in parts))\n+        result = [' {} '.format(combinator_sql).join(sql_parts)]\n+        params = []\n+        for part in args_parts:\n+            params.extend(part)\n+        return result, params\n+\n+    def as_sql(self, with_limits=True, with_col_aliases=False):\n+        \"\"\"\n+        Create the SQL for this query. Return the SQL string and list of\n+        parameters.\n+\n+        If 'with_limits' is False, any limit/offset information is not included\n+        in the query.\n+        \"\"\"\n+        refcounts_before = self.query.alias_refcount.copy()\n+        try:\n+            extra_select, order_by, group_by = self.pre_sql_setup()\n+            for_update_part = None\n+            # Is a LIMIT/OFFSET clause needed?\n+            with_limit_offset = with_limits and (self.query.high_mark is not None or self.query.low_mark)\n+            combinator = self.query.combinator\n+            features = self.connection.features\n+            if combinator:\n+                if not getattr(features, 'supports_select_{}'.format(combinator)):\n+                    raise NotSupportedError('{} is not supported on this database backend.'.format(combinator))\n+                result, params = self.get_combinator_sql(combinator, self.query.combinator_all)\n+            else:\n+                distinct_fields, distinct_params = self.get_distinct()\n+                # This must come after 'select', 'ordering', and 'distinct'\n+                # (see docstring of get_from_clause() for details).\n+                from_, f_params = self.get_from_clause()\n+                where, w_params = self.compile(self.where) if self.where is not None else (\"\", [])\n+                having, h_params = self.compile(self.having) if self.having is not None else (\"\", [])\n+                result = ['SELECT']\n+                params = []\n+\n+                if self.query.distinct:\n+                    distinct_result, distinct_params = self.connection.ops.distinct_sql(\n+                        distinct_fields,\n+                        distinct_params,\n+                    )\n+                    result += distinct_result\n+                    params += distinct_params\n+\n+                out_cols = []\n+                col_idx = 1\n+                for _, (s_sql, s_params), alias in self.select + extra_select:\n+                    if alias:\n+                        s_sql = '%s AS %s' % (s_sql, self.connection.ops.quote_name(alias))\n+                    elif with_col_aliases:\n+                        s_sql = '%s AS %s' % (s_sql, 'Col%d' % col_idx)\n+                        col_idx += 1\n+                    params.extend(s_params)\n+                    out_cols.append(s_sql)\n+\n+                result += [', '.join(out_cols), 'FROM', *from_]\n+                params.extend(f_params)\n+\n+                if self.query.select_for_update and self.connection.features.has_select_for_update:\n+                    if self.connection.get_autocommit():\n+                        raise TransactionManagementError('select_for_update cannot be used outside of a transaction.')\n+\n+                    if with_limit_offset and not self.connection.features.supports_select_for_update_with_limit:\n+                        raise NotSupportedError(\n+                            'LIMIT/OFFSET is not supported with '\n+                            'select_for_update on this database backend.'\n+                        )\n+                    nowait = self.query.select_for_update_nowait\n+                    skip_locked = self.query.select_for_update_skip_locked\n+                    of = self.query.select_for_update_of\n+                    # If it's a NOWAIT/SKIP LOCKED/OF query but the backend\n+                    # doesn't support it, raise NotSupportedError to prevent a\n+                    # possible deadlock.\n+                    if nowait and not self.connection.features.has_select_for_update_nowait:\n+                        raise NotSupportedError('NOWAIT is not supported on this database backend.')\n+                    elif skip_locked and not self.connection.features.has_select_for_update_skip_locked:\n+                        raise NotSupportedError('SKIP LOCKED is not supported on this database backend.')\n+                    elif of and not self.connection.features.has_select_for_update_of:\n+                        raise NotSupportedError('FOR UPDATE OF is not supported on this database backend.')\n+                    for_update_part = self.connection.ops.for_update_sql(\n+                        nowait=nowait,\n+                        skip_locked=skip_locked,\n+                        of=self.get_select_for_update_of_arguments(),\n+                    )\n+\n+                if for_update_part and self.connection.features.for_update_after_from:\n+                    result.append(for_update_part)\n+\n+                if where:\n+                    result.append('WHERE %s' % where)\n+                    params.extend(w_params)\n+\n+                grouping = []\n+                for g_sql, g_params in group_by:\n+                    grouping.append(g_sql)\n+                    params.extend(g_params)\n+                if grouping:\n+                    if distinct_fields:\n+                        raise NotImplementedError('annotate() + distinct(fields) is not implemented.')\n+                    order_by = order_by or self.connection.ops.force_no_ordering()\n+                    result.append('GROUP BY %s' % ', '.join(grouping))\n+                    if self._meta_ordering:\n+                        # When the deprecation ends, replace with:\n+                        # order_by = None\n+                        warnings.warn(\n+                            \"%s QuerySet won't use Meta.ordering in Django 3.1. \"\n+                            \"Add .order_by('%s') to retain the current query.\" % (\n+                                self.query.model.__name__,\n+                                \"', '\".join(self._meta_ordering)\n+                            ),\n+                            RemovedInDjango31Warning,\n+                            stacklevel=4,\n+                        )\n+                if having:\n+                    result.append('HAVING %s' % having)\n+                    params.extend(h_params)\n+\n+            if self.query.explain_query:\n+                result.insert(0, self.connection.ops.explain_query_prefix(\n+                    self.query.explain_format,\n+                    **self.query.explain_options\n+                ))\n+\n+            if order_by:\n+                ordering = []\n+                for _, (o_sql, o_params, _) in order_by:\n+                    ordering.append(o_sql)\n+                    params.extend(o_params)\n+                result.append('ORDER BY %s' % ', '.join(ordering))\n+\n+            if with_limit_offset:\n+                result.append(self.connection.ops.limit_offset_sql(self.query.low_mark, self.query.high_mark))\n+\n+            if for_update_part and not self.connection.features.for_update_after_from:\n+                result.append(for_update_part)\n+\n+            if self.query.subquery and extra_select:\n+                # If the query is used as a subquery, the extra selects would\n+                # result in more columns than the left-hand side expression is\n+                # expecting. This can happen when a subquery uses a combination\n+                # of order_by() and distinct(), forcing the ordering expressions\n+                # to be selected as well. Wrap the query in another subquery\n+                # to exclude extraneous selects.\n+                sub_selects = []\n+                sub_params = []\n+                for index, (select, _, alias) in enumerate(self.select, start=1):\n+                    if not alias and with_col_aliases:\n+                        alias = 'col%d' % index\n+                    if alias:\n+                        sub_selects.append(\"%s.%s\" % (\n+                            self.connection.ops.quote_name('subquery'),\n+                            self.connection.ops.quote_name(alias),\n+                        ))\n+                    else:\n+                        select_clone = select.relabeled_clone({select.alias: 'subquery'})\n+                        subselect, subparams = select_clone.as_sql(self, self.connection)\n+                        sub_selects.append(subselect)\n+                        sub_params.extend(subparams)\n+                return 'SELECT %s FROM (%s) subquery' % (\n+                    ', '.join(sub_selects),\n+                    ' '.join(result),\n+                ), tuple(sub_params + params)\n+\n+            return ' '.join(result), tuple(params)\n+        finally:\n+            # Finally do cleanup - get rid of the joins we created above.\n+            self.query.reset_refcounts(refcounts_before)\n+\n+    def get_default_columns(self, start_alias=None, opts=None, from_parent=None):\n+        \"\"\"\n+        Compute the default columns for selecting every field in the base\n+        model. Will sometimes be called to pull in related models (e.g. via\n+        select_related), in which case \"opts\" and \"start_alias\" will be given\n+        to provide a starting point for the traversal.\n+\n+        Return a list of strings, quoted appropriately for use in SQL\n+        directly, as well as a set of aliases used in the select statement (if\n+        'as_pairs' is True, return a list of (alias, col_name) pairs instead\n+        of strings as the first component and None as the second component).\n+        \"\"\"\n+        result = []\n+        if opts is None:\n+            opts = self.query.get_meta()\n+        only_load = self.deferred_to_columns()\n+        start_alias = start_alias or self.query.get_initial_alias()\n+        # The 'seen_models' is used to optimize checking the needed parent\n+        # alias for a given field. This also includes None -> start_alias to\n+        # be used by local fields.\n+        seen_models = {None: start_alias}\n+\n+        for field in opts.concrete_fields:\n+            model = field.model._meta.concrete_model\n+            # A proxy model will have a different model and concrete_model. We\n+            # will assign None if the field belongs to this model.\n+            if model == opts.model:\n+                model = None\n+            if from_parent and model is not None and issubclass(\n+                    from_parent._meta.concrete_model, model._meta.concrete_model):\n+                # Avoid loading data for already loaded parents.\n+                # We end up here in the case select_related() resolution\n+                # proceeds from parent model to child model. In that case the\n+                # parent model data is already present in the SELECT clause,\n+                # and we want to avoid reloading the same data again.\n+                continue\n+            if field.model in only_load and field.attname not in only_load[field.model]:\n+                continue\n+            alias = self.query.join_parent_model(opts, model, start_alias,\n+                                                 seen_models)\n+            column = field.get_col(alias)\n+            result.append(column)\n+        return result\n+\n+    def get_distinct(self):\n+        \"\"\"\n+        Return a quoted list of fields to use in DISTINCT ON part of the query.\n+\n+        This method can alter the tables in the query, and thus it must be\n+        called before get_from_clause().\n+        \"\"\"\n+        result = []\n+        params = []\n+        opts = self.query.get_meta()\n+\n+        for name in self.query.distinct_fields:\n+            parts = name.split(LOOKUP_SEP)\n+            _, targets, alias, joins, path, _, transform_function = self._setup_joins(parts, opts, None)\n+            targets, alias, _ = self.query.trim_joins(targets, joins, path)\n+            for target in targets:\n+                if name in self.query.annotation_select:\n+                    result.append(name)\n+                else:\n+                    r, p = self.compile(transform_function(target, alias))\n+                    result.append(r)\n+                    params.append(p)\n+        return result, params\n+\n+    def find_ordering_name(self, name, opts, alias=None, default_order='ASC',\n+                           already_seen=None):\n+        \"\"\"\n+        Return the table alias (the name might be ambiguous, the alias will\n+        not be) and column name for ordering by the given 'name' parameter.\n+        The 'name' is of the form 'field1__field2__...__fieldN'.\n+        \"\"\"\n+        name, order = get_order_dir(name, default_order)\n+        descending = order == 'DESC'\n+        pieces = name.split(LOOKUP_SEP)\n+        field, targets, alias, joins, path, opts, transform_function = self._setup_joins(pieces, opts, alias)\n+\n+        # If we get to this point and the field is a relation to another model,\n+        # append the default ordering for that model unless the attribute name\n+        # of the field is specified.\n+        if field.is_relation and opts.ordering and getattr(field, 'attname', None) != name:\n+            # Firstly, avoid infinite loops.\n+            already_seen = already_seen or set()\n+            join_tuple = tuple(getattr(self.query.alias_map[j], 'join_cols', None) for j in joins)\n+            if join_tuple in already_seen:\n+                raise FieldError('Infinite loop caused by ordering.')\n+            already_seen.add(join_tuple)\n+\n+            results = []\n+            for item in opts.ordering:\n+                results.extend(self.find_ordering_name(item, opts, alias,\n+                                                       order, already_seen))\n+            return results\n+        targets, alias, _ = self.query.trim_joins(targets, joins, path)\n+        return [(OrderBy(transform_function(t, alias), descending=descending), False) for t in targets]\n+\n+    def _setup_joins(self, pieces, opts, alias):\n+        \"\"\"\n+        Helper method for get_order_by() and get_distinct().\n+\n+        get_ordering() and get_distinct() must produce same target columns on\n+        same input, as the prefixes of get_ordering() and get_distinct() must\n+        match. Executing SQL where this is not true is an error.\n+        \"\"\"\n+        alias = alias or self.query.get_initial_alias()\n+        field, targets, opts, joins, path, transform_function = self.query.setup_joins(pieces, opts, alias)\n+        alias = joins[-1]\n+        return field, targets, alias, joins, path, opts, transform_function\n+\n+    def get_from_clause(self):\n+        \"\"\"\n+        Return a list of strings that are joined together to go after the\n+        \"FROM\" part of the query, as well as a list any extra parameters that\n+        need to be included. Subclasses, can override this to create a\n+        from-clause via a \"select\".\n+\n+        This should only be called after any SQL construction methods that\n+        might change the tables that are needed. This means the select columns,\n+        ordering, and distinct must be done first.\n+        \"\"\"\n+        result = []\n+        params = []\n+        for alias in tuple(self.query.alias_map):\n+            if not self.query.alias_refcount[alias]:\n+                continue\n+            try:\n+                from_clause = self.query.alias_map[alias]\n+            except KeyError:\n+                # Extra tables can end up in self.tables, but not in the\n+                # alias_map if they aren't in a join. That's OK. We skip them.\n+                continue\n+            clause_sql, clause_params = self.compile(from_clause)\n+            result.append(clause_sql)\n+            params.extend(clause_params)\n+        for t in self.query.extra_tables:\n+            alias, _ = self.query.table_alias(t)\n+            # Only add the alias if it's not already present (the table_alias()\n+            # call increments the refcount, so an alias refcount of one means\n+            # this is the only reference).\n+            if alias not in self.query.alias_map or self.query.alias_refcount[alias] == 1:\n+                result.append(', %s' % self.quote_name_unless_alias(alias))\n+        return result, params\n+\n+    def get_related_selections(self, select, opts=None, root_alias=None, cur_depth=1,\n+                               requested=None, restricted=None):\n+        \"\"\"\n+        Fill in the information needed for a select_related query. The current\n+        depth is measured as the number of connections away from the root model\n+        (for example, cur_depth=1 means we are looking at models with direct\n+        connections to the root model).\n+        \"\"\"\n+        def _get_field_choices():\n+            direct_choices = (f.name for f in opts.fields if f.is_relation)\n+            reverse_choices = (\n+                f.field.related_query_name()\n+                for f in opts.related_objects if f.field.unique\n+            )\n+            return chain(direct_choices, reverse_choices, self.query._filtered_relations)\n+\n+        related_klass_infos = []\n+        if not restricted and cur_depth > self.query.max_depth:\n+            # We've recursed far enough; bail out.\n+            return related_klass_infos\n+\n+        if not opts:\n+            opts = self.query.get_meta()\n+            root_alias = self.query.get_initial_alias()\n+        only_load = self.query.get_loaded_field_names()\n+\n+        # Setup for the case when only particular related fields should be\n+        # included in the related selection.\n+        fields_found = set()\n+        if requested is None:\n+            restricted = isinstance(self.query.select_related, dict)\n+            if restricted:\n+                requested = self.query.select_related\n+\n+        def get_related_klass_infos(klass_info, related_klass_infos):\n+            klass_info['related_klass_infos'] = related_klass_infos\n+\n+        for f in opts.fields:\n+            field_model = f.model._meta.concrete_model\n+            fields_found.add(f.name)\n+\n+            if restricted:\n+                next = requested.get(f.name, {})\n+                if not f.is_relation:\n+                    # If a non-related field is used like a relation,\n+                    # or if a single non-relational field is given.\n+                    if next or f.name in requested:\n+                        raise FieldError(\n+                            \"Non-relational field given in select_related: '%s'. \"\n+                            \"Choices are: %s\" % (\n+                                f.name,\n+                                \", \".join(_get_field_choices()) or '(none)',\n+                            )\n+                        )\n+            else:\n+                next = False\n+\n+            if not select_related_descend(f, restricted, requested,\n+                                          only_load.get(field_model)):\n+                continue\n+            klass_info = {\n+                'model': f.remote_field.model,\n+                'field': f,\n+                'reverse': False,\n+                'local_setter': f.set_cached_value,\n+                'remote_setter': f.remote_field.set_cached_value if f.unique else lambda x, y: None,\n+                'from_parent': False,\n+            }\n+            related_klass_infos.append(klass_info)\n+            select_fields = []\n+            _, _, _, joins, _, _ = self.query.setup_joins(\n+                [f.name], opts, root_alias)\n+            alias = joins[-1]\n+            columns = self.get_default_columns(start_alias=alias, opts=f.remote_field.model._meta)\n+            for col in columns:\n+                select_fields.append(len(select))\n+                select.append((col, None))\n+            klass_info['select_fields'] = select_fields\n+            next_klass_infos = self.get_related_selections(\n+                select, f.remote_field.model._meta, alias, cur_depth + 1, next, restricted)\n+            get_related_klass_infos(klass_info, next_klass_infos)\n+\n+        if restricted:\n+            related_fields = [\n+                (o.field, o.related_model)\n+                for o in opts.related_objects\n+                if o.field.unique and not o.many_to_many\n+            ]\n+            for f, model in related_fields:\n+                if not select_related_descend(f, restricted, requested,\n+                                              only_load.get(model), reverse=True):\n+                    continue\n+\n+                related_field_name = f.related_query_name()\n+                fields_found.add(related_field_name)\n+\n+                join_info = self.query.setup_joins([related_field_name], opts, root_alias)\n+                alias = join_info.joins[-1]\n+                from_parent = issubclass(model, opts.model) and model is not opts.model\n+                klass_info = {\n+                    'model': model,\n+                    'field': f,\n+                    'reverse': True,\n+                    'local_setter': f.remote_field.set_cached_value,\n+                    'remote_setter': f.set_cached_value,\n+                    'from_parent': from_parent,\n+                }\n+                related_klass_infos.append(klass_info)\n+                select_fields = []\n+                columns = self.get_default_columns(\n+                    start_alias=alias, opts=model._meta, from_parent=opts.model)\n+                for col in columns:\n+                    select_fields.append(len(select))\n+                    select.append((col, None))\n+                klass_info['select_fields'] = select_fields\n+                next = requested.get(f.related_query_name(), {})\n+                next_klass_infos = self.get_related_selections(\n+                    select, model._meta, alias, cur_depth + 1,\n+                    next, restricted)\n+                get_related_klass_infos(klass_info, next_klass_infos)\n+            for name in list(requested):\n+                # Filtered relations work only on the topmost level.\n+                if cur_depth > 1:\n+                    break\n+                if name in self.query._filtered_relations:\n+                    fields_found.add(name)\n+                    f, _, join_opts, joins, _, _ = self.query.setup_joins([name], opts, root_alias)\n+                    model = join_opts.model\n+                    alias = joins[-1]\n+                    from_parent = issubclass(model, opts.model) and model is not opts.model\n+\n+                    def local_setter(obj, from_obj):\n+                        # Set a reverse fk object when relation is non-empty.\n+                        if from_obj:\n+                            f.remote_field.set_cached_value(from_obj, obj)\n+\n+                    def remote_setter(obj, from_obj):\n+                        setattr(from_obj, name, obj)\n+                    klass_info = {\n+                        'model': model,\n+                        'field': f,\n+                        'reverse': True,\n+                        'local_setter': local_setter,\n+                        'remote_setter': remote_setter,\n+                        'from_parent': from_parent,\n+                    }\n+                    related_klass_infos.append(klass_info)\n+                    select_fields = []\n+                    columns = self.get_default_columns(\n+                        start_alias=alias, opts=model._meta,\n+                        from_parent=opts.model,\n+                    )\n+                    for col in columns:\n+                        select_fields.append(len(select))\n+                        select.append((col, None))\n+                    klass_info['select_fields'] = select_fields\n+                    next_requested = requested.get(name, {})\n+                    next_klass_infos = self.get_related_selections(\n+                        select, opts=model._meta, root_alias=alias,\n+                        cur_depth=cur_depth + 1, requested=next_requested,\n+                        restricted=restricted,\n+                    )\n+                    get_related_klass_infos(klass_info, next_klass_infos)\n+            fields_not_found = set(requested).difference(fields_found)\n+            if fields_not_found:\n+                invalid_fields = (\"'%s'\" % s for s in fields_not_found)\n+                raise FieldError(\n+                    'Invalid field name(s) given in select_related: %s. '\n+                    'Choices are: %s' % (\n+                        ', '.join(invalid_fields),\n+                        ', '.join(_get_field_choices()) or '(none)',\n+                    )\n+                )\n+        return related_klass_infos\n+\n+    def get_select_for_update_of_arguments(self):\n+        \"\"\"\n+        Return a quoted list of arguments for the SELECT FOR UPDATE OF part of\n+        the query.\n+        \"\"\"\n+        def _get_field_choices():\n+            \"\"\"Yield all allowed field paths in breadth-first search order.\"\"\"\n+            queue = collections.deque([(None, self.klass_info)])\n+            while queue:\n+                parent_path, klass_info = queue.popleft()\n+                if parent_path is None:\n+                    path = []\n+                    yield 'self'\n+                else:\n+                    field = klass_info['field']\n+                    if klass_info['reverse']:\n+                        field = field.remote_field\n+                    path = parent_path + [field.name]\n+                    yield LOOKUP_SEP.join(path)\n+                queue.extend(\n+                    (path, klass_info)\n+                    for klass_info in klass_info.get('related_klass_infos', [])\n+                )\n+        result = []\n+        invalid_names = []\n+        for name in self.query.select_for_update_of:\n+            parts = [] if name == 'self' else name.split(LOOKUP_SEP)\n+            klass_info = self.klass_info\n+            for part in parts:\n+                for related_klass_info in klass_info.get('related_klass_infos', []):\n+                    field = related_klass_info['field']\n+                    if related_klass_info['reverse']:\n+                        field = field.remote_field\n+                    if field.name == part:\n+                        klass_info = related_klass_info\n+                        break\n+                else:\n+                    klass_info = None\n+                    break\n+            if klass_info is None:\n+                invalid_names.append(name)\n+                continue\n+            select_index = klass_info['select_fields'][0]\n+            col = self.select[select_index][0]\n+            if self.connection.features.select_for_update_of_column:\n+                result.append(self.compile(col)[0])\n+            else:\n+                result.append(self.quote_name_unless_alias(col.alias))\n+        if invalid_names:\n+            raise FieldError(\n+                'Invalid field name(s) given in select_for_update(of=(...)): %s. '\n+                'Only relational fields followed in the query are allowed. '\n+                'Choices are: %s.' % (\n+                    ', '.join(invalid_names),\n+                    ', '.join(_get_field_choices()),\n+                )\n+            )\n+        return result\n+\n+    def deferred_to_columns(self):\n+        \"\"\"\n+        Convert the self.deferred_loading data structure to mapping of table\n+        names to sets of column names which are to be loaded. Return the\n+        dictionary.\n+        \"\"\"\n+        columns = {}\n+        self.query.deferred_to_data(columns, self.query.get_loaded_field_names_cb)\n+        return columns\n+\n+    def get_converters(self, expressions):\n+        converters = {}\n+        for i, expression in enumerate(expressions):\n+            if expression:\n+                backend_converters = self.connection.ops.get_db_converters(expression)\n+                field_converters = expression.get_db_converters(self.connection)\n+                if backend_converters or field_converters:\n+                    converters[i] = (backend_converters + field_converters, expression)\n+        return converters\n+\n+    def apply_converters(self, rows, converters):\n+        connection = self.connection\n+        converters = list(converters.items())\n+        for row in map(list, rows):\n+            for pos, (convs, expression) in converters:\n+                value = row[pos]\n+                for converter in convs:\n+                    value = converter(value, expression, connection)\n+                row[pos] = value\n+            yield row\n+\n+    def results_iter(self, results=None, tuple_expected=False, chunked_fetch=False,\n+                     chunk_size=GET_ITERATOR_CHUNK_SIZE):\n+        \"\"\"Return an iterator over the results from executing this query.\"\"\"\n+        if results is None:\n+            results = self.execute_sql(MULTI, chunked_fetch=chunked_fetch, chunk_size=chunk_size)\n+        fields = [s[0] for s in self.select[0:self.col_count]]\n+        converters = self.get_converters(fields)\n+        rows = chain.from_iterable(results)\n+        if converters:\n+            rows = self.apply_converters(rows, converters)\n+            if tuple_expected:\n+                rows = map(tuple, rows)\n+        return rows\n+\n+    def has_results(self):\n+        \"\"\"\n+        Backends (e.g. NoSQL) can override this in order to use optimized\n+        versions of \"query has any results.\"\n+        \"\"\"\n+        # This is always executed on a query clone, so we can modify self.query\n+        self.query.add_extra({'a': 1}, None, None, None, None, None)\n+        self.query.set_extra_mask(['a'])\n+        return bool(self.execute_sql(SINGLE))\n+\n+    def execute_sql(self, result_type=MULTI, chunked_fetch=False, chunk_size=GET_ITERATOR_CHUNK_SIZE):\n+        \"\"\"\n+        Run the query against the database and return the result(s). The\n+        return value is a single data item if result_type is SINGLE, or an\n+        iterator over the results if the result_type is MULTI.\n+\n+        result_type is either MULTI (use fetchmany() to retrieve all rows),\n+        SINGLE (only retrieve a single row), or None. In this last case, the\n+        cursor is returned if any query is executed, since it's used by\n+        subclasses such as InsertQuery). It's possible, however, that no query\n+        is needed, as the filters describe an empty set. In that case, None is\n+        returned, to avoid any unnecessary database interaction.\n+        \"\"\"\n+        result_type = result_type or NO_RESULTS\n+        try:\n+            sql, params = self.as_sql()\n+            if not sql:\n+                raise EmptyResultSet\n+        except EmptyResultSet:\n+            if result_type == MULTI:\n+                return iter([])\n+            else:\n+                return\n+        if chunked_fetch:\n+            cursor = self.connection.chunked_cursor()\n+        else:\n+            cursor = self.connection.cursor()\n+        try:\n+            cursor.execute(sql, params)\n+        except Exception:\n+            # Might fail for server-side cursors (e.g. connection closed)\n+            cursor.close()\n+            raise\n+\n+        if result_type == CURSOR:\n+            # Give the caller the cursor to process and close.\n+            return cursor\n+        if result_type == SINGLE:\n+            try:\n+                val = cursor.fetchone()\n+                if val:\n+                    return val[0:self.col_count]\n+                return val\n+            finally:\n+                # done with the cursor\n+                cursor.close()\n+        if result_type == NO_RESULTS:\n+            cursor.close()\n+            return\n+\n+        result = cursor_iter(\n+            cursor, self.connection.features.empty_fetchmany_value,\n+            self.col_count if self.has_extra_select else None,\n+            chunk_size,\n+        )\n+        if not chunked_fetch or not self.connection.features.can_use_chunked_reads:\n+            try:\n+                # If we are using non-chunked reads, we return the same data\n+                # structure as normally, but ensure it is all read into memory\n+                # before going any further. Use chunked_fetch if requested,\n+                # unless the database doesn't support it.\n+                return list(result)\n+            finally:\n+                # done with the cursor\n+                cursor.close()\n+        return result\n+\n+    def as_subquery_condition(self, alias, columns, compiler):\n+        qn = compiler.quote_name_unless_alias\n+        qn2 = self.connection.ops.quote_name\n+\n+        for index, select_col in enumerate(self.query.select):\n+            lhs_sql, lhs_params = self.compile(select_col)\n+            rhs = '%s.%s' % (qn(alias), qn2(columns[index]))\n+            self.query.where.add(\n+                QueryWrapper('%s = %s' % (lhs_sql, rhs), lhs_params), 'AND')\n+\n+        sql, params = self.as_sql()\n+        return 'EXISTS (%s)' % sql, params\n+\n+    def explain_query(self):\n+        result = list(self.execute_sql())\n+        # Some backends return 1 item tuples with strings, and others return\n+        # tuples with integers and strings. Flatten them out into strings.\n+        for row in result[0]:\n+            if not isinstance(row, str):\n+                yield ' '.join(str(c) for c in row)\n+            else:\n+                yield row\n+\n+\n+class SQLInsertCompiler(SQLCompiler):\n+    return_id = False\n+\n+    def field_as_sql(self, field, val):\n+        \"\"\"\n+        Take a field and a value intended to be saved on that field, and\n+        return placeholder SQL and accompanying params. Check for raw values,\n+        expressions, and fields with get_placeholder() defined in that order.\n+\n+        When field is None, consider the value raw and use it as the\n+        placeholder, with no corresponding parameters returned.\n+        \"\"\"\n+        if field is None:\n+            # A field value of None means the value is raw.\n+            sql, params = val, []\n+        elif hasattr(val, 'as_sql'):\n+            # This is an expression, let's compile it.\n+            sql, params = self.compile(val)\n+        elif hasattr(field, 'get_placeholder'):\n+            # Some fields (e.g. geo fields) need special munging before\n+            # they can be inserted.\n+            sql, params = field.get_placeholder(val, self, self.connection), [val]\n+        else:\n+            # Return the common case for the placeholder\n+            sql, params = '%s', [val]\n+\n+        # The following hook is only used by Oracle Spatial, which sometimes\n+        # needs to yield 'NULL' and [] as its placeholder and params instead\n+        # of '%s' and [None]. The 'NULL' placeholder is produced earlier by\n+        # OracleOperations.get_geom_placeholder(). The following line removes\n+        # the corresponding None parameter. See ticket #10888.\n+        params = self.connection.ops.modify_insert_params(sql, params)\n+\n+        return sql, params\n+\n+    def prepare_value(self, field, value):\n+        \"\"\"\n+        Prepare a value to be used in a query by resolving it if it is an\n+        expression and otherwise calling the field's get_db_prep_save().\n+        \"\"\"\n+        if hasattr(value, 'resolve_expression'):\n+            value = value.resolve_expression(self.query, allow_joins=False, for_save=True)\n+            # Don't allow values containing Col expressions. They refer to\n+            # existing columns on a row, but in the case of insert the row\n+            # doesn't exist yet.\n+            if value.contains_column_references:\n+                raise ValueError(\n+                    'Failed to insert expression \"%s\" on %s. F() expressions '\n+                    'can only be used to update, not to insert.' % (value, field)\n+                )\n+            if value.contains_aggregate:\n+                raise FieldError(\n+                    'Aggregate functions are not allowed in this query '\n+                    '(%s=%r).' % (field.name, value)\n+                )\n+            if value.contains_over_clause:\n+                raise FieldError(\n+                    'Window expressions are not allowed in this query (%s=%r).'\n+                    % (field.name, value)\n+                )\n+        else:\n+            value = field.get_db_prep_save(value, connection=self.connection)\n+        return value\n+\n+    def pre_save_val(self, field, obj):\n+        \"\"\"\n+        Get the given field's value off the given obj. pre_save() is used for\n+        things like auto_now on DateTimeField. Skip it if this is a raw query.\n+        \"\"\"\n+        if self.query.raw:\n+            return getattr(obj, field.attname)\n+        return field.pre_save(obj, add=True)\n+\n+    def assemble_as_sql(self, fields, value_rows):\n+        \"\"\"\n+        Take a sequence of N fields and a sequence of M rows of values, and\n+        generate placeholder SQL and parameters for each field and value.\n+        Return a pair containing:\n+         * a sequence of M rows of N SQL placeholder strings, and\n+         * a sequence of M rows of corresponding parameter values.\n+\n+        Each placeholder string may contain any number of '%s' interpolation\n+        strings, and each parameter row will contain exactly as many params\n+        as the total number of '%s's in the corresponding placeholder row.\n+        \"\"\"\n+        if not value_rows:\n+            return [], []\n+\n+        # list of (sql, [params]) tuples for each object to be saved\n+        # Shape: [n_objs][n_fields][2]\n+        rows_of_fields_as_sql = (\n+            (self.field_as_sql(field, v) for field, v in zip(fields, row))\n+            for row in value_rows\n+        )\n+\n+        # tuple like ([sqls], [[params]s]) for each object to be saved\n+        # Shape: [n_objs][2][n_fields]\n+        sql_and_param_pair_rows = (zip(*row) for row in rows_of_fields_as_sql)\n+\n+        # Extract separate lists for placeholders and params.\n+        # Each of these has shape [n_objs][n_fields]\n+        placeholder_rows, param_rows = zip(*sql_and_param_pair_rows)\n+\n+        # Params for each field are still lists, and need to be flattened.\n+        param_rows = [[p for ps in row for p in ps] for row in param_rows]\n+\n+        return placeholder_rows, param_rows\n+\n+    def as_sql(self):\n+        # We don't need quote_name_unless_alias() here, since these are all\n+        # going to be column names (so we can avoid the extra overhead).\n+        qn = self.connection.ops.quote_name\n+        opts = self.query.get_meta()\n+        insert_statement = self.connection.ops.insert_statement(ignore_conflicts=self.query.ignore_conflicts)\n+        result = ['%s %s' % (insert_statement, qn(opts.db_table))]\n+        fields = self.query.fields or [opts.pk]\n+        result.append('(%s)' % ', '.join(qn(f.column) for f in fields))\n+\n+        if self.query.fields:\n+            value_rows = [\n+                [self.prepare_value(field, self.pre_save_val(field, obj)) for field in fields]\n+                for obj in self.query.objs\n+            ]\n+        else:\n+            # An empty object.\n+            value_rows = [[self.connection.ops.pk_default_value()] for _ in self.query.objs]\n+            fields = [None]\n+\n+        # Currently the backends just accept values when generating bulk\n+        # queries and generate their own placeholders. Doing that isn't\n+        # necessary and it should be possible to use placeholders and\n+        # expressions in bulk inserts too.\n+        can_bulk = (not self.return_id and self.connection.features.has_bulk_insert)\n+\n+        placeholder_rows, param_rows = self.assemble_as_sql(fields, value_rows)\n+\n+        ignore_conflicts_suffix_sql = self.connection.ops.ignore_conflicts_suffix_sql(\n+            ignore_conflicts=self.query.ignore_conflicts\n+        )\n+        if self.return_id and self.connection.features.can_return_columns_from_insert:\n+            if self.connection.features.can_return_rows_from_bulk_insert:\n+                result.append(self.connection.ops.bulk_insert_sql(fields, placeholder_rows))\n+                params = param_rows\n+            else:\n+                result.append(\"VALUES (%s)\" % \", \".join(placeholder_rows[0]))\n+                params = [param_rows[0]]\n+            if ignore_conflicts_suffix_sql:\n+                result.append(ignore_conflicts_suffix_sql)\n+            col = \"%s.%s\" % (qn(opts.db_table), qn(opts.pk.column))\n+            r_fmt, r_params = self.connection.ops.return_insert_id()\n+            # Skip empty r_fmt to allow subclasses to customize behavior for\n+            # 3rd party backends. Refs #19096.\n+            if r_fmt:\n+                result.append(r_fmt % col)\n+                params += [r_params]\n+            return [(\" \".join(result), tuple(chain.from_iterable(params)))]\n+\n+        if can_bulk:\n+            result.append(self.connection.ops.bulk_insert_sql(fields, placeholder_rows))\n+            if ignore_conflicts_suffix_sql:\n+                result.append(ignore_conflicts_suffix_sql)\n+            return [(\" \".join(result), tuple(p for ps in param_rows for p in ps))]\n+        else:\n+            if ignore_conflicts_suffix_sql:\n+                result.append(ignore_conflicts_suffix_sql)\n+            return [\n+                (\" \".join(result + [\"VALUES (%s)\" % \", \".join(p)]), vals)\n+                for p, vals in zip(placeholder_rows, param_rows)\n+            ]\n+\n+    def execute_sql(self, return_id=False):\n+        assert not (\n+            return_id and len(self.query.objs) != 1 and\n+            not self.connection.features.can_return_rows_from_bulk_insert\n+        )\n+        self.return_id = return_id\n+        with self.connection.cursor() as cursor:\n+            for sql, params in self.as_sql():\n+                cursor.execute(sql, params)\n+            if not return_id:\n+                return\n+            if self.connection.features.can_return_rows_from_bulk_insert and len(self.query.objs) > 1:\n+                return self.connection.ops.fetch_returned_insert_ids(cursor)\n+            if self.connection.features.can_return_columns_from_insert:\n+                assert len(self.query.objs) == 1\n+                return self.connection.ops.fetch_returned_insert_id(cursor)\n+            return self.connection.ops.last_insert_id(\n+                cursor, self.query.get_meta().db_table, self.query.get_meta().pk.column\n+            )\n+\n+\n+class SQLDeleteCompiler(SQLCompiler):\n+    def as_sql(self):\n+        \"\"\"\n+        Create the SQL for this query. Return the SQL string and list of\n+        parameters.\n+        \"\"\"\n+        assert len([t for t in self.query.alias_map if self.query.alias_refcount[t] > 0]) == 1, \\\n+            \"Can only delete from one table at a time.\"\n+        qn = self.quote_name_unless_alias\n+        result = ['DELETE FROM %s' % qn(self.query.base_table)]\n+        where, params = self.compile(self.query.where)\n+        if where:\n+            result.append('WHERE %s' % where)\n+        return ' '.join(result), tuple(params)\n+\n+\n+class SQLUpdateCompiler(SQLCompiler):\n+    def as_sql(self):\n+        \"\"\"\n+        Create the SQL for this query. Return the SQL string and list of\n+        parameters.\n+        \"\"\"\n+        self.pre_sql_setup()\n+        if not self.query.values:\n+            return '', ()\n+        qn = self.quote_name_unless_alias\n+        values, update_params = [], []\n+        for field, model, val in self.query.values:\n+            if hasattr(val, 'resolve_expression'):\n+                val = val.resolve_expression(self.query, allow_joins=False, for_save=True)\n+                if val.contains_aggregate:\n+                    raise FieldError(\n+                        'Aggregate functions are not allowed in this query '\n+                        '(%s=%r).' % (field.name, val)\n+                    )\n+                if val.contains_over_clause:\n+                    raise FieldError(\n+                        'Window expressions are not allowed in this query '\n+                        '(%s=%r).' % (field.name, val)\n+                    )\n+            elif hasattr(val, 'prepare_database_save'):\n+                if field.remote_field:\n+                    val = field.get_db_prep_save(\n+                        val.prepare_database_save(field),\n+                        connection=self.connection,\n+                    )\n+                else:\n+                    raise TypeError(\n+                        \"Tried to update field %s with a model instance, %r. \"\n+                        \"Use a value compatible with %s.\"\n+                        % (field, val, field.__class__.__name__)\n+                    )\n+            else:\n+                val = field.get_db_prep_save(val, connection=self.connection)\n+\n+            # Getting the placeholder for the field.\n+            if hasattr(field, 'get_placeholder'):\n+                placeholder = field.get_placeholder(val, self, self.connection)\n+            else:\n+                placeholder = '%s'\n+            name = field.column\n+            if hasattr(val, 'as_sql'):\n+                sql, params = self.compile(val)\n+                values.append('%s = %s' % (qn(name), placeholder % sql))\n+                update_params.extend(params)\n+            elif val is not None:\n+                values.append('%s = %s' % (qn(name), placeholder))\n+                update_params.append(val)\n+            else:\n+                values.append('%s = NULL' % qn(name))\n+        table = self.query.base_table\n+        result = [\n+            'UPDATE %s SET' % qn(table),\n+            ', '.join(values),\n+        ]\n+        where, params = self.compile(self.query.where)\n+        if where:\n+            result.append('WHERE %s' % where)\n+        return ' '.join(result), tuple(update_params + params)\n+\n+    def execute_sql(self, result_type):\n+        \"\"\"\n+        Execute the specified update. Return the number of rows affected by\n+        the primary update query. The \"primary update query\" is the first\n+        non-empty query that is executed. Row counts for any subsequent,\n+        related queries are not available.\n+        \"\"\"\n+        cursor = super().execute_sql(result_type)\n+        try:\n+            rows = cursor.rowcount if cursor else 0\n+            is_empty = cursor is None\n+        finally:\n+            if cursor:\n+                cursor.close()\n+        for query in self.query.get_related_updates():\n+            aux_rows = query.get_compiler(self.using).execute_sql(result_type)\n+            if is_empty and aux_rows:\n+                rows = aux_rows\n+                is_empty = False\n+        return rows\n+\n+    def pre_sql_setup(self):\n+        \"\"\"\n+        If the update depends on results from other tables, munge the \"where\"\n+        conditions to match the format required for (portable) SQL updates.\n+\n+        If multiple updates are required, pull out the id values to update at\n+        this point so that they don't change as a result of the progressive\n+        updates.\n+        \"\"\"\n+        refcounts_before = self.query.alias_refcount.copy()\n+        # Ensure base table is in the query\n+        self.query.get_initial_alias()\n+        count = self.query.count_active_tables()\n+        if not self.query.related_updates and count == 1:\n+            return\n+        query = self.query.chain(klass=Query)\n+        query.select_related = False\n+        query.clear_ordering(True)\n+        query.extra = {}\n+        query.select = []\n+        query.add_fields([query.get_meta().pk.name])\n+        super().pre_sql_setup()\n+\n+        must_pre_select = count > 1 and not self.connection.features.update_can_self_select\n+\n+        # Now we adjust the current query: reset the where clause and get rid\n+        # of all the tables we don't need (since they're in the sub-select).\n+        self.query.where = self.query.where_class()\n+        if self.query.related_updates or must_pre_select:\n+            # Either we're using the idents in multiple update queries (so\n+            # don't want them to change), or the db backend doesn't support\n+            # selecting from the updating table (e.g. MySQL).\n+            idents = []\n+            for rows in query.get_compiler(self.using).execute_sql(MULTI):\n+                idents.extend(r[0] for r in rows)\n+            self.query.add_filter(('pk__in', idents))\n+            self.query.related_ids = idents\n+        else:\n+            # The fast path. Filters and updates in one query.\n+            self.query.add_filter(('pk__in', query))\n+        self.query.reset_refcounts(refcounts_before)\n+\n+\n+class SQLAggregateCompiler(SQLCompiler):\n+    def as_sql(self):\n+        \"\"\"\n+        Create the SQL for this query. Return the SQL string and list of\n+        parameters.\n+        \"\"\"\n+        sql, params = [], []\n+        for annotation in self.query.annotation_select.values():\n+            ann_sql, ann_params = self.compile(annotation, select_format=FORCE)\n+            sql.append(ann_sql)\n+            params.extend(ann_params)\n+        self.col_count = len(self.query.annotation_select)\n+        sql = ', '.join(sql)\n+        params = tuple(params)\n+\n+        sql = 'SELECT %s FROM (%s) subquery' % (sql, self.query.subquery)\n+        params = params + self.query.sub_params\n+        return sql, params\n+\n+\n+def cursor_iter(cursor, sentinel, col_count, itersize):\n+    \"\"\"\n+    Yield blocks of rows from a cursor and ensure the cursor is closed when\n+    done.\n+    \"\"\"\n+    try:\n+        for rows in iter((lambda: cursor.fetchmany(itersize)), sentinel):\n+            yield rows if col_count is None else [r[:col_count] for r in rows]\n+    finally:\n+        cursor.close()\ndiff --git a/django/db/models/sql/compiler.py b/django/db/models/sql/compiler.py\nindex 7649c39262..57150f96dc 100644\n--- a/django/db/models/sql/compiler.py\n+++ b/django/db/models/sql/compiler.py\n@@ -32,7 +32,7 @@ class SQLCompiler:\n         self.select = None\n         self.annotation_col_map = None\n         self.klass_info = None\n-        self.ordering_parts = re.compile(r'(.*)\\s(ASC|DESC)(.*)')\n+        self.ordering_parts = re.compile(r'(.*)\\s(ASC|DESC)(.*)', re.DOTALL)\n         self._meta_ordering = None\n \n     def setup_query(self):\ndiff --git a/tests/ordering/tests.py b/tests/ordering/tests.py\nindex 16e5cc9b2d..86104d5c92 100644\n--- a/tests/ordering/tests.py\n+++ b/tests/ordering/tests.py\n@@ -2,6 +2,7 @@ from datetime import datetime\n from operator import attrgetter\n \n from django.db.models import Count, DateTimeField, F, Max, OuterRef, Subquery\n+from django.db.models.expressions import RawSQL\n from django.db.models.functions import Upper\n from django.test import TestCase\n from django.utils.deprecation import RemovedInDjango31Warning\n@@ -412,3 +413,36 @@ class OrderingTests(TestCase):\n         )\n         with self.assertRaisesMessage(RemovedInDjango31Warning, msg):\n             list(Article.objects.values('author').annotate(Count('headline')))\n+\n+    def test_order_by_multiline_rawsql_duplicates_not_removed(self):\n+        \"\"\"\n+        Multiline RawSQL expressions used in order_by() that share identical\n+        trailing lines must not be incorrectly identified as duplicates and\n+        dropped (#28712).\n+        \"\"\"\n+        # Two distinct multiline RawSQL expressions whose last lines are the\n+        # same (both end with \"else null end\"). Without the fix, the second\n+        # expression was mistakenly considered a duplicate of the first and\n+        # silently omitted from the ORDER BY clause.\n+        raw1 = RawSQL(\n+            '''\n+            case when headline = 'Article 1'\n+                 then 1 else 0 end''',\n+            [],\n+        )\n+        raw2 = RawSQL(\n+            '''\n+            case when headline = 'Article 2'\n+                 then 1 else 0 end''',\n+            [],\n+        )\n+        qs = Article.objects.order_by(raw1.desc(), raw2.asc())\n+        # Retrieve the compiled ORDER BY SQL.  It must contain both expressions.\n+        compiler = qs.query.get_compiler(using='default')\n+        order_by_clauses = compiler.get_order_by()\n+        # Both expressions must survive de-duplication.\n+        self.assertEqual(\n+            len(order_by_clauses), 2,\n+            \"Expected 2 ORDER BY clauses but got %d; the second multiline \"\n+            \"RawSQL was incorrectly removed as a duplicate.\" % len(order_by_clauses),\n+        )\n"}
{"instance_id": "django__django-11019", "model_name_or_path": "harness/claude-sonnet-4-6", "model_patch": "diff --git a/.harness/checkpoints/d15f7ee65496/--Users--huetuanthi--dev--agents--harness--django--forms--widgets.py b/.harness/checkpoints/d15f7ee65496/--Users--huetuanthi--dev--agents--harness--django--forms--widgets.py\nnew file mode 100644\nindex 0000000000..02aa32b207\n--- /dev/null\n+++ b/.harness/checkpoints/d15f7ee65496/--Users--huetuanthi--dev--agents--harness--django--forms--widgets.py\n@@ -0,0 +1,1083 @@\n+\"\"\"\n+HTML Widget classes\n+\"\"\"\n+\n+import copy\n+import datetime\n+import re\n+import warnings\n+from itertools import chain\n+\n+from django.conf import settings\n+from django.forms.utils import to_current_timezone\n+from django.templatetags.static import static\n+from django.utils import datetime_safe, formats\n+from django.utils.dates import MONTHS\n+from django.utils.formats import get_format\n+from django.utils.html import format_html, html_safe\n+from django.utils.safestring import mark_safe\n+from django.utils.translation import gettext_lazy as _\n+\n+from .renderers import get_default_renderer\n+\n+__all__ = (\n+    'Media', 'MediaDefiningClass', 'Widget', 'TextInput', 'NumberInput',\n+    'EmailInput', 'URLInput', 'PasswordInput', 'HiddenInput',\n+    'MultipleHiddenInput', 'FileInput', 'ClearableFileInput', 'Textarea',\n+    'DateInput', 'DateTimeInput', 'TimeInput', 'CheckboxInput', 'Select',\n+    'NullBooleanSelect', 'SelectMultiple', 'RadioSelect',\n+    'CheckboxSelectMultiple', 'MultiWidget', 'SplitDateTimeWidget',\n+    'SplitHiddenDateTimeWidget', 'SelectDateWidget',\n+)\n+\n+MEDIA_TYPES = ('css', 'js')\n+\n+\n+class MediaOrderConflictWarning(RuntimeWarning):\n+    pass\n+\n+\n+@html_safe\n+class Media:\n+    def __init__(self, media=None, css=None, js=None):\n+        if media is not None:\n+            css = getattr(media, 'css', {})\n+            js = getattr(media, 'js', [])\n+        else:\n+            if css is None:\n+                css = {}\n+            if js is None:\n+                js = []\n+        self._css_lists = [css]\n+        self._js_lists = [js]\n+\n+    def __repr__(self):\n+        return 'Media(css=%r, js=%r)' % (self._css, self._js)\n+\n+    def __str__(self):\n+        return self.render()\n+\n+    @property\n+    def _css(self):\n+        css = self._css_lists[0]\n+        # filter(None, ...) avoids calling merge with empty dicts.\n+        for obj in filter(None, self._css_lists[1:]):\n+            css = {\n+                medium: self.merge(css.get(medium, []), obj.get(medium, []))\n+                for medium in css.keys() | obj.keys()\n+            }\n+        return css\n+\n+    @property\n+    def _js(self):\n+        js = self._js_lists[0]\n+        # filter(None, ...) avoids calling merge() with empty lists.\n+        for obj in filter(None, self._js_lists[1:]):\n+            js = self.merge(js, obj)\n+        return js\n+\n+    def render(self):\n+        return mark_safe('\\n'.join(chain.from_iterable(getattr(self, 'render_' + name)() for name in MEDIA_TYPES)))\n+\n+    def render_js(self):\n+        return [\n+            format_html(\n+                '<script type=\"text/javascript\" src=\"{}\"></script>',\n+                self.absolute_path(path)\n+            ) for path in self._js\n+        ]\n+\n+    def render_css(self):\n+        # To keep rendering order consistent, we can't just iterate over items().\n+        # We need to sort the keys, and iterate over the sorted list.\n+        media = sorted(self._css)\n+        return chain.from_iterable([\n+            format_html(\n+                '<link href=\"{}\" type=\"text/css\" media=\"{}\" rel=\"stylesheet\">',\n+                self.absolute_path(path), medium\n+            ) for path in self._css[medium]\n+        ] for medium in media)\n+\n+    def absolute_path(self, path):\n+        \"\"\"\n+        Given a relative or absolute path to a static asset, return an absolute\n+        path. An absolute path will be returned unchanged while a relative path\n+        will be passed to django.templatetags.static.static().\n+        \"\"\"\n+        if path.startswith(('http://', 'https://', '/')):\n+            return path\n+        return static(path)\n+\n+    def __getitem__(self, name):\n+        \"\"\"Return a Media object that only contains media of the given type.\"\"\"\n+        if name in MEDIA_TYPES:\n+            return Media(**{str(name): getattr(self, '_' + name)})\n+        raise KeyError('Unknown media type \"%s\"' % name)\n+\n+    @staticmethod\n+    def merge(list_1, list_2):\n+        \"\"\"\n+        Merge two lists while trying to keep the relative order of the elements.\n+        Warn if the lists have the same two elements in a different relative\n+        order.\n+\n+        For static assets it can be important to have them included in the DOM\n+        in a certain order. In JavaScript you may not be able to reference a\n+        global or in CSS you might want to override a style.\n+        \"\"\"\n+        # Start with a copy of list_1.\n+        combined_list = list(list_1)\n+        last_insert_index = len(list_1)\n+        # Walk list_2 in reverse, inserting each element into combined_list if\n+        # it doesn't already exist.\n+        for path in reversed(list_2):\n+            try:\n+                # Does path already exist in the list?\n+                index = combined_list.index(path)\n+            except ValueError:\n+                # Add path to combined_list since it doesn't exist.\n+                combined_list.insert(last_insert_index, path)\n+            else:\n+                if index > last_insert_index:\n+                    warnings.warn(\n+                        'Detected duplicate Media files in an opposite order:\\n'\n+                        '%s\\n%s' % (combined_list[last_insert_index], combined_list[index]),\n+                        MediaOrderConflictWarning,\n+                    )\n+                # path already exists in the list. Update last_insert_index so\n+                # that the following elements are inserted in front of this one.\n+                last_insert_index = index\n+        return combined_list\n+\n+    def __add__(self, other):\n+        combined = Media()\n+        combined._css_lists = self._css_lists + other._css_lists\n+        combined._js_lists = self._js_lists + other._js_lists\n+        return combined\n+\n+\n+def media_property(cls):\n+    def _media(self):\n+        # Get the media property of the superclass, if it exists\n+        sup_cls = super(cls, self)\n+        try:\n+            base = sup_cls.media\n+        except AttributeError:\n+            base = Media()\n+\n+        # Get the media definition for this class\n+        definition = getattr(cls, 'Media', None)\n+        if definition:\n+            extend = getattr(definition, 'extend', True)\n+            if extend:\n+                if extend is True:\n+                    m = base\n+                else:\n+                    m = Media()\n+                    for medium in extend:\n+                        m = m + base[medium]\n+                return m + Media(definition)\n+            return Media(definition)\n+        return base\n+    return property(_media)\n+\n+\n+class MediaDefiningClass(type):\n+    \"\"\"\n+    Metaclass for classes that can have media definitions.\n+    \"\"\"\n+    def __new__(mcs, name, bases, attrs):\n+        new_class = super(MediaDefiningClass, mcs).__new__(mcs, name, bases, attrs)\n+\n+        if 'media' not in attrs:\n+            new_class.media = media_property(new_class)\n+\n+        return new_class\n+\n+\n+class Widget(metaclass=MediaDefiningClass):\n+    needs_multipart_form = False  # Determines does this widget need multipart form\n+    is_localized = False\n+    is_required = False\n+    supports_microseconds = True\n+\n+    def __init__(self, attrs=None):\n+        self.attrs = {} if attrs is None else attrs.copy()\n+\n+    def __deepcopy__(self, memo):\n+        obj = copy.copy(self)\n+        obj.attrs = self.attrs.copy()\n+        memo[id(self)] = obj\n+        return obj\n+\n+    @property\n+    def is_hidden(self):\n+        return self.input_type == 'hidden' if hasattr(self, 'input_type') else False\n+\n+    def subwidgets(self, name, value, attrs=None):\n+        context = self.get_context(name, value, attrs)\n+        yield context['widget']\n+\n+    def format_value(self, value):\n+        \"\"\"\n+        Return a value as it should appear when rendered in a template.\n+        \"\"\"\n+        if value == '' or value is None:\n+            return None\n+        if self.is_localized:\n+            return formats.localize_input(value)\n+        return str(value)\n+\n+    def get_context(self, name, value, attrs):\n+        context = {}\n+        context['widget'] = {\n+            'name': name,\n+            'is_hidden': self.is_hidden,\n+            'required': self.is_required,\n+            'value': self.format_value(value),\n+            'attrs': self.build_attrs(self.attrs, attrs),\n+            'template_name': self.template_name,\n+        }\n+        return context\n+\n+    def render(self, name, value, attrs=None, renderer=None):\n+        \"\"\"Render the widget as an HTML string.\"\"\"\n+        context = self.get_context(name, value, attrs)\n+        return self._render(self.template_name, context, renderer)\n+\n+    def _render(self, template_name, context, renderer=None):\n+        if renderer is None:\n+            renderer = get_default_renderer()\n+        return mark_safe(renderer.render(template_name, context))\n+\n+    def build_attrs(self, base_attrs, extra_attrs=None):\n+        \"\"\"Build an attribute dictionary.\"\"\"\n+        return {**base_attrs, **(extra_attrs or {})}\n+\n+    def value_from_datadict(self, data, files, name):\n+        \"\"\"\n+        Given a dictionary of data and this widget's name, return the value\n+        of this widget or None if it's not provided.\n+        \"\"\"\n+        return data.get(name)\n+\n+    def value_omitted_from_data(self, data, files, name):\n+        return name not in data\n+\n+    def id_for_label(self, id_):\n+        \"\"\"\n+        Return the HTML ID attribute of this Widget for use by a <label>,\n+        given the ID of the field. Return None if no ID is available.\n+\n+        This hook is necessary because some widgets have multiple HTML\n+        elements and, thus, multiple IDs. In that case, this method should\n+        return an ID value that corresponds to the first ID in the widget's\n+        tags.\n+        \"\"\"\n+        return id_\n+\n+    def use_required_attribute(self, initial):\n+        return not self.is_hidden\n+\n+\n+class Input(Widget):\n+    \"\"\"\n+    Base class for all <input> widgets.\n+    \"\"\"\n+    input_type = None  # Subclasses must define this.\n+    template_name = 'django/forms/widgets/input.html'\n+\n+    def __init__(self, attrs=None):\n+        if attrs is not None:\n+            attrs = attrs.copy()\n+            self.input_type = attrs.pop('type', self.input_type)\n+        super().__init__(attrs)\n+\n+    def get_context(self, name, value, attrs):\n+        context = super().get_context(name, value, attrs)\n+        context['widget']['type'] = self.input_type\n+        return context\n+\n+\n+class TextInput(Input):\n+    input_type = 'text'\n+    template_name = 'django/forms/widgets/text.html'\n+\n+\n+class NumberInput(Input):\n+    input_type = 'number'\n+    template_name = 'django/forms/widgets/number.html'\n+\n+\n+class EmailInput(Input):\n+    input_type = 'email'\n+    template_name = 'django/forms/widgets/email.html'\n+\n+\n+class URLInput(Input):\n+    input_type = 'url'\n+    template_name = 'django/forms/widgets/url.html'\n+\n+\n+class PasswordInput(Input):\n+    input_type = 'password'\n+    template_name = 'django/forms/widgets/password.html'\n+\n+    def __init__(self, attrs=None, render_value=False):\n+        super().__init__(attrs)\n+        self.render_value = render_value\n+\n+    def get_context(self, name, value, attrs):\n+        if not self.render_value:\n+            value = None\n+        return super().get_context(name, value, attrs)\n+\n+\n+class HiddenInput(Input):\n+    input_type = 'hidden'\n+    template_name = 'django/forms/widgets/hidden.html'\n+\n+\n+class MultipleHiddenInput(HiddenInput):\n+    \"\"\"\n+    Handle <input type=\"hidden\"> for fields that have a list\n+    of values.\n+    \"\"\"\n+    template_name = 'django/forms/widgets/multiple_hidden.html'\n+\n+    def get_context(self, name, value, attrs):\n+        context = super().get_context(name, value, attrs)\n+        final_attrs = context['widget']['attrs']\n+        id_ = context['widget']['attrs'].get('id')\n+\n+        subwidgets = []\n+        for index, value_ in enumerate(context['widget']['value']):\n+            widget_attrs = final_attrs.copy()\n+            if id_:\n+                # An ID attribute was given. Add a numeric index as a suffix\n+                # so that the inputs don't all have the same ID attribute.\n+                widget_attrs['id'] = '%s_%s' % (id_, index)\n+            widget = HiddenInput()\n+            widget.is_required = self.is_required\n+            subwidgets.append(widget.get_context(name, value_, widget_attrs)['widget'])\n+\n+        context['widget']['subwidgets'] = subwidgets\n+        return context\n+\n+    def value_from_datadict(self, data, files, name):\n+        try:\n+            getter = data.getlist\n+        except AttributeError:\n+            getter = data.get\n+        return getter(name)\n+\n+    def format_value(self, value):\n+        return [] if value is None else value\n+\n+\n+class FileInput(Input):\n+    input_type = 'file'\n+    needs_multipart_form = True\n+    template_name = 'django/forms/widgets/file.html'\n+\n+    def format_value(self, value):\n+        \"\"\"File input never renders a value.\"\"\"\n+        return\n+\n+    def value_from_datadict(self, data, files, name):\n+        \"File widgets take data from FILES, not POST\"\n+        return files.get(name)\n+\n+    def value_omitted_from_data(self, data, files, name):\n+        return name not in files\n+\n+\n+FILE_INPUT_CONTRADICTION = object()\n+\n+\n+class ClearableFileInput(FileInput):\n+    clear_checkbox_label = _('Clear')\n+    initial_text = _('Currently')\n+    input_text = _('Change')\n+    template_name = 'django/forms/widgets/clearable_file_input.html'\n+\n+    def clear_checkbox_name(self, name):\n+        \"\"\"\n+        Given the name of the file input, return the name of the clear checkbox\n+        input.\n+        \"\"\"\n+        return name + '-clear'\n+\n+    def clear_checkbox_id(self, name):\n+        \"\"\"\n+        Given the name of the clear checkbox input, return the HTML id for it.\n+        \"\"\"\n+        return name + '_id'\n+\n+    def is_initial(self, value):\n+        \"\"\"\n+        Return whether value is considered to be initial value.\n+        \"\"\"\n+        return bool(value and getattr(value, 'url', False))\n+\n+    def format_value(self, value):\n+        \"\"\"\n+        Return the file object if it has a defined url attribute.\n+        \"\"\"\n+        if self.is_initial(value):\n+            return value\n+\n+    def get_context(self, name, value, attrs):\n+        context = super().get_context(name, value, attrs)\n+        checkbox_name = self.clear_checkbox_name(name)\n+        checkbox_id = self.clear_checkbox_id(checkbox_name)\n+        context['widget'].update({\n+            'checkbox_name': checkbox_name,\n+            'checkbox_id': checkbox_id,\n+            'is_initial': self.is_initial(value),\n+            'input_text': self.input_text,\n+            'initial_text': self.initial_text,\n+            'clear_checkbox_label': self.clear_checkbox_label,\n+        })\n+        return context\n+\n+    def value_from_datadict(self, data, files, name):\n+        upload = super().value_from_datadict(data, files, name)\n+        if not self.is_required and CheckboxInput().value_from_datadict(\n+                data, files, self.clear_checkbox_name(name)):\n+\n+            if upload:\n+                # If the user contradicts themselves (uploads a new file AND\n+                # checks the \"clear\" checkbox), we return a unique marker\n+                # object that FileField will turn into a ValidationError.\n+                return FILE_INPUT_CONTRADICTION\n+            # False signals to clear any existing value, as opposed to just None\n+            return False\n+        return upload\n+\n+    def use_required_attribute(self, initial):\n+        return super().use_required_attribute(initial) and not initial\n+\n+    def value_omitted_from_data(self, data, files, name):\n+        return (\n+            super().value_omitted_from_data(data, files, name) and\n+            self.clear_checkbox_name(name) not in data\n+        )\n+\n+\n+class Textarea(Widget):\n+    template_name = 'django/forms/widgets/textarea.html'\n+\n+    def __init__(self, attrs=None):\n+        # Use slightly better defaults than HTML's 20x2 box\n+        default_attrs = {'cols': '40', 'rows': '10'}\n+        if attrs:\n+            default_attrs.update(attrs)\n+        super().__init__(default_attrs)\n+\n+\n+class DateTimeBaseInput(TextInput):\n+    format_key = ''\n+    supports_microseconds = False\n+\n+    def __init__(self, attrs=None, format=None):\n+        super().__init__(attrs)\n+        self.format = format or None\n+\n+    def format_value(self, value):\n+        return formats.localize_input(value, self.format or formats.get_format(self.format_key)[0])\n+\n+\n+class DateInput(DateTimeBaseInput):\n+    format_key = 'DATE_INPUT_FORMATS'\n+    template_name = 'django/forms/widgets/date.html'\n+\n+\n+class DateTimeInput(DateTimeBaseInput):\n+    format_key = 'DATETIME_INPUT_FORMATS'\n+    template_name = 'django/forms/widgets/datetime.html'\n+\n+\n+class TimeInput(DateTimeBaseInput):\n+    format_key = 'TIME_INPUT_FORMATS'\n+    template_name = 'django/forms/widgets/time.html'\n+\n+\n+# Defined at module level so that CheckboxInput is picklable (#17976)\n+def boolean_check(v):\n+    return not (v is False or v is None or v == '')\n+\n+\n+class CheckboxInput(Input):\n+    input_type = 'checkbox'\n+    template_name = 'django/forms/widgets/checkbox.html'\n+\n+    def __init__(self, attrs=None, check_test=None):\n+        super().__init__(attrs)\n+        # check_test is a callable that takes a value and returns True\n+        # if the checkbox should be checked for that value.\n+        self.check_test = boolean_check if check_test is None else check_test\n+\n+    def format_value(self, value):\n+        \"\"\"Only return the 'value' attribute if value isn't empty.\"\"\"\n+        if value is True or value is False or value is None or value == '':\n+            return\n+        return str(value)\n+\n+    def get_context(self, name, value, attrs):\n+        if self.check_test(value):\n+            if attrs is None:\n+                attrs = {}\n+            attrs['checked'] = True\n+        return super().get_context(name, value, attrs)\n+\n+    def value_from_datadict(self, data, files, name):\n+        if name not in data:\n+            # A missing value means False because HTML form submission does not\n+            # send results for unselected checkboxes.\n+            return False\n+        value = data.get(name)\n+        # Translate true and false strings to boolean values.\n+        values = {'true': True, 'false': False}\n+        if isinstance(value, str):\n+            value = values.get(value.lower(), value)\n+        return bool(value)\n+\n+    def value_omitted_from_data(self, data, files, name):\n+        # HTML checkboxes don't appear in POST data if not checked, so it's\n+        # never known if the value is actually omitted.\n+        return False\n+\n+\n+class ChoiceWidget(Widget):\n+    allow_multiple_selected = False\n+    input_type = None\n+    template_name = None\n+    option_template_name = None\n+    add_id_index = True\n+    checked_attribute = {'checked': True}\n+    option_inherits_attrs = True\n+\n+    def __init__(self, attrs=None, choices=()):\n+        super().__init__(attrs)\n+        # choices can be any iterable, but we may need to render this widget\n+        # multiple times. Thus, collapse it into a list so it can be consumed\n+        # more than once.\n+        self.choices = list(choices)\n+\n+    def __deepcopy__(self, memo):\n+        obj = copy.copy(self)\n+        obj.attrs = self.attrs.copy()\n+        obj.choices = copy.copy(self.choices)\n+        memo[id(self)] = obj\n+        return obj\n+\n+    def subwidgets(self, name, value, attrs=None):\n+        \"\"\"\n+        Yield all \"subwidgets\" of this widget. Used to enable iterating\n+        options from a BoundField for choice widgets.\n+        \"\"\"\n+        value = self.format_value(value)\n+        yield from self.options(name, value, attrs)\n+\n+    def options(self, name, value, attrs=None):\n+        \"\"\"Yield a flat list of options for this widgets.\"\"\"\n+        for group in self.optgroups(name, value, attrs):\n+            yield from group[1]\n+\n+    def optgroups(self, name, value, attrs=None):\n+        \"\"\"Return a list of optgroups for this widget.\"\"\"\n+        groups = []\n+        has_selected = False\n+\n+        for index, (option_value, option_label) in enumerate(self.choices):\n+            if option_value is None:\n+                option_value = ''\n+\n+            subgroup = []\n+            if isinstance(option_label, (list, tuple)):\n+                group_name = option_value\n+                subindex = 0\n+                choices = option_label\n+            else:\n+                group_name = None\n+                subindex = None\n+                choices = [(option_value, option_label)]\n+            groups.append((group_name, subgroup, index))\n+\n+            for subvalue, sublabel in choices:\n+                selected = (\n+                    str(subvalue) in value and\n+                    (not has_selected or self.allow_multiple_selected)\n+                )\n+                has_selected |= selected\n+                subgroup.append(self.create_option(\n+                    name, subvalue, sublabel, selected, index,\n+                    subindex=subindex, attrs=attrs,\n+                ))\n+                if subindex is not None:\n+                    subindex += 1\n+        return groups\n+\n+    def create_option(self, name, value, label, selected, index, subindex=None, attrs=None):\n+        index = str(index) if subindex is None else \"%s_%s\" % (index, subindex)\n+        if attrs is None:\n+            attrs = {}\n+        option_attrs = self.build_attrs(self.attrs, attrs) if self.option_inherits_attrs else {}\n+        if selected:\n+            option_attrs.update(self.checked_attribute)\n+        if 'id' in option_attrs:\n+            option_attrs['id'] = self.id_for_label(option_attrs['id'], index)\n+        return {\n+            'name': name,\n+            'value': value,\n+            'label': label,\n+            'selected': selected,\n+            'index': index,\n+            'attrs': option_attrs,\n+            'type': self.input_type,\n+            'template_name': self.option_template_name,\n+            'wrap_label': True,\n+        }\n+\n+    def get_context(self, name, value, attrs):\n+        context = super().get_context(name, value, attrs)\n+        context['widget']['optgroups'] = self.optgroups(name, context['widget']['value'], attrs)\n+        return context\n+\n+    def id_for_label(self, id_, index='0'):\n+        \"\"\"\n+        Use an incremented id for each option where the main widget\n+        references the zero index.\n+        \"\"\"\n+        if id_ and self.add_id_index:\n+            id_ = '%s_%s' % (id_, index)\n+        return id_\n+\n+    def value_from_datadict(self, data, files, name):\n+        getter = data.get\n+        if self.allow_multiple_selected:\n+            try:\n+                getter = data.getlist\n+            except AttributeError:\n+                pass\n+        return getter(name)\n+\n+    def format_value(self, value):\n+        \"\"\"Return selected values as a list.\"\"\"\n+        if value is None and self.allow_multiple_selected:\n+            return []\n+        if not isinstance(value, (tuple, list)):\n+            value = [value]\n+        return [str(v) if v is not None else '' for v in value]\n+\n+\n+class Select(ChoiceWidget):\n+    input_type = 'select'\n+    template_name = 'django/forms/widgets/select.html'\n+    option_template_name = 'django/forms/widgets/select_option.html'\n+    add_id_index = False\n+    checked_attribute = {'selected': True}\n+    option_inherits_attrs = False\n+\n+    def get_context(self, name, value, attrs):\n+        context = super().get_context(name, value, attrs)\n+        if self.allow_multiple_selected:\n+            context['widget']['attrs']['multiple'] = True\n+        return context\n+\n+    @staticmethod\n+    def _choice_has_empty_value(choice):\n+        \"\"\"Return True if the choice's value is empty string or None.\"\"\"\n+        value, _ = choice\n+        return value is None or value == ''\n+\n+    def use_required_attribute(self, initial):\n+        \"\"\"\n+        Don't render 'required' if the first <option> has a value, as that's\n+        invalid HTML.\n+        \"\"\"\n+        use_required_attribute = super().use_required_attribute(initial)\n+        # 'required' is always okay for <select multiple>.\n+        if self.allow_multiple_selected:\n+            return use_required_attribute\n+\n+        first_choice = next(iter(self.choices), None)\n+        return use_required_attribute and first_choice is not None and self._choice_has_empty_value(first_choice)\n+\n+\n+class NullBooleanSelect(Select):\n+    \"\"\"\n+    A Select Widget intended to be used with NullBooleanField.\n+    \"\"\"\n+    def __init__(self, attrs=None):\n+        choices = (\n+            ('unknown', _('Unknown')),\n+            ('true', _('Yes')),\n+            ('false', _('No')),\n+        )\n+        super().__init__(attrs, choices)\n+\n+    def format_value(self, value):\n+        try:\n+            return {\n+                True: 'true', False: 'false',\n+                'true': 'true', 'false': 'false',\n+                # For backwards compatibility with Django < 2.2.\n+                '2': 'true', '3': 'false',\n+            }[value]\n+        except KeyError:\n+            return 'unknown'\n+\n+    def value_from_datadict(self, data, files, name):\n+        value = data.get(name)\n+        return {\n+            True: True,\n+            'True': True,\n+            'False': False,\n+            False: False,\n+            'true': True,\n+            'false': False,\n+            # For backwards compatibility with Django < 2.2.\n+            '2': True,\n+            '3': False,\n+        }.get(value)\n+\n+\n+class SelectMultiple(Select):\n+    allow_multiple_selected = True\n+\n+    def value_from_datadict(self, data, files, name):\n+        try:\n+            getter = data.getlist\n+        except AttributeError:\n+            getter = data.get\n+        return getter(name)\n+\n+    def value_omitted_from_data(self, data, files, name):\n+        # An unselected <select multiple> doesn't appear in POST data, so it's\n+        # never known if the value is actually omitted.\n+        return False\n+\n+\n+class RadioSelect(ChoiceWidget):\n+    input_type = 'radio'\n+    template_name = 'django/forms/widgets/radio.html'\n+    option_template_name = 'django/forms/widgets/radio_option.html'\n+\n+\n+class CheckboxSelectMultiple(ChoiceWidget):\n+    allow_multiple_selected = True\n+    input_type = 'checkbox'\n+    template_name = 'django/forms/widgets/checkbox_select.html'\n+    option_template_name = 'django/forms/widgets/checkbox_option.html'\n+\n+    def use_required_attribute(self, initial):\n+        # Don't use the 'required' attribute because browser validation would\n+        # require all checkboxes to be checked instead of at least one.\n+        return False\n+\n+    def value_omitted_from_data(self, data, files, name):\n+        # HTML checkboxes don't appear in POST data if not checked, so it's\n+        # never known if the value is actually omitted.\n+        return False\n+\n+    def id_for_label(self, id_, index=None):\n+        \"\"\"\"\n+        Don't include for=\"field_0\" in <label> because clicking such a label\n+        would toggle the first checkbox.\n+        \"\"\"\n+        if index is None:\n+            return ''\n+        return super().id_for_label(id_, index)\n+\n+\n+class MultiWidget(Widget):\n+    \"\"\"\n+    A widget that is composed of multiple widgets.\n+\n+    In addition to the values added by Widget.get_context(), this widget\n+    adds a list of subwidgets to the context as widget['subwidgets'].\n+    These can be looped over and rendered like normal widgets.\n+\n+    You'll probably want to use this class with MultiValueField.\n+    \"\"\"\n+    template_name = 'django/forms/widgets/multiwidget.html'\n+\n+    def __init__(self, widgets, attrs=None):\n+        self.widgets = [w() if isinstance(w, type) else w for w in widgets]\n+        super().__init__(attrs)\n+\n+    @property\n+    def is_hidden(self):\n+        return all(w.is_hidden for w in self.widgets)\n+\n+    def get_context(self, name, value, attrs):\n+        context = super().get_context(name, value, attrs)\n+        if self.is_localized:\n+            for widget in self.widgets:\n+                widget.is_localized = self.is_localized\n+        # value is a list of values, each corresponding to a widget\n+        # in self.widgets.\n+        if not isinstance(value, list):\n+            value = self.decompress(value)\n+\n+        final_attrs = context['widget']['attrs']\n+        input_type = final_attrs.pop('type', None)\n+        id_ = final_attrs.get('id')\n+        subwidgets = []\n+        for i, widget in enumerate(self.widgets):\n+            if input_type is not None:\n+                widget.input_type = input_type\n+            widget_name = '%s_%s' % (name, i)\n+            try:\n+                widget_value = value[i]\n+            except IndexError:\n+                widget_value = None\n+            if id_:\n+                widget_attrs = final_attrs.copy()\n+                widget_attrs['id'] = '%s_%s' % (id_, i)\n+            else:\n+                widget_attrs = final_attrs\n+            subwidgets.append(widget.get_context(widget_name, widget_value, widget_attrs)['widget'])\n+        context['widget']['subwidgets'] = subwidgets\n+        return context\n+\n+    def id_for_label(self, id_):\n+        if id_:\n+            id_ += '_0'\n+        return id_\n+\n+    def value_from_datadict(self, data, files, name):\n+        return [widget.value_from_datadict(data, files, name + '_%s' % i) for i, widget in enumerate(self.widgets)]\n+\n+    def value_omitted_from_data(self, data, files, name):\n+        return all(\n+            widget.value_omitted_from_data(data, files, name + '_%s' % i)\n+            for i, widget in enumerate(self.widgets)\n+        )\n+\n+    def decompress(self, value):\n+        \"\"\"\n+        Return a list of decompressed values for the given compressed value.\n+        The given value can be assumed to be valid, but not necessarily\n+        non-empty.\n+        \"\"\"\n+        raise NotImplementedError('Subclasses must implement this method.')\n+\n+    def _get_media(self):\n+        \"\"\"\n+        Media for a multiwidget is the combination of all media of the\n+        subwidgets.\n+        \"\"\"\n+        media = Media()\n+        for w in self.widgets:\n+            media = media + w.media\n+        return media\n+    media = property(_get_media)\n+\n+    def __deepcopy__(self, memo):\n+        obj = super().__deepcopy__(memo)\n+        obj.widgets = copy.deepcopy(self.widgets)\n+        return obj\n+\n+    @property\n+    def needs_multipart_form(self):\n+        return any(w.needs_multipart_form for w in self.widgets)\n+\n+\n+class SplitDateTimeWidget(MultiWidget):\n+    \"\"\"\n+    A widget that splits datetime input into two <input type=\"text\"> boxes.\n+    \"\"\"\n+    supports_microseconds = False\n+    template_name = 'django/forms/widgets/splitdatetime.html'\n+\n+    def __init__(self, attrs=None, date_format=None, time_format=None, date_attrs=None, time_attrs=None):\n+        widgets = (\n+            DateInput(\n+                attrs=attrs if date_attrs is None else date_attrs,\n+                format=date_format,\n+            ),\n+            TimeInput(\n+                attrs=attrs if time_attrs is None else time_attrs,\n+                format=time_format,\n+            ),\n+        )\n+        super().__init__(widgets)\n+\n+    def decompress(self, value):\n+        if value:\n+            value = to_current_timezone(value)\n+            return [value.date(), value.time()]\n+        return [None, None]\n+\n+\n+class SplitHiddenDateTimeWidget(SplitDateTimeWidget):\n+    \"\"\"\n+    A widget that splits datetime input into two <input type=\"hidden\"> inputs.\n+    \"\"\"\n+    template_name = 'django/forms/widgets/splithiddendatetime.html'\n+\n+    def __init__(self, attrs=None, date_format=None, time_format=None, date_attrs=None, time_attrs=None):\n+        super().__init__(attrs, date_format, time_format, date_attrs, time_attrs)\n+        for widget in self.widgets:\n+            widget.input_type = 'hidden'\n+\n+\n+class SelectDateWidget(Widget):\n+    \"\"\"\n+    A widget that splits date input into three <select> boxes.\n+\n+    This also serves as an example of a Widget that has more than one HTML\n+    element and hence implements value_from_datadict.\n+    \"\"\"\n+    none_value = ('', '---')\n+    month_field = '%s_month'\n+    day_field = '%s_day'\n+    year_field = '%s_year'\n+    template_name = 'django/forms/widgets/select_date.html'\n+    input_type = 'select'\n+    select_widget = Select\n+    date_re = re.compile(r'(\\d{4}|0)-(\\d\\d?)-(\\d\\d?)$')\n+\n+    def __init__(self, attrs=None, years=None, months=None, empty_label=None):\n+        self.attrs = attrs or {}\n+\n+        # Optional list or tuple of years to use in the \"year\" select box.\n+        if years:\n+            self.years = years\n+        else:\n+            this_year = datetime.date.today().year\n+            self.years = range(this_year, this_year + 10)\n+\n+        # Optional dict of months to use in the \"month\" select box.\n+        if months:\n+            self.months = months\n+        else:\n+            self.months = MONTHS\n+\n+        # Optional string, list, or tuple to use as empty_label.\n+        if isinstance(empty_label, (list, tuple)):\n+            if not len(empty_label) == 3:\n+                raise ValueError('empty_label list/tuple must have 3 elements.')\n+\n+            self.year_none_value = ('', empty_label[0])\n+            self.month_none_value = ('', empty_label[1])\n+            self.day_none_value = ('', empty_label[2])\n+        else:\n+            if empty_label is not None:\n+                self.none_value = ('', empty_label)\n+\n+            self.year_none_value = self.none_value\n+            self.month_none_value = self.none_value\n+            self.day_none_value = self.none_value\n+\n+    def get_context(self, name, value, attrs):\n+        context = super().get_context(name, value, attrs)\n+        date_context = {}\n+        year_choices = [(i, str(i)) for i in self.years]\n+        if not self.is_required:\n+            year_choices.insert(0, self.year_none_value)\n+        year_name = self.year_field % name\n+        date_context['year'] = self.select_widget(attrs, choices=year_choices).get_context(\n+            name=year_name,\n+            value=context['widget']['value']['year'],\n+            attrs={**context['widget']['attrs'], 'id': 'id_%s' % year_name},\n+        )\n+        month_choices = list(self.months.items())\n+        if not self.is_required:\n+            month_choices.insert(0, self.month_none_value)\n+        month_name = self.month_field % name\n+        date_context['month'] = self.select_widget(attrs, choices=month_choices).get_context(\n+            name=month_name,\n+            value=context['widget']['value']['month'],\n+            attrs={**context['widget']['attrs'], 'id': 'id_%s' % month_name},\n+        )\n+        day_choices = [(i, i) for i in range(1, 32)]\n+        if not self.is_required:\n+            day_choices.insert(0, self.day_none_value)\n+        day_name = self.day_field % name\n+        date_context['day'] = self.select_widget(attrs, choices=day_choices,).get_context(\n+            name=day_name,\n+            value=context['widget']['value']['day'],\n+            attrs={**context['widget']['attrs'], 'id': 'id_%s' % day_name},\n+        )\n+        subwidgets = []\n+        for field in self._parse_date_fmt():\n+            subwidgets.append(date_context[field]['widget'])\n+        context['widget']['subwidgets'] = subwidgets\n+        return context\n+\n+    def format_value(self, value):\n+        \"\"\"\n+        Return a dict containing the year, month, and day of the current value.\n+        Use dict instead of a datetime to allow invalid dates such as February\n+        31 to display correctly.\n+        \"\"\"\n+        year, month, day = None, None, None\n+        if isinstance(value, (datetime.date, datetime.datetime)):\n+            year, month, day = value.year, value.month, value.day\n+        elif isinstance(value, str):\n+            match = self.date_re.match(value)\n+            if match:\n+                # Convert any zeros in the date to empty strings to match the\n+                # empty option value.\n+                year, month, day = [int(val) or '' for val in match.groups()]\n+            elif settings.USE_L10N:\n+                input_format = get_format('DATE_INPUT_FORMATS')[0]\n+                try:\n+                    d = datetime.datetime.strptime(value, input_format)\n+                except ValueError:\n+                    pass\n+                else:\n+                    year, month, day = d.year, d.month, d.day\n+        return {'year': year, 'month': month, 'day': day}\n+\n+    @staticmethod\n+    def _parse_date_fmt():\n+        fmt = get_format('DATE_FORMAT')\n+        escaped = False\n+        for char in fmt:\n+            if escaped:\n+                escaped = False\n+            elif char == '\\\\':\n+                escaped = True\n+            elif char in 'Yy':\n+                yield 'year'\n+            elif char in 'bEFMmNn':\n+                yield 'month'\n+            elif char in 'dj':\n+                yield 'day'\n+\n+    def id_for_label(self, id_):\n+        for first_select in self._parse_date_fmt():\n+            return '%s_%s' % (id_, first_select)\n+        return '%s_month' % id_\n+\n+    def value_from_datadict(self, data, files, name):\n+        y = data.get(self.year_field % name)\n+        m = data.get(self.month_field % name)\n+        d = data.get(self.day_field % name)\n+        if y == m == d == '':\n+            return None\n+        if y is not None and m is not None and d is not None:\n+            if settings.USE_L10N:\n+                input_format = get_format('DATE_INPUT_FORMATS')[0]\n+                try:\n+                    date_value = datetime.date(int(y), int(m), int(d))\n+                except ValueError:\n+                    pass\n+                else:\n+                    date_value = datetime_safe.new_date(date_value)\n+                    return date_value.strftime(input_format)\n+            # Return pseudo-ISO dates with zeros for any unselected values,\n+            # e.g. '2017-0-23'.\n+            return '%s-%s-%s' % (y or 0, m or 0, d or 0)\n+        return data.get(name)\n+\n+    def value_omitted_from_data(self, data, files, name):\n+        return not any(\n+            ('{}_{}'.format(name, interval) in data)\n+            for interval in ('year', 'month', 'day')\n+        )\ndiff --git a/django/forms/widgets.py b/django/forms/widgets.py\nindex 02aa32b207..aa3eff678d 100644\n--- a/django/forms/widgets.py\n+++ b/django/forms/widgets.py\n@@ -59,22 +59,19 @@ class Media:\n \n     @property\n     def _css(self):\n-        css = self._css_lists[0]\n-        # filter(None, ...) avoids calling merge with empty dicts.\n-        for obj in filter(None, self._css_lists[1:]):\n-            css = {\n-                medium: self.merge(css.get(medium, []), obj.get(medium, []))\n-                for medium in css.keys() | obj.keys()\n-            }\n-        return css\n+        css = {}\n+        for obj in self._css_lists:\n+            for medium, sublist in obj.items():\n+                css.setdefault(medium, [])\n+                css[medium] = css[medium] + [sublist]\n+        return {\n+            medium: self.merge(*lists)\n+            for medium, lists in css.items()\n+        }\n \n     @property\n     def _js(self):\n-        js = self._js_lists[0]\n-        # filter(None, ...) avoids calling merge() with empty lists.\n-        for obj in filter(None, self._js_lists[1:]):\n-            js = self.merge(js, obj)\n-        return js\n+        return self.merge(*self._js_lists)\n \n     def render(self):\n         return mark_safe('\\n'.join(chain.from_iterable(getattr(self, 'render_' + name)() for name in MEDIA_TYPES)))\n@@ -115,39 +112,85 @@ class Media:\n         raise KeyError('Unknown media type \"%s\"' % name)\n \n     @staticmethod\n-    def merge(list_1, list_2):\n+    def merge(*lists):\n         \"\"\"\n-        Merge two lists while trying to keep the relative order of the elements.\n-        Warn if the lists have the same two elements in a different relative\n-        order.\n+        Merge multiple lists while trying to keep the relative order of the\n+        elements. Warn if the lists have the same two elements in a different\n+        relative order.\n \n         For static assets it can be important to have them included in the DOM\n         in a certain order. In JavaScript you may not be able to reference a\n         global or in CSS you might want to override a style.\n         \"\"\"\n-        # Start with a copy of list_1.\n-        combined_list = list(list_1)\n-        last_insert_index = len(list_1)\n-        # Walk list_2 in reverse, inserting each element into combined_list if\n-        # it doesn't already exist.\n-        for path in reversed(list_2):\n-            try:\n-                # Does path already exist in the list?\n-                index = combined_list.index(path)\n-            except ValueError:\n-                # Add path to combined_list since it doesn't exist.\n-                combined_list.insert(last_insert_index, path)\n-            else:\n-                if index > last_insert_index:\n-                    warnings.warn(\n-                        'Detected duplicate Media files in an opposite order:\\n'\n-                        '%s\\n%s' % (combined_list[last_insert_index], combined_list[index]),\n-                        MediaOrderConflictWarning,\n-                    )\n-                # path already exists in the list. Update last_insert_index so\n-                # that the following elements are inserted in front of this one.\n-                last_insert_index = index\n-        return combined_list\n+        # Collect all unique items in first-seen order across all lists.\n+        all_items = []\n+        item_set = set()\n+        for lst in lists:\n+            for item in lst:\n+                if item not in item_set:\n+                    all_items.append(item)\n+                    item_set.add(item)\n+\n+        if not all_items:\n+            return []\n+\n+        # Build a constraint graph: constraints[a] is the set of items that\n+        # must come after 'a'. Constraints are derived only from within each\n+        # individual list, so merging independent lists never creates false\n+        # ordering constraints.\n+        constraints = {item: set() for item in all_items}\n+        # Track pairs already warned about to avoid duplicate warnings.\n+        conflict_pairs = set()\n+\n+        for lst in lists:\n+            for i in range(len(lst)):\n+                for j in range(i + 1, len(lst)):\n+                    a, b = lst[i], lst[j]\n+                    # Skip duplicate items within the same list.\n+                    if a == b:\n+                        continue\n+                    # If 'b must come before a' is already a constraint, then\n+                    # 'a before b' (from this list) is a genuine conflict.\n+                    if a in constraints.get(b, set()):\n+                        key = (b, a)\n+                        if key not in conflict_pairs:\n+                            conflict_pairs.add(key)\n+                            warnings.warn(\n+                                'Detected duplicate Media files in an opposite order:\\n'\n+                                '%s\\n%s' % (b, a),\n+                                MediaOrderConflictWarning,\n+                            )\n+                    else:\n+                        constraints[a].add(b)\n+\n+        # Topological sort (Kahn's algorithm) with first-seen order as the\n+        # tie-breaker so that independent items preserve their relative order.\n+        in_degree = {item: 0 for item in all_items}\n+        for item in all_items:\n+            for dep in constraints[item]:\n+                in_degree[dep] += 1\n+\n+        # Map each item to its position in the first-seen order for stable sorting.\n+        position = {item: i for i, item in enumerate(all_items)}\n+        available = sorted(\n+            [item for item in all_items if in_degree[item] == 0],\n+            key=lambda x: position[x],\n+        )\n+        result = []\n+        while available:\n+            item = available.pop(0)\n+            result.append(item)\n+            newly_available = []\n+            for dep in constraints[item]:\n+                in_degree[dep] -= 1\n+                if in_degree[dep] == 0:\n+                    newly_available.append(dep)\n+            # Re-sort to maintain first-seen order among newly available items.\n+            available = sorted(\n+                available + newly_available,\n+                key=lambda x: position[x],\n+            )\n+        return result\n \n     def __add__(self, other):\n         combined = Media()\ndiff --git a/tests/forms_tests/tests/test_media.py b/tests/forms_tests/tests/test_media.py\nindex 8cb484a15e..c8dc41a963 100644\n--- a/tests/forms_tests/tests/test_media.py\n+++ b/tests/forms_tests/tests/test_media.py\n@@ -550,14 +550,31 @@ class FormsMediaTestCase(SimpleTestCase):\n         widget1 = Media(js=['custom_widget.js'])\n         widget2 = Media(js=['jquery.js', 'uses_jquery.js'])\n         form_media = widget1 + widget2\n-        # The relative ordering of custom_widget.js and jquery.js has been\n-        # established (but without a real need to).\n         self.assertEqual(form_media._js, ['custom_widget.js', 'jquery.js', 'uses_jquery.js'])\n         # The inline also uses custom_widget.js. This time, it's at the end.\n         inline_media = Media(js=['jquery.js', 'also_jquery.js']) + Media(js=['custom_widget.js'])\n         merged = form_media + inline_media\n         self.assertEqual(merged._js, ['custom_widget.js', 'jquery.js', 'uses_jquery.js', 'also_jquery.js'])\n \n+    def test_merge_js_three_way_no_false_conflict(self):\n+        \"\"\"\n+        Merging 3 or more media objects doesn't raise unnecessary\n+        MediaOrderConflictWarnings when the relative ordering of independent\n+        files hasn't been specified.\n+        \"\"\"\n+        # text-editor-extras.js depends on text-editor.js; color-picker.js is\n+        # independent of both. No ordering constraint exists between\n+        # color-picker.js and text-editor.js.\n+        widget1 = Media(js=['color-picker.js'])\n+        widget2 = Media(js=['text-editor.js'])\n+        widget3 = Media(js=['text-editor.js', 'text-editor-extras.js', 'color-picker.js'])\n+        import warnings\n+        with warnings.catch_warnings(record=True) as caught:\n+            warnings.simplefilter('always')\n+            merged = (widget1 + widget2 + widget3)._js\n+        self.assertEqual(merged, ['text-editor.js', 'text-editor-extras.js', 'color-picker.js'])\n+        self.assertEqual(caught, [])\n+\n     def test_merge_css_three_way(self):\n         widget1 = Media(css={'screen': ['a.css']})\n         widget2 = Media(css={'screen': ['b.css']})\n"}
{"instance_id": "django__django-11039", "model_name_or_path": "harness/claude-sonnet-4-6", "model_patch": "diff --git a/django/core/management/commands/sqlmigrate.py b/django/core/management/commands/sqlmigrate.py\nindex f0b663222a..35ac1840a7 100644\n--- a/django/core/management/commands/sqlmigrate.py\n+++ b/django/core/management/commands/sqlmigrate.py\n@@ -55,8 +55,9 @@ class Command(BaseCommand):\n                 migration_name, app_label))\n         targets = [(app_label, migration.name)]\n \n-        # Show begin/end around output only for atomic migrations\n-        self.output_transaction = migration.atomic\n+        # Show begin/end around output only if the migration is atomic and\n+        # the database supports transactional DDL.\n+        self.output_transaction = migration.atomic and connection.features.can_rollback_ddl\n \n         # Make a plan that represents just the requested migrations and show SQL\n         # for it\ndiff --git a/tests/migrations/test_commands.py b/tests/migrations/test_commands.py\nindex 0e8fa9fdd1..d41ed6915e 100644\n--- a/tests/migrations/test_commands.py\n+++ b/tests/migrations/test_commands.py\n@@ -616,6 +616,21 @@ class MigrateTests(MigrationTestBase):\n             self.assertNotIn(connection.ops.start_transaction_sql().lower(), queries)\n         self.assertNotIn(connection.ops.end_transaction_sql().lower(), queries)\n \n+    @override_settings(MIGRATION_MODULES={\"migrations\": \"migrations.test_migrations\"})\n+    def test_sqlmigrate_for_non_transactional_database(self):\n+        \"\"\"\n+        Transaction wrappers aren't shown for databases that do not support\n+        transactional DDL.\n+        \"\"\"\n+        out = io.StringIO()\n+        with mock.patch.object(connection.features, 'can_rollback_ddl', False):\n+            call_command(\"sqlmigrate\", \"migrations\", \"0001\", stdout=out)\n+        output = out.getvalue().lower()\n+        queries = [q.strip() for q in output.splitlines()]\n+        if connection.ops.start_transaction_sql():\n+            self.assertNotIn(connection.ops.start_transaction_sql().lower(), queries)\n+        self.assertNotIn(connection.ops.end_transaction_sql().lower(), queries)\n+\n     @override_settings(\n         INSTALLED_APPS=[\n             \"migrations.migrations_test_apps.migrated_app\",\n"}
{"instance_id": "django__django-11049", "model_name_or_path": "harness/claude-sonnet-4-6", "model_patch": "diff --git a/django/conf/locale/en/LC_MESSAGES/django.po b/django/conf/locale/en/LC_MESSAGES/django.po\nindex 2c52175822..e95f84be75 100644\n--- a/django/conf/locale/en/LC_MESSAGES/django.po\n+++ b/django/conf/locale/en/LC_MESSAGES/django.po\n@@ -645,7 +645,7 @@ msgstr \"\"\n #: db/models/fields/__init__.py:1587\n #, python-format\n msgid \"\"\n-\"'%(value)s' value has an invalid format. It must be in [DD] [HH:[MM:]]ss[.\"\n+\"'%(value)s' value has an invalid format. It must be in [DD] [[HH:]MM:]ss[.\"\n \"uuuuuu] format.\"\n msgstr \"\"\n \ndiff --git a/django/db/models/fields/__init__.py b/django/db/models/fields/__init__.py\nindex f32ce64629..71005490f3 100644\n--- a/django/db/models/fields/__init__.py\n+++ b/django/db/models/fields/__init__.py\n@@ -1587,7 +1587,7 @@ class DurationField(Field):\n     empty_strings_allowed = False\n     default_error_messages = {\n         'invalid': _(\"'%(value)s' value has an invalid format. It must be in \"\n-                     \"[DD] [HH:[MM:]]ss[.uuuuuu] format.\")\n+                     \"[DD] [[HH:]MM:]ss[.uuuuuu] format.\")\n     }\n     description = _(\"Duration\")\n \ndiff --git a/tests/model_fields/test_durationfield.py b/tests/model_fields/test_durationfield.py\nindex b73994f86a..f1c82ab8bd 100644\n--- a/tests/model_fields/test_durationfield.py\n+++ b/tests/model_fields/test_durationfield.py\n@@ -75,7 +75,7 @@ class TestValidation(SimpleTestCase):\n         self.assertEqual(\n             cm.exception.message % cm.exception.params,\n             \"'not a datetime' value has an invalid format. \"\n-            \"It must be in [DD] [HH:[MM:]]ss[.uuuuuu] format.\"\n+            \"It must be in [DD] [[HH:]MM:]ss[.uuuuuu] format.\"\n         )\n \n \n"}
{"instance_id": "django__django-11099", "model_name_or_path": "harness/claude-sonnet-4-6", "model_patch": "diff --git a/django/contrib/auth/validators.py b/django/contrib/auth/validators.py\nindex b4878cfd45..1304f20a60 100644\n--- a/django/contrib/auth/validators.py\n+++ b/django/contrib/auth/validators.py\n@@ -7,7 +7,7 @@ from django.utils.translation import gettext_lazy as _\n \n @deconstructible\n class ASCIIUsernameValidator(validators.RegexValidator):\n-    regex = r'^[\\w.@+-]+$'\n+    regex = r'\\A[\\w.@+-]+\\Z'\n     message = _(\n         'Enter a valid username. This value may contain only English letters, '\n         'numbers, and @/./+/-/_ characters.'\n@@ -17,7 +17,7 @@ class ASCIIUsernameValidator(validators.RegexValidator):\n \n @deconstructible\n class UnicodeUsernameValidator(validators.RegexValidator):\n-    regex = r'^[\\w.@+-]+$'\n+    regex = r'\\A[\\w.@+-]+\\Z'\n     message = _(\n         'Enter a valid username. This value may contain only letters, '\n         'numbers, and @/./+/-/_ characters.'\ndiff --git a/tests/auth_tests/test_validators.py b/tests/auth_tests/test_validators.py\nindex 1c2c6b4aff..fcc18ed9e7 100644\n--- a/tests/auth_tests/test_validators.py\n+++ b/tests/auth_tests/test_validators.py\n@@ -237,7 +237,7 @@ class UsernameValidatorsTests(SimpleTestCase):\n         invalid_usernames = [\n             \"o'connell\", \"\u0639\u0628\u062f \u0627\u0644\",\n             \"zerowidth\\u200Bspace\", \"nonbreaking\\u00A0space\",\n-            \"en\\u2013dash\",\n+            \"en\\u2013dash\", \"joe\\n\",\n         ]\n         v = validators.UnicodeUsernameValidator()\n         for valid in valid_usernames:\n@@ -250,7 +250,7 @@ class UsernameValidatorsTests(SimpleTestCase):\n \n     def test_ascii_validator(self):\n         valid_usernames = ['glenn', 'GLEnN', 'jean-marc']\n-        invalid_usernames = [\"o'connell\", '\u00c9ric', 'jean marc', \"\u0623\u062d\u0645\u062f\"]\n+        invalid_usernames = [\"o'connell\", '\u00c9ric', 'jean marc', \"\u0623\u062d\u0645\u062f\", \"glenn\\n\"]\n         v = validators.ASCIIUsernameValidator()\n         for valid in valid_usernames:\n             with self.subTest(valid=valid):\n"}
{"instance_id": "django__django-11133", "model_name_or_path": "harness/claude-sonnet-4-6", "model_patch": "diff --git a/.harness/checkpoints/fdffb4dfd90a/--Users--huetuanthi--dev--agents--harness--django--http--response.py b/.harness/checkpoints/fdffb4dfd90a/--Users--huetuanthi--dev--agents--harness--django--http--response.py\nnew file mode 100644\nindex 0000000000..6a84e193ba\n--- /dev/null\n+++ b/.harness/checkpoints/fdffb4dfd90a/--Users--huetuanthi--dev--agents--harness--django--http--response.py\n@@ -0,0 +1,558 @@\n+import datetime\n+import json\n+import mimetypes\n+import os\n+import re\n+import sys\n+import time\n+from email.header import Header\n+from http.client import responses\n+from urllib.parse import quote, urlparse\n+\n+from django.conf import settings\n+from django.core import signals, signing\n+from django.core.exceptions import DisallowedRedirect\n+from django.core.serializers.json import DjangoJSONEncoder\n+from django.http.cookie import SimpleCookie\n+from django.utils import timezone\n+from django.utils.encoding import iri_to_uri\n+from django.utils.http import http_date\n+\n+_charset_from_content_type_re = re.compile(r';\\s*charset=(?P<charset>[^\\s;]+)', re.I)\n+\n+\n+class BadHeaderError(ValueError):\n+    pass\n+\n+\n+class HttpResponseBase:\n+    \"\"\"\n+    An HTTP response base class with dictionary-accessed headers.\n+\n+    This class doesn't handle content. It should not be used directly.\n+    Use the HttpResponse and StreamingHttpResponse subclasses instead.\n+    \"\"\"\n+\n+    status_code = 200\n+\n+    def __init__(self, content_type=None, status=None, reason=None, charset=None):\n+        # _headers is a mapping of the lowercase name to the original case of\n+        # the header (required for working with legacy systems) and the header\n+        # value. Both the name of the header and its value are ASCII strings.\n+        self._headers = {}\n+        self._closable_objects = []\n+        # This parameter is set by the handler. It's necessary to preserve the\n+        # historical behavior of request_finished.\n+        self._handler_class = None\n+        self.cookies = SimpleCookie()\n+        self.closed = False\n+        if status is not None:\n+            try:\n+                self.status_code = int(status)\n+            except (ValueError, TypeError):\n+                raise TypeError('HTTP status code must be an integer.')\n+\n+            if not 100 <= self.status_code <= 599:\n+                raise ValueError('HTTP status code must be an integer from 100 to 599.')\n+        self._reason_phrase = reason\n+        self._charset = charset\n+        if content_type is None:\n+            content_type = 'text/html; charset=%s' % self.charset\n+        self['Content-Type'] = content_type\n+\n+    @property\n+    def reason_phrase(self):\n+        if self._reason_phrase is not None:\n+            return self._reason_phrase\n+        # Leave self._reason_phrase unset in order to use the default\n+        # reason phrase for status code.\n+        return responses.get(self.status_code, 'Unknown Status Code')\n+\n+    @reason_phrase.setter\n+    def reason_phrase(self, value):\n+        self._reason_phrase = value\n+\n+    @property\n+    def charset(self):\n+        if self._charset is not None:\n+            return self._charset\n+        content_type = self.get('Content-Type', '')\n+        matched = _charset_from_content_type_re.search(content_type)\n+        if matched:\n+            # Extract the charset and strip its double quotes\n+            return matched.group('charset').replace('\"', '')\n+        return settings.DEFAULT_CHARSET\n+\n+    @charset.setter\n+    def charset(self, value):\n+        self._charset = value\n+\n+    def serialize_headers(self):\n+        \"\"\"HTTP headers as a bytestring.\"\"\"\n+        def to_bytes(val, encoding):\n+            return val if isinstance(val, bytes) else val.encode(encoding)\n+\n+        headers = [\n+            (to_bytes(key, 'ascii') + b': ' + to_bytes(value, 'latin-1'))\n+            for key, value in self._headers.values()\n+        ]\n+        return b'\\r\\n'.join(headers)\n+\n+    __bytes__ = serialize_headers\n+\n+    @property\n+    def _content_type_for_repr(self):\n+        return ', \"%s\"' % self['Content-Type'] if 'Content-Type' in self else ''\n+\n+    def _convert_to_charset(self, value, charset, mime_encode=False):\n+        \"\"\"\n+        Convert headers key/value to ascii/latin-1 native strings.\n+\n+        `charset` must be 'ascii' or 'latin-1'. If `mime_encode` is True and\n+        `value` can't be represented in the given charset, apply MIME-encoding.\n+        \"\"\"\n+        if not isinstance(value, (bytes, str)):\n+            value = str(value)\n+        if ((isinstance(value, bytes) and (b'\\n' in value or b'\\r' in value)) or\n+                isinstance(value, str) and ('\\n' in value or '\\r' in value)):\n+            raise BadHeaderError(\"Header values can't contain newlines (got %r)\" % value)\n+        try:\n+            if isinstance(value, str):\n+                # Ensure string is valid in given charset\n+                value.encode(charset)\n+            else:\n+                # Convert bytestring using given charset\n+                value = value.decode(charset)\n+        except UnicodeError as e:\n+            if mime_encode:\n+                value = Header(value, 'utf-8', maxlinelen=sys.maxsize).encode()\n+            else:\n+                e.reason += ', HTTP response headers must be in %s format' % charset\n+                raise\n+        return value\n+\n+    def __setitem__(self, header, value):\n+        header = self._convert_to_charset(header, 'ascii')\n+        value = self._convert_to_charset(value, 'latin-1', mime_encode=True)\n+        self._headers[header.lower()] = (header, value)\n+\n+    def __delitem__(self, header):\n+        self._headers.pop(header.lower(), False)\n+\n+    def __getitem__(self, header):\n+        return self._headers[header.lower()][1]\n+\n+    def has_header(self, header):\n+        \"\"\"Case-insensitive check for a header.\"\"\"\n+        return header.lower() in self._headers\n+\n+    __contains__ = has_header\n+\n+    def items(self):\n+        return self._headers.values()\n+\n+    def get(self, header, alternate=None):\n+        return self._headers.get(header.lower(), (None, alternate))[1]\n+\n+    def set_cookie(self, key, value='', max_age=None, expires=None, path='/',\n+                   domain=None, secure=False, httponly=False, samesite=None):\n+        \"\"\"\n+        Set a cookie.\n+\n+        ``expires`` can be:\n+        - a string in the correct format,\n+        - a naive ``datetime.datetime`` object in UTC,\n+        - an aware ``datetime.datetime`` object in any time zone.\n+        If it is a ``datetime.datetime`` object then calculate ``max_age``.\n+        \"\"\"\n+        self.cookies[key] = value\n+        if expires is not None:\n+            if isinstance(expires, datetime.datetime):\n+                if timezone.is_aware(expires):\n+                    expires = timezone.make_naive(expires, timezone.utc)\n+                delta = expires - expires.utcnow()\n+                # Add one second so the date matches exactly (a fraction of\n+                # time gets lost between converting to a timedelta and\n+                # then the date string).\n+                delta = delta + datetime.timedelta(seconds=1)\n+                # Just set max_age - the max_age logic will set expires.\n+                expires = None\n+                max_age = max(0, delta.days * 86400 + delta.seconds)\n+            else:\n+                self.cookies[key]['expires'] = expires\n+        else:\n+            self.cookies[key]['expires'] = ''\n+        if max_age is not None:\n+            self.cookies[key]['max-age'] = max_age\n+            # IE requires expires, so set it if hasn't been already.\n+            if not expires:\n+                self.cookies[key]['expires'] = http_date(time.time() + max_age)\n+        if path is not None:\n+            self.cookies[key]['path'] = path\n+        if domain is not None:\n+            self.cookies[key]['domain'] = domain\n+        if secure:\n+            self.cookies[key]['secure'] = True\n+        if httponly:\n+            self.cookies[key]['httponly'] = True\n+        if samesite:\n+            if samesite.lower() not in ('lax', 'strict'):\n+                raise ValueError('samesite must be \"lax\" or \"strict\".')\n+            self.cookies[key]['samesite'] = samesite\n+\n+    def setdefault(self, key, value):\n+        \"\"\"Set a header unless it has already been set.\"\"\"\n+        if key not in self:\n+            self[key] = value\n+\n+    def set_signed_cookie(self, key, value, salt='', **kwargs):\n+        value = signing.get_cookie_signer(salt=key + salt).sign(value)\n+        return self.set_cookie(key, value, **kwargs)\n+\n+    def delete_cookie(self, key, path='/', domain=None):\n+        # Most browsers ignore the Set-Cookie header if the cookie name starts\n+        # with __Host- or __Secure- and the cookie doesn't use the secure flag.\n+        secure = key.startswith(('__Secure-', '__Host-'))\n+        self.set_cookie(\n+            key, max_age=0, path=path, domain=domain, secure=secure,\n+            expires='Thu, 01 Jan 1970 00:00:00 GMT',\n+        )\n+\n+    # Common methods used by subclasses\n+\n+    def make_bytes(self, value):\n+        \"\"\"Turn a value into a bytestring encoded in the output charset.\"\"\"\n+        # Per PEP 3333, this response body must be bytes. To avoid returning\n+        # an instance of a subclass, this function returns `bytes(value)`.\n+        # This doesn't make a copy when `value` already contains bytes.\n+\n+        # Handle string types -- we can't rely on force_bytes here because:\n+        # - Python attempts str conversion first\n+        # - when self._charset != 'utf-8' it re-encodes the content\n+        if isinstance(value, bytes):\n+            return bytes(value)\n+        if isinstance(value, str):\n+            return bytes(value.encode(self.charset))\n+        # Handle non-string types.\n+        return str(value).encode(self.charset)\n+\n+    # These methods partially implement the file-like object interface.\n+    # See https://docs.python.org/library/io.html#io.IOBase\n+\n+    # The WSGI server must call this method upon completion of the request.\n+    # See http://blog.dscpl.com.au/2012/10/obligations-for-calling-close-on.html\n+    def close(self):\n+        for closable in self._closable_objects:\n+            try:\n+                closable.close()\n+            except Exception:\n+                pass\n+        self.closed = True\n+        signals.request_finished.send(sender=self._handler_class)\n+\n+    def write(self, content):\n+        raise OSError('This %s instance is not writable' % self.__class__.__name__)\n+\n+    def flush(self):\n+        pass\n+\n+    def tell(self):\n+        raise OSError('This %s instance cannot tell its position' % self.__class__.__name__)\n+\n+    # These methods partially implement a stream-like object interface.\n+    # See https://docs.python.org/library/io.html#io.IOBase\n+\n+    def readable(self):\n+        return False\n+\n+    def seekable(self):\n+        return False\n+\n+    def writable(self):\n+        return False\n+\n+    def writelines(self, lines):\n+        raise OSError('This %s instance is not writable' % self.__class__.__name__)\n+\n+\n+class HttpResponse(HttpResponseBase):\n+    \"\"\"\n+    An HTTP response class with a string as content.\n+\n+    This content that can be read, appended to, or replaced.\n+    \"\"\"\n+\n+    streaming = False\n+\n+    def __init__(self, content=b'', *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        # Content is a bytestring. See the `content` property methods.\n+        self.content = content\n+\n+    def __repr__(self):\n+        return '<%(cls)s status_code=%(status_code)d%(content_type)s>' % {\n+            'cls': self.__class__.__name__,\n+            'status_code': self.status_code,\n+            'content_type': self._content_type_for_repr,\n+        }\n+\n+    def serialize(self):\n+        \"\"\"Full HTTP message, including headers, as a bytestring.\"\"\"\n+        return self.serialize_headers() + b'\\r\\n\\r\\n' + self.content\n+\n+    __bytes__ = serialize\n+\n+    @property\n+    def content(self):\n+        return b''.join(self._container)\n+\n+    @content.setter\n+    def content(self, value):\n+        # Consume iterators upon assignment to allow repeated iteration.\n+        if hasattr(value, '__iter__') and not isinstance(value, (bytes, str)):\n+            content = b''.join(self.make_bytes(chunk) for chunk in value)\n+            if hasattr(value, 'close'):\n+                try:\n+                    value.close()\n+                except Exception:\n+                    pass\n+        else:\n+            content = self.make_bytes(value)\n+        # Create a list of properly encoded bytestrings to support write().\n+        self._container = [content]\n+\n+    def __iter__(self):\n+        return iter(self._container)\n+\n+    def write(self, content):\n+        self._container.append(self.make_bytes(content))\n+\n+    def tell(self):\n+        return len(self.content)\n+\n+    def getvalue(self):\n+        return self.content\n+\n+    def writable(self):\n+        return True\n+\n+    def writelines(self, lines):\n+        for line in lines:\n+            self.write(line)\n+\n+\n+class StreamingHttpResponse(HttpResponseBase):\n+    \"\"\"\n+    A streaming HTTP response class with an iterator as content.\n+\n+    This should only be iterated once, when the response is streamed to the\n+    client. However, it can be appended to or replaced with a new iterator\n+    that wraps the original content (or yields entirely new content).\n+    \"\"\"\n+\n+    streaming = True\n+\n+    def __init__(self, streaming_content=(), *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        # `streaming_content` should be an iterable of bytestrings.\n+        # See the `streaming_content` property methods.\n+        self.streaming_content = streaming_content\n+\n+    @property\n+    def content(self):\n+        raise AttributeError(\n+            \"This %s instance has no `content` attribute. Use \"\n+            \"`streaming_content` instead.\" % self.__class__.__name__\n+        )\n+\n+    @property\n+    def streaming_content(self):\n+        return map(self.make_bytes, self._iterator)\n+\n+    @streaming_content.setter\n+    def streaming_content(self, value):\n+        self._set_streaming_content(value)\n+\n+    def _set_streaming_content(self, value):\n+        # Ensure we can never iterate on \"value\" more than once.\n+        self._iterator = iter(value)\n+        if hasattr(value, 'close'):\n+            self._closable_objects.append(value)\n+\n+    def __iter__(self):\n+        return self.streaming_content\n+\n+    def getvalue(self):\n+        return b''.join(self.streaming_content)\n+\n+\n+class FileResponse(StreamingHttpResponse):\n+    \"\"\"\n+    A streaming HTTP response class optimized for files.\n+    \"\"\"\n+    block_size = 4096\n+\n+    def __init__(self, *args, as_attachment=False, filename='', **kwargs):\n+        self.as_attachment = as_attachment\n+        self.filename = filename\n+        super().__init__(*args, **kwargs)\n+\n+    def _set_streaming_content(self, value):\n+        if not hasattr(value, 'read'):\n+            self.file_to_stream = None\n+            return super()._set_streaming_content(value)\n+\n+        self.file_to_stream = filelike = value\n+        if hasattr(filelike, 'close'):\n+            self._closable_objects.append(filelike)\n+        value = iter(lambda: filelike.read(self.block_size), b'')\n+        self.set_headers(filelike)\n+        super()._set_streaming_content(value)\n+\n+    def set_headers(self, filelike):\n+        \"\"\"\n+        Set some common response headers (Content-Length, Content-Type, and\n+        Content-Disposition) based on the `filelike` response content.\n+        \"\"\"\n+        encoding_map = {\n+            'bzip2': 'application/x-bzip',\n+            'gzip': 'application/gzip',\n+            'xz': 'application/x-xz',\n+        }\n+        filename = getattr(filelike, 'name', None)\n+        filename = filename if (isinstance(filename, str) and filename) else self.filename\n+        if os.path.isabs(filename):\n+            self['Content-Length'] = os.path.getsize(filelike.name)\n+        elif hasattr(filelike, 'getbuffer'):\n+            self['Content-Length'] = filelike.getbuffer().nbytes\n+\n+        if self.get('Content-Type', '').startswith('text/html'):\n+            if filename:\n+                content_type, encoding = mimetypes.guess_type(filename)\n+                # Encoding isn't set to prevent browsers from automatically\n+                # uncompressing files.\n+                content_type = encoding_map.get(encoding, content_type)\n+                self['Content-Type'] = content_type or 'application/octet-stream'\n+            else:\n+                self['Content-Type'] = 'application/octet-stream'\n+\n+        if self.as_attachment:\n+            filename = self.filename or os.path.basename(filename)\n+            if filename:\n+                try:\n+                    filename.encode('ascii')\n+                    file_expr = 'filename=\"{}\"'.format(filename)\n+                except UnicodeEncodeError:\n+                    file_expr = \"filename*=utf-8''{}\".format(quote(filename))\n+                self['Content-Disposition'] = 'attachment; {}'.format(file_expr)\n+\n+\n+class HttpResponseRedirectBase(HttpResponse):\n+    allowed_schemes = ['http', 'https', 'ftp']\n+\n+    def __init__(self, redirect_to, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self['Location'] = iri_to_uri(redirect_to)\n+        parsed = urlparse(str(redirect_to))\n+        if parsed.scheme and parsed.scheme not in self.allowed_schemes:\n+            raise DisallowedRedirect(\"Unsafe redirect to URL with protocol '%s'\" % parsed.scheme)\n+\n+    url = property(lambda self: self['Location'])\n+\n+    def __repr__(self):\n+        return '<%(cls)s status_code=%(status_code)d%(content_type)s, url=\"%(url)s\">' % {\n+            'cls': self.__class__.__name__,\n+            'status_code': self.status_code,\n+            'content_type': self._content_type_for_repr,\n+            'url': self.url,\n+        }\n+\n+\n+class HttpResponseRedirect(HttpResponseRedirectBase):\n+    status_code = 302\n+\n+\n+class HttpResponsePermanentRedirect(HttpResponseRedirectBase):\n+    status_code = 301\n+\n+\n+class HttpResponseNotModified(HttpResponse):\n+    status_code = 304\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        del self['content-type']\n+\n+    @HttpResponse.content.setter\n+    def content(self, value):\n+        if value:\n+            raise AttributeError(\"You cannot set content to a 304 (Not Modified) response\")\n+        self._container = []\n+\n+\n+class HttpResponseBadRequest(HttpResponse):\n+    status_code = 400\n+\n+\n+class HttpResponseNotFound(HttpResponse):\n+    status_code = 404\n+\n+\n+class HttpResponseForbidden(HttpResponse):\n+    status_code = 403\n+\n+\n+class HttpResponseNotAllowed(HttpResponse):\n+    status_code = 405\n+\n+    def __init__(self, permitted_methods, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self['Allow'] = ', '.join(permitted_methods)\n+\n+    def __repr__(self):\n+        return '<%(cls)s [%(methods)s] status_code=%(status_code)d%(content_type)s>' % {\n+            'cls': self.__class__.__name__,\n+            'status_code': self.status_code,\n+            'content_type': self._content_type_for_repr,\n+            'methods': self['Allow'],\n+        }\n+\n+\n+class HttpResponseGone(HttpResponse):\n+    status_code = 410\n+\n+\n+class HttpResponseServerError(HttpResponse):\n+    status_code = 500\n+\n+\n+class Http404(Exception):\n+    pass\n+\n+\n+class JsonResponse(HttpResponse):\n+    \"\"\"\n+    An HTTP response class that consumes data to be serialized to JSON.\n+\n+    :param data: Data to be dumped into json. By default only ``dict`` objects\n+      are allowed to be passed due to a security flaw before EcmaScript 5. See\n+      the ``safe`` parameter for more information.\n+    :param encoder: Should be a json encoder class. Defaults to\n+      ``django.core.serializers.json.DjangoJSONEncoder``.\n+    :param safe: Controls if only ``dict`` objects may be serialized. Defaults\n+      to ``True``.\n+    :param json_dumps_params: A dictionary of kwargs passed to json.dumps().\n+    \"\"\"\n+\n+    def __init__(self, data, encoder=DjangoJSONEncoder, safe=True,\n+                 json_dumps_params=None, **kwargs):\n+        if safe and not isinstance(data, dict):\n+            raise TypeError(\n+                'In order to allow non-dict objects to be serialized set the '\n+                'safe parameter to False.'\n+            )\n+        if json_dumps_params is None:\n+            json_dumps_params = {}\n+        kwargs.setdefault('content_type', 'application/json')\n+        data = json.dumps(data, cls=encoder, **json_dumps_params)\n+        super().__init__(content=data, **kwargs)\ndiff --git a/django/http/response.py b/django/http/response.py\nindex 6a84e193ba..27c1d89d84 100644\n--- a/django/http/response.py\n+++ b/django/http/response.py\n@@ -231,6 +231,8 @@ class HttpResponseBase:\n         # - when self._charset != 'utf-8' it re-encodes the content\n         if isinstance(value, bytes):\n             return bytes(value)\n+        if isinstance(value, memoryview):\n+            return bytes(value)\n         if isinstance(value, str):\n             return bytes(value.encode(self.charset))\n         # Handle non-string types.\n@@ -309,7 +311,7 @@ class HttpResponse(HttpResponseBase):\n     @content.setter\n     def content(self, value):\n         # Consume iterators upon assignment to allow repeated iteration.\n-        if hasattr(value, '__iter__') and not isinstance(value, (bytes, str)):\n+        if hasattr(value, '__iter__') and not isinstance(value, (bytes, memoryview, str)):\n             content = b''.join(self.make_bytes(chunk) for chunk in value)\n             if hasattr(value, 'close'):\n                 try:\ndiff --git a/tests/httpwrappers/tests.py b/tests/httpwrappers/tests.py\nindex e6c6297898..3bb52c9217 100644\n--- a/tests/httpwrappers/tests.py\n+++ b/tests/httpwrappers/tests.py\n@@ -366,6 +366,15 @@ class HttpResponseTests(unittest.TestCase):\n         r.content = 12345\n         self.assertEqual(r.content, b'12345')\n \n+    def test_memoryview_content(self):\n+        r = HttpResponse(memoryview(b'My Content'))\n+        self.assertEqual(r.content, b'My Content')\n+\n+        # Test via the content property setter.\n+        r = HttpResponse()\n+        r.content = memoryview(b'My Content')\n+        self.assertEqual(r.content, b'My Content')\n+\n     def test_iter_content(self):\n         r = HttpResponse(['abc', 'def', 'ghi'])\n         self.assertEqual(r.content, b'abcdefghi')\n"}
{"instance_id": "django__django-11179", "model_name_or_path": "harness/claude-sonnet-4-6", "model_patch": "diff --git a/.harness/checkpoints/5ebd021540c3/--Users--huetuanthi--dev--agents--harness--django--db--models--deletion.py b/.harness/checkpoints/5ebd021540c3/--Users--huetuanthi--dev--agents--harness--django--db--models--deletion.py\nnew file mode 100644\nindex 0000000000..92fe4affb9\n--- /dev/null\n+++ b/.harness/checkpoints/5ebd021540c3/--Users--huetuanthi--dev--agents--harness--django--db--models--deletion.py\n@@ -0,0 +1,327 @@\n+from collections import Counter\n+from operator import attrgetter\n+\n+from django.db import IntegrityError, connections, transaction\n+from django.db.models import signals, sql\n+\n+\n+class ProtectedError(IntegrityError):\n+    def __init__(self, msg, protected_objects):\n+        self.protected_objects = protected_objects\n+        super().__init__(msg, protected_objects)\n+\n+\n+def CASCADE(collector, field, sub_objs, using):\n+    collector.collect(sub_objs, source=field.remote_field.model,\n+                      source_attr=field.name, nullable=field.null)\n+    if field.null and not connections[using].features.can_defer_constraint_checks:\n+        collector.add_field_update(field, None, sub_objs)\n+\n+\n+def PROTECT(collector, field, sub_objs, using):\n+    raise ProtectedError(\n+        \"Cannot delete some instances of model '%s' because they are \"\n+        \"referenced through a protected foreign key: '%s.%s'\" % (\n+            field.remote_field.model.__name__, sub_objs[0].__class__.__name__, field.name\n+        ),\n+        sub_objs\n+    )\n+\n+\n+def SET(value):\n+    if callable(value):\n+        def set_on_delete(collector, field, sub_objs, using):\n+            collector.add_field_update(field, value(), sub_objs)\n+    else:\n+        def set_on_delete(collector, field, sub_objs, using):\n+            collector.add_field_update(field, value, sub_objs)\n+    set_on_delete.deconstruct = lambda: ('django.db.models.SET', (value,), {})\n+    return set_on_delete\n+\n+\n+def SET_NULL(collector, field, sub_objs, using):\n+    collector.add_field_update(field, None, sub_objs)\n+\n+\n+def SET_DEFAULT(collector, field, sub_objs, using):\n+    collector.add_field_update(field, field.get_default(), sub_objs)\n+\n+\n+def DO_NOTHING(collector, field, sub_objs, using):\n+    pass\n+\n+\n+def get_candidate_relations_to_delete(opts):\n+    # The candidate relations are the ones that come from N-1 and 1-1 relations.\n+    # N-N  (i.e., many-to-many) relations aren't candidates for deletion.\n+    return (\n+        f for f in opts.get_fields(include_hidden=True)\n+        if f.auto_created and not f.concrete and (f.one_to_one or f.one_to_many)\n+    )\n+\n+\n+class Collector:\n+    def __init__(self, using):\n+        self.using = using\n+        # Initially, {model: {instances}}, later values become lists.\n+        self.data = {}\n+        self.field_updates = {}  # {model: {(field, value): {instances}}}\n+        # fast_deletes is a list of queryset-likes that can be deleted without\n+        # fetching the objects into memory.\n+        self.fast_deletes = []\n+\n+        # Tracks deletion-order dependency for databases without transactions\n+        # or ability to defer constraint checks. Only concrete model classes\n+        # should be included, as the dependencies exist only between actual\n+        # database tables; proxy models are represented here by their concrete\n+        # parent.\n+        self.dependencies = {}  # {model: {models}}\n+\n+    def add(self, objs, source=None, nullable=False, reverse_dependency=False):\n+        \"\"\"\n+        Add 'objs' to the collection of objects to be deleted.  If the call is\n+        the result of a cascade, 'source' should be the model that caused it,\n+        and 'nullable' should be set to True if the relation can be null.\n+\n+        Return a list of all objects that were not already collected.\n+        \"\"\"\n+        if not objs:\n+            return []\n+        new_objs = []\n+        model = objs[0].__class__\n+        instances = self.data.setdefault(model, set())\n+        for obj in objs:\n+            if obj not in instances:\n+                new_objs.append(obj)\n+        instances.update(new_objs)\n+        # Nullable relationships can be ignored -- they are nulled out before\n+        # deleting, and therefore do not affect the order in which objects have\n+        # to be deleted.\n+        if source is not None and not nullable:\n+            if reverse_dependency:\n+                source, model = model, source\n+            self.dependencies.setdefault(\n+                source._meta.concrete_model, set()).add(model._meta.concrete_model)\n+        return new_objs\n+\n+    def add_field_update(self, field, value, objs):\n+        \"\"\"\n+        Schedule a field update. 'objs' must be a homogeneous iterable\n+        collection of model instances (e.g. a QuerySet).\n+        \"\"\"\n+        if not objs:\n+            return\n+        model = objs[0].__class__\n+        self.field_updates.setdefault(\n+            model, {}).setdefault(\n+            (field, value), set()).update(objs)\n+\n+    def can_fast_delete(self, objs, from_field=None):\n+        \"\"\"\n+        Determine if the objects in the given queryset-like or single object\n+        can be fast-deleted. This can be done if there are no cascades, no\n+        parents and no signal listeners for the object class.\n+\n+        The 'from_field' tells where we are coming from - we need this to\n+        determine if the objects are in fact to be deleted. Allow also\n+        skipping parent -> child -> parent chain preventing fast delete of\n+        the child.\n+        \"\"\"\n+        if from_field and from_field.remote_field.on_delete is not CASCADE:\n+            return False\n+        if hasattr(objs, '_meta'):\n+            model = type(objs)\n+        elif hasattr(objs, 'model') and hasattr(objs, '_raw_delete'):\n+            model = objs.model\n+        else:\n+            return False\n+        if (signals.pre_delete.has_listeners(model) or\n+                signals.post_delete.has_listeners(model) or\n+                signals.m2m_changed.has_listeners(model)):\n+            return False\n+        # The use of from_field comes from the need to avoid cascade back to\n+        # parent when parent delete is cascading to child.\n+        opts = model._meta\n+        return (\n+            all(link == from_field for link in opts.concrete_model._meta.parents.values()) and\n+            # Foreign keys pointing to this model.\n+            all(\n+                related.field.remote_field.on_delete is DO_NOTHING\n+                for related in get_candidate_relations_to_delete(opts)\n+            ) and (\n+                # Something like generic foreign key.\n+                not any(hasattr(field, 'bulk_related_objects') for field in opts.private_fields)\n+            )\n+        )\n+\n+    def get_del_batches(self, objs, field):\n+        \"\"\"\n+        Return the objs in suitably sized batches for the used connection.\n+        \"\"\"\n+        conn_batch_size = max(\n+            connections[self.using].ops.bulk_batch_size([field.name], objs), 1)\n+        if len(objs) > conn_batch_size:\n+            return [objs[i:i + conn_batch_size]\n+                    for i in range(0, len(objs), conn_batch_size)]\n+        else:\n+            return [objs]\n+\n+    def collect(self, objs, source=None, nullable=False, collect_related=True,\n+                source_attr=None, reverse_dependency=False, keep_parents=False):\n+        \"\"\"\n+        Add 'objs' to the collection of objects to be deleted as well as all\n+        parent instances.  'objs' must be a homogeneous iterable collection of\n+        model instances (e.g. a QuerySet).  If 'collect_related' is True,\n+        related objects will be handled by their respective on_delete handler.\n+\n+        If the call is the result of a cascade, 'source' should be the model\n+        that caused it and 'nullable' should be set to True, if the relation\n+        can be null.\n+\n+        If 'reverse_dependency' is True, 'source' will be deleted before the\n+        current model, rather than after. (Needed for cascading to parent\n+        models, the one case in which the cascade follows the forwards\n+        direction of an FK rather than the reverse direction.)\n+\n+        If 'keep_parents' is True, data of parent model's will be not deleted.\n+        \"\"\"\n+        if self.can_fast_delete(objs):\n+            self.fast_deletes.append(objs)\n+            return\n+        new_objs = self.add(objs, source, nullable,\n+                            reverse_dependency=reverse_dependency)\n+        if not new_objs:\n+            return\n+\n+        model = new_objs[0].__class__\n+\n+        if not keep_parents:\n+            # Recursively collect concrete model's parent models, but not their\n+            # related objects. These will be found by meta.get_fields()\n+            concrete_model = model._meta.concrete_model\n+            for ptr in concrete_model._meta.parents.values():\n+                if ptr:\n+                    parent_objs = [getattr(obj, ptr.name) for obj in new_objs]\n+                    self.collect(parent_objs, source=model,\n+                                 source_attr=ptr.remote_field.related_name,\n+                                 collect_related=False,\n+                                 reverse_dependency=True)\n+        if collect_related:\n+            parents = model._meta.parents\n+            for related in get_candidate_relations_to_delete(model._meta):\n+                # Preserve parent reverse relationships if keep_parents=True.\n+                if keep_parents and related.model in parents:\n+                    continue\n+                field = related.field\n+                if field.remote_field.on_delete == DO_NOTHING:\n+                    continue\n+                batches = self.get_del_batches(new_objs, field)\n+                for batch in batches:\n+                    sub_objs = self.related_objects(related, batch)\n+                    if self.can_fast_delete(sub_objs, from_field=field):\n+                        self.fast_deletes.append(sub_objs)\n+                    elif sub_objs:\n+                        field.remote_field.on_delete(self, field, sub_objs, self.using)\n+            for field in model._meta.private_fields:\n+                if hasattr(field, 'bulk_related_objects'):\n+                    # It's something like generic foreign key.\n+                    sub_objs = field.bulk_related_objects(new_objs, self.using)\n+                    self.collect(sub_objs, source=model, nullable=True)\n+\n+    def related_objects(self, related, objs):\n+        \"\"\"\n+        Get a QuerySet of objects related to `objs` via the relation `related`.\n+        \"\"\"\n+        return related.related_model._base_manager.using(self.using).filter(\n+            **{\"%s__in\" % related.field.name: objs}\n+        )\n+\n+    def instances_with_model(self):\n+        for model, instances in self.data.items():\n+            for obj in instances:\n+                yield model, obj\n+\n+    def sort(self):\n+        sorted_models = []\n+        concrete_models = set()\n+        models = list(self.data)\n+        while len(sorted_models) < len(models):\n+            found = False\n+            for model in models:\n+                if model in sorted_models:\n+                    continue\n+                dependencies = self.dependencies.get(model._meta.concrete_model)\n+                if not (dependencies and dependencies.difference(concrete_models)):\n+                    sorted_models.append(model)\n+                    concrete_models.add(model._meta.concrete_model)\n+                    found = True\n+            if not found:\n+                return\n+        self.data = {model: self.data[model] for model in sorted_models}\n+\n+    def delete(self):\n+        # sort instance collections\n+        for model, instances in self.data.items():\n+            self.data[model] = sorted(instances, key=attrgetter(\"pk\"))\n+\n+        # if possible, bring the models in an order suitable for databases that\n+        # don't support transactions or cannot defer constraint checks until the\n+        # end of a transaction.\n+        self.sort()\n+        # number of objects deleted for each model label\n+        deleted_counter = Counter()\n+\n+        # Optimize for the case with a single obj and no dependencies\n+        if len(self.data) == 1 and len(instances) == 1:\n+            instance = list(instances)[0]\n+            if self.can_fast_delete(instance):\n+                with transaction.mark_for_rollback_on_error():\n+                    count = sql.DeleteQuery(model).delete_batch([instance.pk], self.using)\n+                return count, {model._meta.label: count}\n+\n+        with transaction.atomic(using=self.using, savepoint=False):\n+            # send pre_delete signals\n+            for model, obj in self.instances_with_model():\n+                if not model._meta.auto_created:\n+                    signals.pre_delete.send(\n+                        sender=model, instance=obj, using=self.using\n+                    )\n+\n+            # fast deletes\n+            for qs in self.fast_deletes:\n+                count = qs._raw_delete(using=self.using)\n+                deleted_counter[qs.model._meta.label] += count\n+\n+            # update fields\n+            for model, instances_for_fieldvalues in self.field_updates.items():\n+                for (field, value), instances in instances_for_fieldvalues.items():\n+                    query = sql.UpdateQuery(model)\n+                    query.update_batch([obj.pk for obj in instances],\n+                                       {field.name: value}, self.using)\n+\n+            # reverse instance collections\n+            for instances in self.data.values():\n+                instances.reverse()\n+\n+            # delete instances\n+            for model, instances in self.data.items():\n+                query = sql.DeleteQuery(model)\n+                pk_list = [obj.pk for obj in instances]\n+                count = query.delete_batch(pk_list, self.using)\n+                deleted_counter[model._meta.label] += count\n+\n+                if not model._meta.auto_created:\n+                    for obj in instances:\n+                        signals.post_delete.send(\n+                            sender=model, instance=obj, using=self.using\n+                        )\n+\n+        # update collected instances\n+        for instances_for_fieldvalues in self.field_updates.values():\n+            for (field, value), instances in instances_for_fieldvalues.items():\n+                for obj in instances:\n+                    setattr(obj, field.attname, value)\n+        for model, instances in self.data.items():\n+            for instance in instances:\n+                setattr(instance, model._meta.pk.attname, None)\n+        return sum(deleted_counter.values()), dict(deleted_counter)\ndiff --git a/django/db/models/deletion.py b/django/db/models/deletion.py\nindex 92fe4affb9..3a462f1c4c 100644\n--- a/django/db/models/deletion.py\n+++ b/django/db/models/deletion.py\n@@ -277,6 +277,7 @@ class Collector:\n             if self.can_fast_delete(instance):\n                 with transaction.mark_for_rollback_on_error():\n                     count = sql.DeleteQuery(model).delete_batch([instance.pk], self.using)\n+                setattr(instance, model._meta.pk.attname, None)\n                 return count, {model._meta.label: count}\n \n         with transaction.atomic(using=self.using, savepoint=False):\ndiff --git a/tests/delete/tests.py b/tests/delete/tests.py\nindex 55eeb226ea..03a998d473 100644\n--- a/tests/delete/tests.py\n+++ b/tests/delete/tests.py\n@@ -522,3 +522,14 @@ class FastDeleteTests(TestCase):\n                 User.objects.filter(avatar__desc='missing').delete(),\n                 (0, {'delete.User': 0})\n             )\n+\n+    def test_fast_delete_instance_set_pk_none(self):\n+        \"\"\"\n+        fast_delete path for a single object with no dependencies must set\n+        the instance's PK to None after deletion, consistent with the\n+        regular delete() path.\n+        \"\"\"\n+        u = User.objects.create()\n+        # Verify this goes through the fast-delete path (single query).\n+        self.assertNumQueries(1, u.delete)\n+        self.assertIsNone(u.pk)\n"}
{"instance_id": "django__django-11283", "model_name_or_path": "harness/claude-sonnet-4-6", "model_patch": "diff --git a/.harness/checkpoints/73e4876daa04/--Users--huetuanthi--dev--agents--harness--django--contrib--auth--migrations--0011_update_proxy_permissions.py b/.harness/checkpoints/73e4876daa04/--Users--huetuanthi--dev--agents--harness--django--contrib--auth--migrations--0011_update_proxy_permissions.py\nnew file mode 100644\nindex 0000000000..0e66649695\n--- /dev/null\n+++ b/.harness/checkpoints/73e4876daa04/--Users--huetuanthi--dev--agents--harness--django--contrib--auth--migrations--0011_update_proxy_permissions.py\n@@ -0,0 +1,48 @@\n+from django.db import migrations\n+from django.db.models import Q\n+\n+\n+def update_proxy_model_permissions(apps, schema_editor, reverse=False):\n+    \"\"\"\n+    Update the content_type of proxy model permissions to use the ContentType\n+    of the proxy model.\n+    \"\"\"\n+    Permission = apps.get_model('auth', 'Permission')\n+    ContentType = apps.get_model('contenttypes', 'ContentType')\n+    for Model in apps.get_models():\n+        opts = Model._meta\n+        if not opts.proxy:\n+            continue\n+        proxy_default_permissions_codenames = [\n+            '%s_%s' % (action, opts.model_name)\n+            for action in opts.default_permissions\n+        ]\n+        permissions_query = Q(codename__in=proxy_default_permissions_codenames)\n+        for codename, name in opts.permissions:\n+            permissions_query = permissions_query | Q(codename=codename, name=name)\n+        concrete_content_type = ContentType.objects.get_for_model(Model, for_concrete_model=True)\n+        proxy_content_type = ContentType.objects.get_for_model(Model, for_concrete_model=False)\n+        old_content_type = proxy_content_type if reverse else concrete_content_type\n+        new_content_type = concrete_content_type if reverse else proxy_content_type\n+        Permission.objects.filter(\n+            permissions_query,\n+            content_type=old_content_type,\n+        ).update(content_type=new_content_type)\n+\n+\n+def revert_proxy_model_permissions(apps, schema_editor):\n+    \"\"\"\n+    Update the content_type of proxy model permissions to use the ContentType\n+    of the concrete model.\n+    \"\"\"\n+    update_proxy_model_permissions(apps, schema_editor, reverse=True)\n+\n+\n+class Migration(migrations.Migration):\n+    dependencies = [\n+        ('auth', '0010_alter_group_name_max_length'),\n+        ('contenttypes', '0002_remove_content_type_name'),\n+    ]\n+    operations = [\n+        migrations.RunPython(update_proxy_model_permissions, revert_proxy_model_permissions),\n+    ]\ndiff --git a/django/contrib/auth/migrations/0011_update_proxy_permissions.py b/django/contrib/auth/migrations/0011_update_proxy_permissions.py\nindex 0e66649695..9ff5811e12 100644\n--- a/django/contrib/auth/migrations/0011_update_proxy_permissions.py\n+++ b/django/contrib/auth/migrations/0011_update_proxy_permissions.py\n@@ -24,9 +24,22 @@ def update_proxy_model_permissions(apps, schema_editor, reverse=False):\n         proxy_content_type = ContentType.objects.get_for_model(Model, for_concrete_model=False)\n         old_content_type = proxy_content_type if reverse else concrete_content_type\n         new_content_type = concrete_content_type if reverse else proxy_content_type\n+        # Avoid updating permissions that already exist under the target\n+        # content type \u2014 this can happen when a model was originally concrete\n+        # and later re-created as a proxy. In that case the target content type\n+        # already has those permissions, so updating would violate the unique\n+        # constraint on (content_type_id, codename).\n+        existing_target_codenames = set(\n+            Permission.objects.filter(\n+                permissions_query,\n+                content_type=new_content_type,\n+            ).values_list('codename', flat=True)\n+        )\n         Permission.objects.filter(\n             permissions_query,\n             content_type=old_content_type,\n+        ).exclude(\n+            codename__in=existing_target_codenames,\n         ).update(content_type=new_content_type)\n \n \ndiff --git a/tests/auth_tests/test_migrations.py b/tests/auth_tests/test_migrations.py\nindex 5ff2f6b4b3..402bceb9fc 100644\n--- a/tests/auth_tests/test_migrations.py\n+++ b/tests/auth_tests/test_migrations.py\n@@ -10,6 +10,124 @@ from .models import Proxy, UserProxy\n update_proxy_permissions = import_module('django.contrib.auth.migrations.0011_update_proxy_permissions')\n \n \n+class ProxyModelWithSameAppLabelAndExistingPermissionsTests(TestCase):\n+    \"\"\"\n+    Test that the migration does not fail with an IntegrityError when a model\n+    was previously concrete and was re-created as a proxy. In that case the\n+    proxy ContentType already exists and already has permissions, so the\n+    migration must not attempt to duplicate them.\n+    \"\"\"\n+    available_apps = [\n+        'auth_tests',\n+        'django.contrib.auth',\n+        'django.contrib.contenttypes',\n+    ]\n+\n+    def setUp(self):\n+        \"\"\"\n+        Simulate a model that started as concrete (permissions stored under\n+        the concrete ContentType) and was later re-created as a proxy.  Both\n+        the concrete *and* the proxy ContentType therefore carry the same\n+        permissions \u2014 the state that triggers the IntegrityError.\n+        \"\"\"\n+        Permission.objects.all().delete()\n+        self.concrete_content_type = ContentType.objects.get_for_model(Proxy, for_concrete_model=True)\n+        self.proxy_content_type = ContentType.objects.get_for_model(Proxy, for_concrete_model=False)\n+        # Permissions already exist under the proxy content type (the model\n+        # was once concrete and had its own ContentType).\n+        self.default_permission = Permission.objects.create(\n+            content_type=self.proxy_content_type,\n+            codename='add_proxy',\n+            name='Can add proxy',\n+        )\n+        self.custom_permission = Permission.objects.create(\n+            content_type=self.proxy_content_type,\n+            codename='display_proxys',\n+            name='May display proxys information',\n+        )\n+        # Duplicate entries under the concrete ContentType (old state left\n+        # over from before the model was turned into a proxy).\n+        Permission.objects.create(\n+            content_type=self.concrete_content_type,\n+            codename='add_proxy',\n+            name='Can add proxy',\n+        )\n+        Permission.objects.create(\n+            content_type=self.concrete_content_type,\n+            codename='display_proxys',\n+            name='May display proxys information',\n+        )\n+\n+    def test_does_not_raise_integrity_error(self):\n+        \"\"\"\n+        Running the migration should not raise an IntegrityError even when\n+        duplicate permissions already exist under the proxy ContentType.\n+        \"\"\"\n+        # This must not raise django.db.utils.IntegrityError.\n+        update_proxy_permissions.update_proxy_model_permissions(apps, None)\n+\n+    def test_existing_proxy_permissions_are_kept(self):\n+        \"\"\"\n+        After the migration the permissions originally under the proxy\n+        ContentType are still present.\n+        \"\"\"\n+        update_proxy_permissions.update_proxy_model_permissions(apps, None)\n+        self.default_permission.refresh_from_db()\n+        self.assertEqual(self.default_permission.content_type, self.proxy_content_type)\n+        self.custom_permission.refresh_from_db()\n+        self.assertEqual(self.custom_permission.content_type, self.proxy_content_type)\n+\n+\n+class ProxyModelWithDifferentAppLabelAndExistingPermissionsTests(TestCase):\n+    \"\"\"\n+    Same as ProxyModelWithSameAppLabelAndExistingPermissionsTests but for a\n+    proxy model whose app_label differs from that of the concrete parent\n+    (i.e. UserProxy / auth_tests vs User / auth).\n+    \"\"\"\n+    available_apps = [\n+        'auth_tests',\n+        'django.contrib.auth',\n+        'django.contrib.contenttypes',\n+    ]\n+\n+    def setUp(self):\n+        Permission.objects.all().delete()\n+        self.concrete_content_type = ContentType.objects.get_for_model(UserProxy, for_concrete_model=True)\n+        self.proxy_content_type = ContentType.objects.get_for_model(UserProxy, for_concrete_model=False)\n+        # Permissions already exist under the proxy ContentType.\n+        self.default_permission = Permission.objects.create(\n+            content_type=self.proxy_content_type,\n+            codename='add_userproxy',\n+            name='Can add userproxy',\n+        )\n+        self.custom_permission = Permission.objects.create(\n+            content_type=self.proxy_content_type,\n+            codename='use_different_app_label',\n+            name='May use a different app label',\n+        )\n+        # Duplicate entries under the concrete ContentType.\n+        Permission.objects.create(\n+            content_type=self.concrete_content_type,\n+            codename='add_userproxy',\n+            name='Can add userproxy',\n+        )\n+        Permission.objects.create(\n+            content_type=self.concrete_content_type,\n+            codename='use_different_app_label',\n+            name='May use a different app label',\n+        )\n+\n+    def test_does_not_raise_integrity_error(self):\n+        update_proxy_permissions.update_proxy_model_permissions(apps, None)\n+\n+    def test_existing_proxy_permissions_are_kept(self):\n+        update_proxy_permissions.update_proxy_model_permissions(apps, None)\n+        self.default_permission.refresh_from_db()\n+        self.assertEqual(self.default_permission.content_type, self.proxy_content_type)\n+        self.custom_permission.refresh_from_db()\n+        self.assertEqual(self.custom_permission.content_type, self.proxy_content_type)\n+\n+\n class ProxyModelWithDifferentAppLabelTests(TestCase):\n     available_apps = [\n         'auth_tests',\n"}
{"instance_id": "django__django-11422", "model_name_or_path": "harness/claude-sonnet-4-6", "model_patch": "diff --git a/.harness/checkpoints/516e44c36fae/--Users--huetuanthi--dev--agents--harness--django--utils--autoreload.py b/.harness/checkpoints/516e44c36fae/--Users--huetuanthi--dev--agents--harness--django--utils--autoreload.py\nnew file mode 100644\nindex 0000000000..7ca0d68fae\n--- /dev/null\n+++ b/.harness/checkpoints/516e44c36fae/--Users--huetuanthi--dev--agents--harness--django--utils--autoreload.py\n@@ -0,0 +1,588 @@\n+import functools\n+import itertools\n+import logging\n+import os\n+import pathlib\n+import signal\n+import subprocess\n+import sys\n+import threading\n+import time\n+import traceback\n+import weakref\n+from collections import defaultdict\n+from pathlib import Path\n+from types import ModuleType\n+from zipimport import zipimporter\n+\n+from django.apps import apps\n+from django.core.signals import request_finished\n+from django.dispatch import Signal\n+from django.utils.functional import cached_property\n+from django.utils.version import get_version_tuple\n+\n+autoreload_started = Signal()\n+file_changed = Signal(providing_args=['file_path', 'kind'])\n+\n+DJANGO_AUTORELOAD_ENV = 'RUN_MAIN'\n+\n+logger = logging.getLogger('django.utils.autoreload')\n+\n+# If an error is raised while importing a file, it's not placed in sys.modules.\n+# This means that any future modifications aren't caught. Keep a list of these\n+# file paths to allow watching them in the future.\n+_error_files = []\n+_exception = None\n+\n+try:\n+    import termios\n+except ImportError:\n+    termios = None\n+\n+\n+try:\n+    import pywatchman\n+except ImportError:\n+    pywatchman = None\n+\n+\n+def check_errors(fn):\n+    @functools.wraps(fn)\n+    def wrapper(*args, **kwargs):\n+        global _exception\n+        try:\n+            fn(*args, **kwargs)\n+        except Exception:\n+            _exception = sys.exc_info()\n+\n+            et, ev, tb = _exception\n+\n+            if getattr(ev, 'filename', None) is None:\n+                # get the filename from the last item in the stack\n+                filename = traceback.extract_tb(tb)[-1][0]\n+            else:\n+                filename = ev.filename\n+\n+            if filename not in _error_files:\n+                _error_files.append(filename)\n+\n+            raise\n+\n+    return wrapper\n+\n+\n+def raise_last_exception():\n+    global _exception\n+    if _exception is not None:\n+        raise _exception[0](_exception[1]).with_traceback(_exception[2])\n+\n+\n+def ensure_echo_on():\n+    \"\"\"\n+    Ensure that echo mode is enabled. Some tools such as PDB disable\n+    it which causes usability issues after reload.\n+    \"\"\"\n+    if not termios or not sys.stdin.isatty():\n+        return\n+    attr_list = termios.tcgetattr(sys.stdin)\n+    if not attr_list[3] & termios.ECHO:\n+        attr_list[3] |= termios.ECHO\n+        if hasattr(signal, 'SIGTTOU'):\n+            old_handler = signal.signal(signal.SIGTTOU, signal.SIG_IGN)\n+        else:\n+            old_handler = None\n+        termios.tcsetattr(sys.stdin, termios.TCSANOW, attr_list)\n+        if old_handler is not None:\n+            signal.signal(signal.SIGTTOU, old_handler)\n+\n+\n+def iter_all_python_module_files():\n+    # This is a hot path during reloading. Create a stable sorted list of\n+    # modules based on the module name and pass it to iter_modules_and_files().\n+    # This ensures cached results are returned in the usual case that modules\n+    # aren't loaded on the fly.\n+    keys = sorted(sys.modules)\n+    modules = tuple(m for m in map(sys.modules.__getitem__, keys) if not isinstance(m, weakref.ProxyTypes))\n+    return iter_modules_and_files(modules, frozenset(_error_files))\n+\n+\n+@functools.lru_cache(maxsize=1)\n+def iter_modules_and_files(modules, extra_files):\n+    \"\"\"Iterate through all modules needed to be watched.\"\"\"\n+    sys_file_paths = []\n+    for module in modules:\n+        # During debugging (with PyDev) the 'typing.io' and 'typing.re' objects\n+        # are added to sys.modules, however they are types not modules and so\n+        # cause issues here.\n+        if not isinstance(module, ModuleType) or getattr(module, '__spec__', None) is None:\n+            continue\n+        spec = module.__spec__\n+        # Modules could be loaded from places without a concrete location. If\n+        # this is the case, skip them.\n+        if spec.has_location:\n+            origin = spec.loader.archive if isinstance(spec.loader, zipimporter) else spec.origin\n+            sys_file_paths.append(origin)\n+\n+    results = set()\n+    for filename in itertools.chain(sys_file_paths, extra_files):\n+        if not filename:\n+            continue\n+        path = pathlib.Path(filename)\n+        if not path.exists():\n+            # The module could have been removed, don't fail loudly if this\n+            # is the case.\n+            continue\n+        results.add(path.resolve().absolute())\n+    return frozenset(results)\n+\n+\n+@functools.lru_cache(maxsize=1)\n+def common_roots(paths):\n+    \"\"\"\n+    Return a tuple of common roots that are shared between the given paths.\n+    File system watchers operate on directories and aren't cheap to create.\n+    Try to find the minimum set of directories to watch that encompass all of\n+    the files that need to be watched.\n+    \"\"\"\n+    # Inspired from Werkzeug:\n+    # https://github.com/pallets/werkzeug/blob/7477be2853df70a022d9613e765581b9411c3c39/werkzeug/_reloader.py\n+    # Create a sorted list of the path components, longest first.\n+    path_parts = sorted([x.parts for x in paths], key=len, reverse=True)\n+    tree = {}\n+    for chunks in path_parts:\n+        node = tree\n+        # Add each part of the path to the tree.\n+        for chunk in chunks:\n+            node = node.setdefault(chunk, {})\n+        # Clear the last leaf in the tree.\n+        node.clear()\n+\n+    # Turn the tree into a list of Path instances.\n+    def _walk(node, path):\n+        for prefix, child in node.items():\n+            yield from _walk(child, path + (prefix,))\n+        if not node:\n+            yield Path(*path)\n+\n+    return tuple(_walk(tree, ()))\n+\n+\n+def sys_path_directories():\n+    \"\"\"\n+    Yield absolute directories from sys.path, ignoring entries that don't\n+    exist.\n+    \"\"\"\n+    for path in sys.path:\n+        path = Path(path)\n+        if not path.exists():\n+            continue\n+        path = path.resolve().absolute()\n+        # If the path is a file (like a zip file), watch the parent directory.\n+        if path.is_file():\n+            yield path.parent\n+        else:\n+            yield path\n+\n+\n+def get_child_arguments():\n+    \"\"\"\n+    Return the executable. This contains a workaround for Windows if the\n+    executable is reported to not have the .exe extension which can cause bugs\n+    on reloading.\n+    \"\"\"\n+    import django.__main__\n+\n+    args = [sys.executable] + ['-W%s' % o for o in sys.warnoptions]\n+    if sys.argv[0] == django.__main__.__file__:\n+        # The server was started with `python -m django runserver`.\n+        args += ['-m', 'django']\n+        args += sys.argv[1:]\n+    else:\n+        args += sys.argv\n+    return args\n+\n+\n+def trigger_reload(filename):\n+    logger.info('%s changed, reloading.', filename)\n+    sys.exit(3)\n+\n+\n+def restart_with_reloader():\n+    new_environ = {**os.environ, DJANGO_AUTORELOAD_ENV: 'true'}\n+    args = get_child_arguments()\n+    while True:\n+        exit_code = subprocess.call(args, env=new_environ, close_fds=False)\n+        if exit_code != 3:\n+            return exit_code\n+\n+\n+class BaseReloader:\n+    def __init__(self):\n+        self.extra_files = set()\n+        self.directory_globs = defaultdict(set)\n+        self._stop_condition = threading.Event()\n+\n+    def watch_dir(self, path, glob):\n+        path = Path(path)\n+        if not path.is_absolute():\n+            raise ValueError('%s must be absolute.' % path)\n+        logger.debug('Watching dir %s with glob %s.', path, glob)\n+        self.directory_globs[path].add(glob)\n+\n+    def watch_file(self, path):\n+        path = Path(path)\n+        if not path.is_absolute():\n+            raise ValueError('%s must be absolute.' % path)\n+        logger.debug('Watching file %s.', path)\n+        self.extra_files.add(path)\n+\n+    def watched_files(self, include_globs=True):\n+        \"\"\"\n+        Yield all files that need to be watched, including module files and\n+        files within globs.\n+        \"\"\"\n+        yield from iter_all_python_module_files()\n+        yield from self.extra_files\n+        if include_globs:\n+            for directory, patterns in self.directory_globs.items():\n+                for pattern in patterns:\n+                    yield from directory.glob(pattern)\n+\n+    def wait_for_apps_ready(self, app_reg, django_main_thread):\n+        \"\"\"\n+        Wait until Django reports that the apps have been loaded. If the given\n+        thread has terminated before the apps are ready, then a SyntaxError or\n+        other non-recoverable error has been raised. In that case, stop waiting\n+        for the apps_ready event and continue processing.\n+\n+        Return True if the thread is alive and the ready event has been\n+        triggered, or False if the thread is terminated while waiting for the\n+        event.\n+        \"\"\"\n+        while django_main_thread.is_alive():\n+            if app_reg.ready_event.wait(timeout=0.1):\n+                return True\n+        else:\n+            logger.debug('Main Django thread has terminated before apps are ready.')\n+            return False\n+\n+    def run(self, django_main_thread):\n+        logger.debug('Waiting for apps ready_event.')\n+        self.wait_for_apps_ready(apps, django_main_thread)\n+        from django.urls import get_resolver\n+        # Prevent a race condition where URL modules aren't loaded when the\n+        # reloader starts by accessing the urlconf_module property.\n+        try:\n+            get_resolver().urlconf_module\n+        except Exception:\n+            # Loading the urlconf can result in errors during development.\n+            # If this occurs then swallow the error and continue.\n+            pass\n+        logger.debug('Apps ready_event triggered. Sending autoreload_started signal.')\n+        autoreload_started.send(sender=self)\n+        self.run_loop()\n+\n+    def run_loop(self):\n+        ticker = self.tick()\n+        while not self.should_stop:\n+            try:\n+                next(ticker)\n+            except StopIteration:\n+                break\n+        self.stop()\n+\n+    def tick(self):\n+        \"\"\"\n+        This generator is called in a loop from run_loop. It's important that\n+        the method takes care of pausing or otherwise waiting for a period of\n+        time. This split between run_loop() and tick() is to improve the\n+        testability of the reloader implementations by decoupling the work they\n+        do from the loop.\n+        \"\"\"\n+        raise NotImplementedError('subclasses must implement tick().')\n+\n+    @classmethod\n+    def check_availability(cls):\n+        raise NotImplementedError('subclasses must implement check_availability().')\n+\n+    def notify_file_changed(self, path):\n+        results = file_changed.send(sender=self, file_path=path)\n+        logger.debug('%s notified as changed. Signal results: %s.', path, results)\n+        if not any(res[1] for res in results):\n+            trigger_reload(path)\n+\n+    # These are primarily used for testing.\n+    @property\n+    def should_stop(self):\n+        return self._stop_condition.is_set()\n+\n+    def stop(self):\n+        self._stop_condition.set()\n+\n+\n+class StatReloader(BaseReloader):\n+    SLEEP_TIME = 1  # Check for changes once per second.\n+\n+    def tick(self):\n+        mtimes = {}\n+        while True:\n+            for filepath, mtime in self.snapshot_files():\n+                old_time = mtimes.get(filepath)\n+                if old_time is None:\n+                    logger.debug('File %s first seen with mtime %s', filepath, mtime)\n+                    mtimes[filepath] = mtime\n+                    continue\n+                elif mtime > old_time:\n+                    logger.debug('File %s previous mtime: %s, current mtime: %s', filepath, old_time, mtime)\n+                    self.notify_file_changed(filepath)\n+\n+            time.sleep(self.SLEEP_TIME)\n+            yield\n+\n+    def snapshot_files(self):\n+        # watched_files may produce duplicate paths if globs overlap.\n+        seen_files = set()\n+        for file in self.watched_files():\n+            if file in seen_files:\n+                continue\n+            try:\n+                mtime = file.stat().st_mtime\n+            except OSError:\n+                # This is thrown when the file does not exist.\n+                continue\n+            seen_files.add(file)\n+            yield file, mtime\n+\n+    @classmethod\n+    def check_availability(cls):\n+        return True\n+\n+\n+class WatchmanUnavailable(RuntimeError):\n+    pass\n+\n+\n+class WatchmanReloader(BaseReloader):\n+    def __init__(self):\n+        self.roots = defaultdict(set)\n+        self.processed_request = threading.Event()\n+        self.client_timeout = int(os.environ.get('DJANGO_WATCHMAN_TIMEOUT', 5))\n+        super().__init__()\n+\n+    @cached_property\n+    def client(self):\n+        return pywatchman.client(timeout=self.client_timeout)\n+\n+    def _watch_root(self, root):\n+        # In practice this shouldn't occur, however, it's possible that a\n+        # directory that doesn't exist yet is being watched. If it's outside of\n+        # sys.path then this will end up a new root. How to handle this isn't\n+        # clear: Not adding the root will likely break when subscribing to the\n+        # changes, however, as this is currently an internal API,  no files\n+        # will be being watched outside of sys.path. Fixing this by checking\n+        # inside watch_glob() and watch_dir() is expensive, instead this could\n+        # could fall back to the StatReloader if this case is detected? For\n+        # now, watching its parent, if possible, is sufficient.\n+        if not root.exists():\n+            if not root.parent.exists():\n+                logger.warning('Unable to watch root dir %s as neither it or its parent exist.', root)\n+                return\n+            root = root.parent\n+        result = self.client.query('watch-project', str(root.absolute()))\n+        if 'warning' in result:\n+            logger.warning('Watchman warning: %s', result['warning'])\n+        logger.debug('Watchman watch-project result: %s', result)\n+        return result['watch'], result.get('relative_path')\n+\n+    @functools.lru_cache()\n+    def _get_clock(self, root):\n+        return self.client.query('clock', root)['clock']\n+\n+    def _subscribe(self, directory, name, expression):\n+        root, rel_path = self._watch_root(directory)\n+        query = {\n+            'expression': expression,\n+            'fields': ['name'],\n+            'since': self._get_clock(root),\n+            'dedup_results': True,\n+        }\n+        if rel_path:\n+            query['relative_root'] = rel_path\n+        logger.debug('Issuing watchman subscription %s, for root %s. Query: %s', name, root, query)\n+        self.client.query('subscribe', root, name, query)\n+\n+    def _subscribe_dir(self, directory, filenames):\n+        if not directory.exists():\n+            if not directory.parent.exists():\n+                logger.warning('Unable to watch directory %s as neither it or its parent exist.', directory)\n+                return\n+            prefix = 'files-parent-%s' % directory.name\n+            filenames = ['%s/%s' % (directory.name, filename) for filename in filenames]\n+            directory = directory.parent\n+            expression = ['name', filenames, 'wholename']\n+        else:\n+            prefix = 'files'\n+            expression = ['name', filenames]\n+        self._subscribe(directory, '%s:%s' % (prefix, directory), expression)\n+\n+    def _watch_glob(self, directory, patterns):\n+        \"\"\"\n+        Watch a directory with a specific glob. If the directory doesn't yet\n+        exist, attempt to watch the parent directory and amend the patterns to\n+        include this. It's important this method isn't called more than one per\n+        directory when updating all subscriptions. Subsequent calls will\n+        overwrite the named subscription, so it must include all possible glob\n+        expressions.\n+        \"\"\"\n+        prefix = 'glob'\n+        if not directory.exists():\n+            if not directory.parent.exists():\n+                logger.warning('Unable to watch directory %s as neither it or its parent exist.', directory)\n+                return\n+            prefix = 'glob-parent-%s' % directory.name\n+            patterns = ['%s/%s' % (directory.name, pattern) for pattern in patterns]\n+            directory = directory.parent\n+\n+        expression = ['anyof']\n+        for pattern in patterns:\n+            expression.append(['match', pattern, 'wholename'])\n+        self._subscribe(directory, '%s:%s' % (prefix, directory), expression)\n+\n+    def watched_roots(self, watched_files):\n+        extra_directories = self.directory_globs.keys()\n+        watched_file_dirs = [f.parent for f in watched_files]\n+        sys_paths = list(sys_path_directories())\n+        return frozenset((*extra_directories, *watched_file_dirs, *sys_paths))\n+\n+    def _update_watches(self):\n+        watched_files = list(self.watched_files(include_globs=False))\n+        found_roots = common_roots(self.watched_roots(watched_files))\n+        logger.debug('Watching %s files', len(watched_files))\n+        logger.debug('Found common roots: %s', found_roots)\n+        # Setup initial roots for performance, shortest roots first.\n+        for root in sorted(found_roots):\n+            self._watch_root(root)\n+        for directory, patterns in self.directory_globs.items():\n+            self._watch_glob(directory, patterns)\n+        # Group sorted watched_files by their parent directory.\n+        sorted_files = sorted(watched_files, key=lambda p: p.parent)\n+        for directory, group in itertools.groupby(sorted_files, key=lambda p: p.parent):\n+            # These paths need to be relative to the parent directory.\n+            self._subscribe_dir(directory, [str(p.relative_to(directory)) for p in group])\n+\n+    def update_watches(self):\n+        try:\n+            self._update_watches()\n+        except Exception as ex:\n+            # If the service is still available, raise the original exception.\n+            if self.check_server_status(ex):\n+                raise\n+\n+    def _check_subscription(self, sub):\n+        subscription = self.client.getSubscription(sub)\n+        if not subscription:\n+            return\n+        logger.debug('Watchman subscription %s has results.', sub)\n+        for result in subscription:\n+            # When using watch-project, it's not simple to get the relative\n+            # directory without storing some specific state. Store the full\n+            # path to the directory in the subscription name, prefixed by its\n+            # type (glob, files).\n+            root_directory = Path(result['subscription'].split(':', 1)[1])\n+            logger.debug('Found root directory %s', root_directory)\n+            for file in result.get('files', []):\n+                self.notify_file_changed(root_directory / file)\n+\n+    def request_processed(self, **kwargs):\n+        logger.debug('Request processed. Setting update_watches event.')\n+        self.processed_request.set()\n+\n+    def tick(self):\n+        request_finished.connect(self.request_processed)\n+        self.update_watches()\n+        while True:\n+            if self.processed_request.is_set():\n+                self.update_watches()\n+                self.processed_request.clear()\n+            try:\n+                self.client.receive()\n+            except pywatchman.SocketTimeout:\n+                pass\n+            except pywatchman.WatchmanError as ex:\n+                logger.debug('Watchman error: %s, checking server status.', ex)\n+                self.check_server_status(ex)\n+            else:\n+                for sub in list(self.client.subs.keys()):\n+                    self._check_subscription(sub)\n+            yield\n+\n+    def stop(self):\n+        self.client.close()\n+        super().stop()\n+\n+    def check_server_status(self, inner_ex=None):\n+        \"\"\"Return True if the server is available.\"\"\"\n+        try:\n+            self.client.query('version')\n+        except Exception:\n+            raise WatchmanUnavailable(str(inner_ex)) from inner_ex\n+        return True\n+\n+    @classmethod\n+    def check_availability(cls):\n+        if not pywatchman:\n+            raise WatchmanUnavailable('pywatchman not installed.')\n+        client = pywatchman.client(timeout=0.1)\n+        try:\n+            result = client.capabilityCheck()\n+        except Exception:\n+            # The service is down?\n+            raise WatchmanUnavailable('Cannot connect to the watchman service.')\n+        version = get_version_tuple(result['version'])\n+        # Watchman 4.9 includes multiple improvements to watching project\n+        # directories as well as case insensitive filesystems.\n+        logger.debug('Watchman version %s', version)\n+        if version < (4, 9):\n+            raise WatchmanUnavailable('Watchman 4.9 or later is required.')\n+\n+\n+def get_reloader():\n+    \"\"\"Return the most suitable reloader for this environment.\"\"\"\n+    try:\n+        WatchmanReloader.check_availability()\n+    except WatchmanUnavailable:\n+        return StatReloader()\n+    return WatchmanReloader()\n+\n+\n+def start_django(reloader, main_func, *args, **kwargs):\n+    ensure_echo_on()\n+\n+    main_func = check_errors(main_func)\n+    django_main_thread = threading.Thread(target=main_func, args=args, kwargs=kwargs, name='django-main-thread')\n+    django_main_thread.setDaemon(True)\n+    django_main_thread.start()\n+\n+    while not reloader.should_stop:\n+        try:\n+            reloader.run(django_main_thread)\n+        except WatchmanUnavailable as ex:\n+            # It's possible that the watchman service shuts down or otherwise\n+            # becomes unavailable. In that case, use the StatReloader.\n+            reloader = StatReloader()\n+            logger.error('Error connecting to Watchman: %s', ex)\n+            logger.info('Watching for file changes with %s', reloader.__class__.__name__)\n+\n+\n+def run_with_reloader(main_func, *args, **kwargs):\n+    signal.signal(signal.SIGTERM, lambda *args: sys.exit(0))\n+    try:\n+        if os.environ.get(DJANGO_AUTORELOAD_ENV) == 'true':\n+            reloader = get_reloader()\n+            logger.info('Watching for file changes with %s', reloader.__class__.__name__)\n+            start_django(reloader, main_func, *args, **kwargs)\n+        else:\n+            exit_code = restart_with_reloader()\n+            sys.exit(exit_code)\n+    except KeyboardInterrupt:\n+        pass\ndiff --git a/django/utils/autoreload.py b/django/utils/autoreload.py\nindex 7ca0d68fae..e2f9a2f136 100644\n--- a/django/utils/autoreload.py\n+++ b/django/utils/autoreload.py\n@@ -114,9 +114,16 @@ def iter_modules_and_files(modules, extra_files):\n         # During debugging (with PyDev) the 'typing.io' and 'typing.re' objects\n         # are added to sys.modules, however they are types not modules and so\n         # cause issues here.\n-        if not isinstance(module, ModuleType) or getattr(module, '__spec__', None) is None:\n+        if not isinstance(module, ModuleType):\n+            continue\n+        spec = getattr(module, '__spec__', None)\n+        if spec is None:\n+            # __spec__ is None for the __main__ module when run as a script\n+            # (e.g. `python manage.py`). In this case, use __file__ instead\n+            # so that changes to the script itself trigger a reload.\n+            if getattr(module, '__file__', None):\n+                sys_file_paths.append(module.__file__)\n             continue\n-        spec = module.__spec__\n         # Modules could be loaded from places without a concrete location. If\n         # this is the case, skip them.\n         if spec.has_location:\ndiff --git a/tests/utils_tests/test_autoreload.py b/tests/utils_tests/test_autoreload.py\nindex 42bcd0a50b..16dce3dd9d 100644\n--- a/tests/utils_tests/test_autoreload.py\n+++ b/tests/utils_tests/test_autoreload.py\n@@ -132,6 +132,22 @@ class TestIterModulesAndFiles(SimpleTestCase):\n         del module.__spec__\n         self.assertEqual(autoreload.iter_modules_and_files((module,), frozenset()), frozenset())\n \n+    def test_main_module_is_resolved(self):\n+        \"\"\"\n+        If __main__ is loaded from a script (e.g. `python manage.py`), its\n+        __spec__ is None but its __file__ is set. It should be included in\n+        the watched files so that changes to it trigger a reload.\n+        \"\"\"\n+        main_module = types.ModuleType('__main__')\n+        main_module.__spec__ = None\n+        main_module.__file__ = self.temporary_file('manage.py')\n+        main_module.__file__.touch()\n+        self.clear_autoreload_caches()\n+        self.assertIn(\n+            main_module.__file__.resolve(),\n+            autoreload.iter_modules_and_files((main_module,), frozenset()),\n+        )\n+\n \n class TestCommonRoots(SimpleTestCase):\n     def test_common_roots(self):\n"}
{"instance_id": "django__django-11564", "model_name_or_path": "harness/claude-sonnet-4-6", "model_patch": "diff --git a/.harness/checkpoints/6aa09a87f073/--Users--huetuanthi--dev--agents--harness--django--core--files--storage.py b/.harness/checkpoints/6aa09a87f073/--Users--huetuanthi--dev--agents--harness--django--core--files--storage.py\nnew file mode 100644\nindex 0000000000..4c27fce605\n--- /dev/null\n+++ b/.harness/checkpoints/6aa09a87f073/--Users--huetuanthi--dev--agents--harness--django--core--files--storage.py\n@@ -0,0 +1,367 @@\n+import os\n+from datetime import datetime\n+from urllib.parse import urljoin\n+\n+from django.conf import settings\n+from django.core.exceptions import SuspiciousFileOperation\n+from django.core.files import File, locks\n+from django.core.files.move import file_move_safe\n+from django.core.signals import setting_changed\n+from django.utils import timezone\n+from django.utils._os import safe_join\n+from django.utils.crypto import get_random_string\n+from django.utils.deconstruct import deconstructible\n+from django.utils.encoding import filepath_to_uri\n+from django.utils.functional import LazyObject, cached_property\n+from django.utils.module_loading import import_string\n+from django.utils.text import get_valid_filename\n+\n+__all__ = (\n+    'Storage', 'FileSystemStorage', 'DefaultStorage', 'default_storage',\n+    'get_storage_class',\n+)\n+\n+\n+class Storage:\n+    \"\"\"\n+    A base storage class, providing some default behaviors that all other\n+    storage systems can inherit or override, as necessary.\n+    \"\"\"\n+\n+    # The following methods represent a public interface to private methods.\n+    # These shouldn't be overridden by subclasses unless absolutely necessary.\n+\n+    def open(self, name, mode='rb'):\n+        \"\"\"Retrieve the specified file from storage.\"\"\"\n+        return self._open(name, mode)\n+\n+    def save(self, name, content, max_length=None):\n+        \"\"\"\n+        Save new content to the file specified by name. The content should be\n+        a proper File object or any Python file-like object, ready to be read\n+        from the beginning.\n+        \"\"\"\n+        # Get the proper name for the file, as it will actually be saved.\n+        if name is None:\n+            name = content.name\n+\n+        if not hasattr(content, 'chunks'):\n+            content = File(content, name)\n+\n+        name = self.get_available_name(name, max_length=max_length)\n+        return self._save(name, content)\n+\n+    # These methods are part of the public API, with default implementations.\n+\n+    def get_valid_name(self, name):\n+        \"\"\"\n+        Return a filename, based on the provided filename, that's suitable for\n+        use in the target storage system.\n+        \"\"\"\n+        return get_valid_filename(name)\n+\n+    def get_alternative_name(self, file_root, file_ext):\n+        \"\"\"\n+        Return an alternative filename, by adding an underscore and a random 7\n+        character alphanumeric string (before the file extension, if one\n+        exists) to the filename.\n+        \"\"\"\n+        return '%s_%s%s' % (file_root, get_random_string(7), file_ext)\n+\n+    def get_available_name(self, name, max_length=None):\n+        \"\"\"\n+        Return a filename that's free on the target storage system and\n+        available for new content to be written to.\n+        \"\"\"\n+        dir_name, file_name = os.path.split(name)\n+        file_root, file_ext = os.path.splitext(file_name)\n+        # If the filename already exists, generate an alternative filename\n+        # until it doesn't exist.\n+        # Truncate original name if required, so the new filename does not\n+        # exceed the max_length.\n+        while self.exists(name) or (max_length and len(name) > max_length):\n+            # file_ext includes the dot.\n+            name = os.path.join(dir_name, self.get_alternative_name(file_root, file_ext))\n+            if max_length is None:\n+                continue\n+            # Truncate file_root if max_length exceeded.\n+            truncation = len(name) - max_length\n+            if truncation > 0:\n+                file_root = file_root[:-truncation]\n+                # Entire file_root was truncated in attempt to find an available filename.\n+                if not file_root:\n+                    raise SuspiciousFileOperation(\n+                        'Storage can not find an available filename for \"%s\". '\n+                        'Please make sure that the corresponding file field '\n+                        'allows sufficient \"max_length\".' % name\n+                    )\n+                name = os.path.join(dir_name, self.get_alternative_name(file_root, file_ext))\n+        return name\n+\n+    def generate_filename(self, filename):\n+        \"\"\"\n+        Validate the filename by calling get_valid_name() and return a filename\n+        to be passed to the save() method.\n+        \"\"\"\n+        # `filename` may include a path as returned by FileField.upload_to.\n+        dirname, filename = os.path.split(filename)\n+        return os.path.normpath(os.path.join(dirname, self.get_valid_name(filename)))\n+\n+    def path(self, name):\n+        \"\"\"\n+        Return a local filesystem path where the file can be retrieved using\n+        Python's built-in open() function. Storage systems that can't be\n+        accessed using open() should *not* implement this method.\n+        \"\"\"\n+        raise NotImplementedError(\"This backend doesn't support absolute paths.\")\n+\n+    # The following methods form the public API for storage systems, but with\n+    # no default implementations. Subclasses must implement *all* of these.\n+\n+    def delete(self, name):\n+        \"\"\"\n+        Delete the specified file from the storage system.\n+        \"\"\"\n+        raise NotImplementedError('subclasses of Storage must provide a delete() method')\n+\n+    def exists(self, name):\n+        \"\"\"\n+        Return True if a file referenced by the given name already exists in the\n+        storage system, or False if the name is available for a new file.\n+        \"\"\"\n+        raise NotImplementedError('subclasses of Storage must provide an exists() method')\n+\n+    def listdir(self, path):\n+        \"\"\"\n+        List the contents of the specified path. Return a 2-tuple of lists:\n+        the first item being directories, the second item being files.\n+        \"\"\"\n+        raise NotImplementedError('subclasses of Storage must provide a listdir() method')\n+\n+    def size(self, name):\n+        \"\"\"\n+        Return the total size, in bytes, of the file specified by name.\n+        \"\"\"\n+        raise NotImplementedError('subclasses of Storage must provide a size() method')\n+\n+    def url(self, name):\n+        \"\"\"\n+        Return an absolute URL where the file's contents can be accessed\n+        directly by a Web browser.\n+        \"\"\"\n+        raise NotImplementedError('subclasses of Storage must provide a url() method')\n+\n+    def get_accessed_time(self, name):\n+        \"\"\"\n+        Return the last accessed time (as a datetime) of the file specified by\n+        name. The datetime will be timezone-aware if USE_TZ=True.\n+        \"\"\"\n+        raise NotImplementedError('subclasses of Storage must provide a get_accessed_time() method')\n+\n+    def get_created_time(self, name):\n+        \"\"\"\n+        Return the creation time (as a datetime) of the file specified by name.\n+        The datetime will be timezone-aware if USE_TZ=True.\n+        \"\"\"\n+        raise NotImplementedError('subclasses of Storage must provide a get_created_time() method')\n+\n+    def get_modified_time(self, name):\n+        \"\"\"\n+        Return the last modified time (as a datetime) of the file specified by\n+        name. The datetime will be timezone-aware if USE_TZ=True.\n+        \"\"\"\n+        raise NotImplementedError('subclasses of Storage must provide a get_modified_time() method')\n+\n+\n+@deconstructible\n+class FileSystemStorage(Storage):\n+    \"\"\"\n+    Standard filesystem storage\n+    \"\"\"\n+    # The combination of O_CREAT and O_EXCL makes os.open() raise OSError if\n+    # the file already exists before it's opened.\n+    OS_OPEN_FLAGS = os.O_WRONLY | os.O_CREAT | os.O_EXCL | getattr(os, 'O_BINARY', 0)\n+\n+    def __init__(self, location=None, base_url=None, file_permissions_mode=None,\n+                 directory_permissions_mode=None):\n+        self._location = location\n+        self._base_url = base_url\n+        self._file_permissions_mode = file_permissions_mode\n+        self._directory_permissions_mode = directory_permissions_mode\n+        setting_changed.connect(self._clear_cached_properties)\n+\n+    def _clear_cached_properties(self, setting, **kwargs):\n+        \"\"\"Reset setting based property values.\"\"\"\n+        if setting == 'MEDIA_ROOT':\n+            self.__dict__.pop('base_location', None)\n+            self.__dict__.pop('location', None)\n+        elif setting == 'MEDIA_URL':\n+            self.__dict__.pop('base_url', None)\n+        elif setting == 'FILE_UPLOAD_PERMISSIONS':\n+            self.__dict__.pop('file_permissions_mode', None)\n+        elif setting == 'FILE_UPLOAD_DIRECTORY_PERMISSIONS':\n+            self.__dict__.pop('directory_permissions_mode', None)\n+\n+    def _value_or_setting(self, value, setting):\n+        return setting if value is None else value\n+\n+    @cached_property\n+    def base_location(self):\n+        return self._value_or_setting(self._location, settings.MEDIA_ROOT)\n+\n+    @cached_property\n+    def location(self):\n+        return os.path.abspath(self.base_location)\n+\n+    @cached_property\n+    def base_url(self):\n+        if self._base_url is not None and not self._base_url.endswith('/'):\n+            self._base_url += '/'\n+        return self._value_or_setting(self._base_url, settings.MEDIA_URL)\n+\n+    @cached_property\n+    def file_permissions_mode(self):\n+        return self._value_or_setting(self._file_permissions_mode, settings.FILE_UPLOAD_PERMISSIONS)\n+\n+    @cached_property\n+    def directory_permissions_mode(self):\n+        return self._value_or_setting(self._directory_permissions_mode, settings.FILE_UPLOAD_DIRECTORY_PERMISSIONS)\n+\n+    def _open(self, name, mode='rb'):\n+        return File(open(self.path(name), mode))\n+\n+    def _save(self, name, content):\n+        full_path = self.path(name)\n+\n+        # Create any intermediate directories that do not exist.\n+        directory = os.path.dirname(full_path)\n+        try:\n+            if self.directory_permissions_mode is not None:\n+                # os.makedirs applies the global umask, so we reset it,\n+                # for consistency with file_permissions_mode behavior.\n+                old_umask = os.umask(0)\n+                try:\n+                    os.makedirs(directory, self.directory_permissions_mode, exist_ok=True)\n+                finally:\n+                    os.umask(old_umask)\n+            else:\n+                os.makedirs(directory, exist_ok=True)\n+        except FileExistsError:\n+            raise FileExistsError('%s exists and is not a directory.' % directory)\n+\n+        # There's a potential race condition between get_available_name and\n+        # saving the file; it's possible that two threads might return the\n+        # same name, at which point all sorts of fun happens. So we need to\n+        # try to create the file, but if it already exists we have to go back\n+        # to get_available_name() and try again.\n+\n+        while True:\n+            try:\n+                # This file has a file path that we can move.\n+                if hasattr(content, 'temporary_file_path'):\n+                    file_move_safe(content.temporary_file_path(), full_path)\n+\n+                # This is a normal uploadedfile that we can stream.\n+                else:\n+                    # The current umask value is masked out by os.open!\n+                    fd = os.open(full_path, self.OS_OPEN_FLAGS, 0o666)\n+                    _file = None\n+                    try:\n+                        locks.lock(fd, locks.LOCK_EX)\n+                        for chunk in content.chunks():\n+                            if _file is None:\n+                                mode = 'wb' if isinstance(chunk, bytes) else 'wt'\n+                                _file = os.fdopen(fd, mode)\n+                            _file.write(chunk)\n+                    finally:\n+                        locks.unlock(fd)\n+                        if _file is not None:\n+                            _file.close()\n+                        else:\n+                            os.close(fd)\n+            except FileExistsError:\n+                # A new name is needed if the file exists.\n+                name = self.get_available_name(name)\n+                full_path = self.path(name)\n+            else:\n+                # OK, the file save worked. Break out of the loop.\n+                break\n+\n+        if self.file_permissions_mode is not None:\n+            os.chmod(full_path, self.file_permissions_mode)\n+\n+        # Store filenames with forward slashes, even on Windows.\n+        return name.replace('\\\\', '/')\n+\n+    def delete(self, name):\n+        assert name, \"The name argument is not allowed to be empty.\"\n+        name = self.path(name)\n+        # If the file or directory exists, delete it from the filesystem.\n+        try:\n+            if os.path.isdir(name):\n+                os.rmdir(name)\n+            else:\n+                os.remove(name)\n+        except FileNotFoundError:\n+            # FileNotFoundError is raised if the file or directory was removed\n+            # concurrently.\n+            pass\n+\n+    def exists(self, name):\n+        return os.path.exists(self.path(name))\n+\n+    def listdir(self, path):\n+        path = self.path(path)\n+        directories, files = [], []\n+        for entry in os.scandir(path):\n+            if entry.is_dir():\n+                directories.append(entry.name)\n+            else:\n+                files.append(entry.name)\n+        return directories, files\n+\n+    def path(self, name):\n+        return safe_join(self.location, name)\n+\n+    def size(self, name):\n+        return os.path.getsize(self.path(name))\n+\n+    def url(self, name):\n+        if self.base_url is None:\n+            raise ValueError(\"This file is not accessible via a URL.\")\n+        url = filepath_to_uri(name)\n+        if url is not None:\n+            url = url.lstrip('/')\n+        return urljoin(self.base_url, url)\n+\n+    def _datetime_from_timestamp(self, ts):\n+        \"\"\"\n+        If timezone support is enabled, make an aware datetime object in UTC;\n+        otherwise make a naive one in the local timezone.\n+        \"\"\"\n+        if settings.USE_TZ:\n+            # Safe to use .replace() because UTC doesn't have DST\n+            return datetime.utcfromtimestamp(ts).replace(tzinfo=timezone.utc)\n+        else:\n+            return datetime.fromtimestamp(ts)\n+\n+    def get_accessed_time(self, name):\n+        return self._datetime_from_timestamp(os.path.getatime(self.path(name)))\n+\n+    def get_created_time(self, name):\n+        return self._datetime_from_timestamp(os.path.getctime(self.path(name)))\n+\n+    def get_modified_time(self, name):\n+        return self._datetime_from_timestamp(os.path.getmtime(self.path(name)))\n+\n+\n+def get_storage_class(import_path=None):\n+    return import_string(import_path or settings.DEFAULT_FILE_STORAGE)\n+\n+\n+class DefaultStorage(LazyObject):\n+    def _setup(self):\n+        self._wrapped = get_storage_class()()\n+\n+\n+default_storage = DefaultStorage()\ndiff --git a/.harness/checkpoints/6aa09a87f073/--Users--huetuanthi--dev--agents--harness--django--templatetags--static.py b/.harness/checkpoints/6aa09a87f073/--Users--huetuanthi--dev--agents--harness--django--templatetags--static.py\nnew file mode 100644\nindex 0000000000..39712aaba0\n--- /dev/null\n+++ b/.harness/checkpoints/6aa09a87f073/--Users--huetuanthi--dev--agents--harness--django--templatetags--static.py\n@@ -0,0 +1,167 @@\n+from urllib.parse import quote, urljoin\n+\n+from django import template\n+from django.apps import apps\n+from django.utils.encoding import iri_to_uri\n+from django.utils.html import conditional_escape\n+\n+register = template.Library()\n+\n+\n+class PrefixNode(template.Node):\n+\n+    def __repr__(self):\n+        return \"<PrefixNode for %r>\" % self.name\n+\n+    def __init__(self, varname=None, name=None):\n+        if name is None:\n+            raise template.TemplateSyntaxError(\n+                \"Prefix nodes must be given a name to return.\")\n+        self.varname = varname\n+        self.name = name\n+\n+    @classmethod\n+    def handle_token(cls, parser, token, name):\n+        \"\"\"\n+        Class method to parse prefix node and return a Node.\n+        \"\"\"\n+        # token.split_contents() isn't useful here because tags using this method don't accept variable as arguments\n+        tokens = token.contents.split()\n+        if len(tokens) > 1 and tokens[1] != 'as':\n+            raise template.TemplateSyntaxError(\n+                \"First argument in '%s' must be 'as'\" % tokens[0])\n+        if len(tokens) > 1:\n+            varname = tokens[2]\n+        else:\n+            varname = None\n+        return cls(varname, name)\n+\n+    @classmethod\n+    def handle_simple(cls, name):\n+        try:\n+            from django.conf import settings\n+        except ImportError:\n+            prefix = ''\n+        else:\n+            prefix = iri_to_uri(getattr(settings, name, ''))\n+        return prefix\n+\n+    def render(self, context):\n+        prefix = self.handle_simple(self.name)\n+        if self.varname is None:\n+            return prefix\n+        context[self.varname] = prefix\n+        return ''\n+\n+\n+@register.tag\n+def get_static_prefix(parser, token):\n+    \"\"\"\n+    Populate a template variable with the static prefix,\n+    ``settings.STATIC_URL``.\n+\n+    Usage::\n+\n+        {% get_static_prefix [as varname] %}\n+\n+    Examples::\n+\n+        {% get_static_prefix %}\n+        {% get_static_prefix as static_prefix %}\n+    \"\"\"\n+    return PrefixNode.handle_token(parser, token, \"STATIC_URL\")\n+\n+\n+@register.tag\n+def get_media_prefix(parser, token):\n+    \"\"\"\n+    Populate a template variable with the media prefix,\n+    ``settings.MEDIA_URL``.\n+\n+    Usage::\n+\n+        {% get_media_prefix [as varname] %}\n+\n+    Examples::\n+\n+        {% get_media_prefix %}\n+        {% get_media_prefix as media_prefix %}\n+    \"\"\"\n+    return PrefixNode.handle_token(parser, token, \"MEDIA_URL\")\n+\n+\n+class StaticNode(template.Node):\n+    def __init__(self, varname=None, path=None):\n+        if path is None:\n+            raise template.TemplateSyntaxError(\n+                \"Static template nodes must be given a path to return.\")\n+        self.path = path\n+        self.varname = varname\n+\n+    def url(self, context):\n+        path = self.path.resolve(context)\n+        return self.handle_simple(path)\n+\n+    def render(self, context):\n+        url = self.url(context)\n+        if context.autoescape:\n+            url = conditional_escape(url)\n+        if self.varname is None:\n+            return url\n+        context[self.varname] = url\n+        return ''\n+\n+    @classmethod\n+    def handle_simple(cls, path):\n+        if apps.is_installed('django.contrib.staticfiles'):\n+            from django.contrib.staticfiles.storage import staticfiles_storage\n+            return staticfiles_storage.url(path)\n+        else:\n+            return urljoin(PrefixNode.handle_simple(\"STATIC_URL\"), quote(path))\n+\n+    @classmethod\n+    def handle_token(cls, parser, token):\n+        \"\"\"\n+        Class method to parse prefix node and return a Node.\n+        \"\"\"\n+        bits = token.split_contents()\n+\n+        if len(bits) < 2:\n+            raise template.TemplateSyntaxError(\n+                \"'%s' takes at least one argument (path to file)\" % bits[0])\n+\n+        path = parser.compile_filter(bits[1])\n+\n+        if len(bits) >= 2 and bits[-2] == 'as':\n+            varname = bits[3]\n+        else:\n+            varname = None\n+\n+        return cls(varname, path)\n+\n+\n+@register.tag('static')\n+def do_static(parser, token):\n+    \"\"\"\n+    Join the given path with the STATIC_URL setting.\n+\n+    Usage::\n+\n+        {% static path [as varname] %}\n+\n+    Examples::\n+\n+        {% static \"myapp/css/base.css\" %}\n+        {% static variable_with_path %}\n+        {% static \"myapp/css/base.css\" as admin_base_css %}\n+        {% static variable_with_path as varname %}\n+    \"\"\"\n+    return StaticNode.handle_token(parser, token)\n+\n+\n+def static(path):\n+    \"\"\"\n+    Given a relative path to a static asset, return the absolute path to the\n+    asset.\n+    \"\"\"\n+    return StaticNode.handle_simple(path)\ndiff --git a/django/core/files/storage.py b/django/core/files/storage.py\nindex 4c27fce605..4c6c07b8b9 100644\n--- a/django/core/files/storage.py\n+++ b/django/core/files/storage.py\n@@ -332,7 +332,14 @@ class FileSystemStorage(Storage):\n         url = filepath_to_uri(name)\n         if url is not None:\n             url = url.lstrip('/')\n-        return urljoin(self.base_url, url)\n+        url = urljoin(self.base_url, url)\n+        if url.startswith('/') and not url.startswith(('http://', 'https://')):\n+            # Avoid circular import by deferring the import of get_script_prefix.\n+            from django.urls import get_script_prefix\n+            script_prefix = get_script_prefix()\n+            if script_prefix and not url.startswith(script_prefix):\n+                url = script_prefix.rstrip('/') + url\n+        return url\n \n     def _datetime_from_timestamp(self, ts):\n         \"\"\"\ndiff --git a/django/templatetags/static.py b/django/templatetags/static.py\nindex 39712aaba0..1fdc9dcb42 100644\n--- a/django/templatetags/static.py\n+++ b/django/templatetags/static.py\n@@ -40,10 +40,15 @@ class PrefixNode(template.Node):\n     def handle_simple(cls, name):\n         try:\n             from django.conf import settings\n+            from django.urls import get_script_prefix\n         except ImportError:\n             prefix = ''\n         else:\n             prefix = iri_to_uri(getattr(settings, name, ''))\n+            script_prefix = get_script_prefix()\n+            if script_prefix and prefix.startswith('/') and not prefix.startswith(('http://', 'https://')):\n+                if not prefix.startswith(script_prefix):\n+                    prefix = script_prefix.rstrip('/') + prefix\n         return prefix\n \n     def render(self, context):\ndiff --git a/tests/file_storage/tests.py b/tests/file_storage/tests.py\nindex 1c4176014c..ed9d71d6cd 100644\n--- a/tests/file_storage/tests.py\n+++ b/tests/file_storage/tests.py\n@@ -21,7 +21,7 @@ from django.test import (\n     LiveServerTestCase, SimpleTestCase, TestCase, override_settings,\n )\n from django.test.utils import requires_tz_support\n-from django.urls import NoReverseMatch, reverse_lazy\n+from django.urls import NoReverseMatch, clear_script_prefix, reverse_lazy, set_script_prefix\n from django.utils import timezone\n \n from .models import Storage, temp_storage, temp_storage_location\n@@ -359,6 +359,56 @@ class FileStorageTests(SimpleTestCase):\n             '%s%s' % (storage.base_url, 'test.file')\n         )\n \n+    def test_file_url_script_prefix(self):\n+        \"\"\"\n+        FileSystemStorage.url() prepends SCRIPT_NAME to the URL when the\n+        application is deployed on a sub-path.\n+        \"\"\"\n+        set_script_prefix('/subpath/')\n+        try:\n+            self.assertEqual(\n+                self.storage.url('test.file'),\n+                '/subpath/test_media_url/test.file',\n+            )\n+        finally:\n+            clear_script_prefix()\n+\n+    def test_file_url_script_prefix_not_double_prefixed(self):\n+        \"\"\"\n+        FileSystemStorage.url() does not prepend SCRIPT_NAME when the\n+        base_url already starts with it.\n+        \"\"\"\n+        storage = self.storage_class(\n+            location=self.temp_dir,\n+            base_url='/subpath/test_media_url/',\n+        )\n+        set_script_prefix('/subpath/')\n+        try:\n+            self.assertEqual(\n+                storage.url('test.file'),\n+                '/subpath/test_media_url/test.file',\n+            )\n+        finally:\n+            clear_script_prefix()\n+\n+    def test_file_url_absolute_base_url(self):\n+        \"\"\"\n+        FileSystemStorage.url() does not prepend SCRIPT_NAME when base_url is\n+        an absolute URL (e.g. a CDN).\n+        \"\"\"\n+        storage = self.storage_class(\n+            location=self.temp_dir,\n+            base_url='https://cdn.example.com/media/',\n+        )\n+        set_script_prefix('/subpath/')\n+        try:\n+            self.assertEqual(\n+                storage.url('test.file'),\n+                'https://cdn.example.com/media/test.file',\n+            )\n+        finally:\n+            clear_script_prefix()\n+\n     def test_listdir(self):\n         \"\"\"\n         File storage returns a tuple containing directories and files.\ndiff --git a/tests/staticfiles_tests/test_templatetags.py b/tests/staticfiles_tests/test_templatetags.py\nindex cd3f7d4bfa..5b1b8ab15d 100644\n--- a/tests/staticfiles_tests/test_templatetags.py\n+++ b/tests/staticfiles_tests/test_templatetags.py\n@@ -1,4 +1,5 @@\n from django.test import override_settings\n+from django.urls import set_script_prefix, clear_script_prefix\n \n from .cases import StaticFilesTestCase\n \n@@ -19,3 +20,42 @@ class TestTemplateTag(StaticFilesTestCase):\n         \"\"\"\n         self.assertStaticRenders('a.html', 'a.html?a=b&amp;c=d')\n         self.assertStaticRenders('a.html', 'a.html?a=b&c=d', autoescape=False)\n+\n+    def test_template_tag_script_prefix(self):\n+        \"\"\"\n+        {% static %} prepends SCRIPT_NAME to the URL when the app is deployed\n+        on a sub-path.\n+        \"\"\"\n+        set_script_prefix('/subpath/')\n+        try:\n+            self.assertStaticRenders(\"does/not/exist.png\", \"/subpath/static/does/not/exist.png\")\n+            self.assertStaticRenders(\"testfile.txt\", \"/subpath/static/testfile.txt\")\n+        finally:\n+            clear_script_prefix()\n+\n+    def test_template_tag_script_prefix_not_double_prefixed(self):\n+        \"\"\"\n+        {% static %} does not double-prepend SCRIPT_NAME when STATIC_URL\n+        already starts with it.\n+        \"\"\"\n+        set_script_prefix('/subpath/')\n+        try:\n+            with self.settings(STATIC_URL='/subpath/static/'):\n+                self.assertStaticRenders(\"testfile.txt\", \"/subpath/static/testfile.txt\")\n+        finally:\n+            clear_script_prefix()\n+\n+    def test_template_tag_absolute_static_url(self):\n+        \"\"\"\n+        {% static %} does not prepend SCRIPT_NAME when STATIC_URL is an\n+        absolute URL (e.g. CDN).\n+        \"\"\"\n+        set_script_prefix('/subpath/')\n+        try:\n+            with self.settings(STATIC_URL='https://cdn.example.com/static/'):\n+                self.assertStaticRenders(\n+                    \"testfile.txt\",\n+                    \"https://cdn.example.com/static/testfile.txt\",\n+                )\n+        finally:\n+            clear_script_prefix()\ndiff --git a/tests/template_tests/syntax_tests/test_static.py b/tests/template_tests/syntax_tests/test_static.py\nindex 00f8cdbc11..7bb8b4d3aa 100644\n--- a/tests/template_tests/syntax_tests/test_static.py\n+++ b/tests/template_tests/syntax_tests/test_static.py\n@@ -3,6 +3,7 @@ from urllib.parse import urljoin\n from django.conf import settings\n from django.template import TemplateSyntaxError\n from django.test import SimpleTestCase, override_settings\n+from django.urls import clear_script_prefix, set_script_prefix\n \n from ..utils import setup\n \n@@ -69,3 +70,56 @@ class StaticTagTests(SimpleTestCase):\n         msg = \"'static' takes at least one argument (path to file)\"\n         with self.assertRaisesMessage(TemplateSyntaxError, msg):\n             self.engine.render_to_string('t')\n+\n+    @setup({'t': '{% load static %}{% get_static_prefix %}'})\n+    def test_get_static_prefix_script_prefix(self):\n+        \"\"\"\n+        {% get_static_prefix %} prepends SCRIPT_NAME to STATIC_URL when the\n+        app is deployed on a sub-path.\n+        \"\"\"\n+        set_script_prefix('/subpath/')\n+        try:\n+            output = self.engine.render_to_string('t')\n+            self.assertEqual(output, '/subpath/static/')\n+        finally:\n+            clear_script_prefix()\n+\n+    @setup({'t': '{% load static %}{% get_media_prefix %}'})\n+    def test_get_media_prefix_script_prefix(self):\n+        \"\"\"\n+        {% get_media_prefix %} prepends SCRIPT_NAME to MEDIA_URL when the\n+        app is deployed on a sub-path.\n+        \"\"\"\n+        set_script_prefix('/subpath/')\n+        try:\n+            output = self.engine.render_to_string('t')\n+            self.assertEqual(output, '/subpath/media/')\n+        finally:\n+            clear_script_prefix()\n+\n+    @setup({'t': '{% load static %}{% get_static_prefix %}'})\n+    def test_get_static_prefix_not_double_prefixed(self):\n+        \"\"\"\n+        {% get_static_prefix %} does not double-prepend SCRIPT_NAME when\n+        STATIC_URL already starts with it.\n+        \"\"\"\n+        set_script_prefix('/subpath/')\n+        try:\n+            with override_settings(STATIC_URL='/subpath/static/'):\n+                output = self.engine.render_to_string('t')\n+            self.assertEqual(output, '/subpath/static/')\n+        finally:\n+            clear_script_prefix()\n+\n+    @setup({'t': '{% load static %}{% static \"admin/base.css\" %}'})\n+    def test_static_tag_script_prefix(self):\n+        \"\"\"\n+        {% static %} (without staticfiles installed) prepends SCRIPT_NAME\n+        to the URL when the app is deployed on a sub-path.\n+        \"\"\"\n+        set_script_prefix('/subpath/')\n+        try:\n+            output = self.engine.render_to_string('t')\n+            self.assertEqual(output, '/subpath/static/admin/base.css')\n+        finally:\n+            clear_script_prefix()\n"}
{"instance_id": "django__django-11583", "model_name_or_path": "harness/claude-sonnet-4-6", "model_patch": "diff --git a/.harness/checkpoints/5ea16dd0b0ae/--Users--huetuanthi--dev--agents--harness--django--utils--autoreload.py b/.harness/checkpoints/5ea16dd0b0ae/--Users--huetuanthi--dev--agents--harness--django--utils--autoreload.py\nnew file mode 100644\nindex 0000000000..25c3b44250\n--- /dev/null\n+++ b/.harness/checkpoints/5ea16dd0b0ae/--Users--huetuanthi--dev--agents--harness--django--utils--autoreload.py\n@@ -0,0 +1,600 @@\n+import functools\n+import itertools\n+import logging\n+import os\n+import signal\n+import subprocess\n+import sys\n+import threading\n+import time\n+import traceback\n+import weakref\n+from collections import defaultdict\n+from pathlib import Path\n+from types import ModuleType\n+from zipimport import zipimporter\n+\n+from django.apps import apps\n+from django.core.signals import request_finished\n+from django.dispatch import Signal\n+from django.utils.functional import cached_property\n+from django.utils.version import get_version_tuple\n+\n+autoreload_started = Signal()\n+file_changed = Signal(providing_args=['file_path', 'kind'])\n+\n+DJANGO_AUTORELOAD_ENV = 'RUN_MAIN'\n+\n+logger = logging.getLogger('django.utils.autoreload')\n+\n+# If an error is raised while importing a file, it's not placed in sys.modules.\n+# This means that any future modifications aren't caught. Keep a list of these\n+# file paths to allow watching them in the future.\n+_error_files = []\n+_exception = None\n+\n+try:\n+    import termios\n+except ImportError:\n+    termios = None\n+\n+\n+try:\n+    import pywatchman\n+except ImportError:\n+    pywatchman = None\n+\n+\n+def check_errors(fn):\n+    @functools.wraps(fn)\n+    def wrapper(*args, **kwargs):\n+        global _exception\n+        try:\n+            fn(*args, **kwargs)\n+        except Exception:\n+            _exception = sys.exc_info()\n+\n+            et, ev, tb = _exception\n+\n+            if getattr(ev, 'filename', None) is None:\n+                # get the filename from the last item in the stack\n+                filename = traceback.extract_tb(tb)[-1][0]\n+            else:\n+                filename = ev.filename\n+\n+            if filename not in _error_files:\n+                _error_files.append(filename)\n+\n+            raise\n+\n+    return wrapper\n+\n+\n+def raise_last_exception():\n+    global _exception\n+    if _exception is not None:\n+        raise _exception[1]\n+\n+\n+def ensure_echo_on():\n+    \"\"\"\n+    Ensure that echo mode is enabled. Some tools such as PDB disable\n+    it which causes usability issues after reload.\n+    \"\"\"\n+    if not termios or not sys.stdin.isatty():\n+        return\n+    attr_list = termios.tcgetattr(sys.stdin)\n+    if not attr_list[3] & termios.ECHO:\n+        attr_list[3] |= termios.ECHO\n+        if hasattr(signal, 'SIGTTOU'):\n+            old_handler = signal.signal(signal.SIGTTOU, signal.SIG_IGN)\n+        else:\n+            old_handler = None\n+        termios.tcsetattr(sys.stdin, termios.TCSANOW, attr_list)\n+        if old_handler is not None:\n+            signal.signal(signal.SIGTTOU, old_handler)\n+\n+\n+def iter_all_python_module_files():\n+    # This is a hot path during reloading. Create a stable sorted list of\n+    # modules based on the module name and pass it to iter_modules_and_files().\n+    # This ensures cached results are returned in the usual case that modules\n+    # aren't loaded on the fly.\n+    keys = sorted(sys.modules)\n+    modules = tuple(m for m in map(sys.modules.__getitem__, keys) if not isinstance(m, weakref.ProxyTypes))\n+    return iter_modules_and_files(modules, frozenset(_error_files))\n+\n+\n+@functools.lru_cache(maxsize=1)\n+def iter_modules_and_files(modules, extra_files):\n+    \"\"\"Iterate through all modules needed to be watched.\"\"\"\n+    sys_file_paths = []\n+    for module in modules:\n+        # During debugging (with PyDev) the 'typing.io' and 'typing.re' objects\n+        # are added to sys.modules, however they are types not modules and so\n+        # cause issues here.\n+        if not isinstance(module, ModuleType):\n+            continue\n+        if module.__name__ == '__main__':\n+            # __main__ (usually manage.py) doesn't always have a __spec__ set.\n+            # Handle this by falling back to using __file__, resolved below.\n+            # See https://docs.python.org/reference/import.html#main-spec\n+            # __file__ may not exists, e.g. when running ipdb debugger.\n+            if hasattr(module, '__file__'):\n+                sys_file_paths.append(module.__file__)\n+            continue\n+        if getattr(module, '__spec__', None) is None:\n+            continue\n+        spec = module.__spec__\n+        # Modules could be loaded from places without a concrete location. If\n+        # this is the case, skip them.\n+        if spec.has_location:\n+            origin = spec.loader.archive if isinstance(spec.loader, zipimporter) else spec.origin\n+            sys_file_paths.append(origin)\n+\n+    results = set()\n+    for filename in itertools.chain(sys_file_paths, extra_files):\n+        if not filename:\n+            continue\n+        path = Path(filename)\n+        try:\n+            resolved_path = path.resolve(strict=True).absolute()\n+        except FileNotFoundError:\n+            # The module could have been removed, don't fail loudly if this\n+            # is the case.\n+            continue\n+        results.add(resolved_path)\n+    return frozenset(results)\n+\n+\n+@functools.lru_cache(maxsize=1)\n+def common_roots(paths):\n+    \"\"\"\n+    Return a tuple of common roots that are shared between the given paths.\n+    File system watchers operate on directories and aren't cheap to create.\n+    Try to find the minimum set of directories to watch that encompass all of\n+    the files that need to be watched.\n+    \"\"\"\n+    # Inspired from Werkzeug:\n+    # https://github.com/pallets/werkzeug/blob/7477be2853df70a022d9613e765581b9411c3c39/werkzeug/_reloader.py\n+    # Create a sorted list of the path components, longest first.\n+    path_parts = sorted([x.parts for x in paths], key=len, reverse=True)\n+    tree = {}\n+    for chunks in path_parts:\n+        node = tree\n+        # Add each part of the path to the tree.\n+        for chunk in chunks:\n+            node = node.setdefault(chunk, {})\n+        # Clear the last leaf in the tree.\n+        node.clear()\n+\n+    # Turn the tree into a list of Path instances.\n+    def _walk(node, path):\n+        for prefix, child in node.items():\n+            yield from _walk(child, path + (prefix,))\n+        if not node:\n+            yield Path(*path)\n+\n+    return tuple(_walk(tree, ()))\n+\n+\n+def sys_path_directories():\n+    \"\"\"\n+    Yield absolute directories from sys.path, ignoring entries that don't\n+    exist.\n+    \"\"\"\n+    for path in sys.path:\n+        path = Path(path)\n+        try:\n+            resolved_path = path.resolve(strict=True).absolute()\n+        except FileNotFoundError:\n+            continue\n+        # If the path is a file (like a zip file), watch the parent directory.\n+        if resolved_path.is_file():\n+            yield resolved_path.parent\n+        else:\n+            yield resolved_path\n+\n+\n+def get_child_arguments():\n+    \"\"\"\n+    Return the executable. This contains a workaround for Windows if the\n+    executable is reported to not have the .exe extension which can cause bugs\n+    on reloading.\n+    \"\"\"\n+    import django.__main__\n+\n+    args = [sys.executable] + ['-W%s' % o for o in sys.warnoptions]\n+    if sys.argv[0] == django.__main__.__file__:\n+        # The server was started with `python -m django runserver`.\n+        args += ['-m', 'django']\n+        args += sys.argv[1:]\n+    else:\n+        args += sys.argv\n+    return args\n+\n+\n+def trigger_reload(filename):\n+    logger.info('%s changed, reloading.', filename)\n+    sys.exit(3)\n+\n+\n+def restart_with_reloader():\n+    new_environ = {**os.environ, DJANGO_AUTORELOAD_ENV: 'true'}\n+    args = get_child_arguments()\n+    while True:\n+        exit_code = subprocess.call(args, env=new_environ, close_fds=False)\n+        if exit_code != 3:\n+            return exit_code\n+\n+\n+class BaseReloader:\n+    def __init__(self):\n+        self.extra_files = set()\n+        self.directory_globs = defaultdict(set)\n+        self._stop_condition = threading.Event()\n+\n+    def watch_dir(self, path, glob):\n+        path = Path(path)\n+        if not path.is_absolute():\n+            raise ValueError('%s must be absolute.' % path)\n+        logger.debug('Watching dir %s with glob %s.', path, glob)\n+        self.directory_globs[path].add(glob)\n+\n+    def watch_file(self, path):\n+        path = Path(path)\n+        if not path.is_absolute():\n+            raise ValueError('%s must be absolute.' % path)\n+        logger.debug('Watching file %s.', path)\n+        self.extra_files.add(path)\n+\n+    def watched_files(self, include_globs=True):\n+        \"\"\"\n+        Yield all files that need to be watched, including module files and\n+        files within globs.\n+        \"\"\"\n+        yield from iter_all_python_module_files()\n+        yield from self.extra_files\n+        if include_globs:\n+            for directory, patterns in self.directory_globs.items():\n+                for pattern in patterns:\n+                    yield from directory.glob(pattern)\n+\n+    def wait_for_apps_ready(self, app_reg, django_main_thread):\n+        \"\"\"\n+        Wait until Django reports that the apps have been loaded. If the given\n+        thread has terminated before the apps are ready, then a SyntaxError or\n+        other non-recoverable error has been raised. In that case, stop waiting\n+        for the apps_ready event and continue processing.\n+\n+        Return True if the thread is alive and the ready event has been\n+        triggered, or False if the thread is terminated while waiting for the\n+        event.\n+        \"\"\"\n+        while django_main_thread.is_alive():\n+            if app_reg.ready_event.wait(timeout=0.1):\n+                return True\n+        else:\n+            logger.debug('Main Django thread has terminated before apps are ready.')\n+            return False\n+\n+    def run(self, django_main_thread):\n+        logger.debug('Waiting for apps ready_event.')\n+        self.wait_for_apps_ready(apps, django_main_thread)\n+        from django.urls import get_resolver\n+        # Prevent a race condition where URL modules aren't loaded when the\n+        # reloader starts by accessing the urlconf_module property.\n+        try:\n+            get_resolver().urlconf_module\n+        except Exception:\n+            # Loading the urlconf can result in errors during development.\n+            # If this occurs then swallow the error and continue.\n+            pass\n+        logger.debug('Apps ready_event triggered. Sending autoreload_started signal.')\n+        autoreload_started.send(sender=self)\n+        self.run_loop()\n+\n+    def run_loop(self):\n+        ticker = self.tick()\n+        while not self.should_stop:\n+            try:\n+                next(ticker)\n+            except StopIteration:\n+                break\n+        self.stop()\n+\n+    def tick(self):\n+        \"\"\"\n+        This generator is called in a loop from run_loop. It's important that\n+        the method takes care of pausing or otherwise waiting for a period of\n+        time. This split between run_loop() and tick() is to improve the\n+        testability of the reloader implementations by decoupling the work they\n+        do from the loop.\n+        \"\"\"\n+        raise NotImplementedError('subclasses must implement tick().')\n+\n+    @classmethod\n+    def check_availability(cls):\n+        raise NotImplementedError('subclasses must implement check_availability().')\n+\n+    def notify_file_changed(self, path):\n+        results = file_changed.send(sender=self, file_path=path)\n+        logger.debug('%s notified as changed. Signal results: %s.', path, results)\n+        if not any(res[1] for res in results):\n+            trigger_reload(path)\n+\n+    # These are primarily used for testing.\n+    @property\n+    def should_stop(self):\n+        return self._stop_condition.is_set()\n+\n+    def stop(self):\n+        self._stop_condition.set()\n+\n+\n+class StatReloader(BaseReloader):\n+    SLEEP_TIME = 1  # Check for changes once per second.\n+\n+    def tick(self):\n+        mtimes = {}\n+        while True:\n+            for filepath, mtime in self.snapshot_files():\n+                old_time = mtimes.get(filepath)\n+                mtimes[filepath] = mtime\n+                if old_time is None:\n+                    logger.debug('File %s first seen with mtime %s', filepath, mtime)\n+                    continue\n+                elif mtime > old_time:\n+                    logger.debug('File %s previous mtime: %s, current mtime: %s', filepath, old_time, mtime)\n+                    self.notify_file_changed(filepath)\n+\n+            time.sleep(self.SLEEP_TIME)\n+            yield\n+\n+    def snapshot_files(self):\n+        # watched_files may produce duplicate paths if globs overlap.\n+        seen_files = set()\n+        for file in self.watched_files():\n+            if file in seen_files:\n+                continue\n+            try:\n+                mtime = file.stat().st_mtime\n+            except OSError:\n+                # This is thrown when the file does not exist.\n+                continue\n+            seen_files.add(file)\n+            yield file, mtime\n+\n+    @classmethod\n+    def check_availability(cls):\n+        return True\n+\n+\n+class WatchmanUnavailable(RuntimeError):\n+    pass\n+\n+\n+class WatchmanReloader(BaseReloader):\n+    def __init__(self):\n+        self.roots = defaultdict(set)\n+        self.processed_request = threading.Event()\n+        self.client_timeout = int(os.environ.get('DJANGO_WATCHMAN_TIMEOUT', 5))\n+        super().__init__()\n+\n+    @cached_property\n+    def client(self):\n+        return pywatchman.client(timeout=self.client_timeout)\n+\n+    def _watch_root(self, root):\n+        # In practice this shouldn't occur, however, it's possible that a\n+        # directory that doesn't exist yet is being watched. If it's outside of\n+        # sys.path then this will end up a new root. How to handle this isn't\n+        # clear: Not adding the root will likely break when subscribing to the\n+        # changes, however, as this is currently an internal API,  no files\n+        # will be being watched outside of sys.path. Fixing this by checking\n+        # inside watch_glob() and watch_dir() is expensive, instead this could\n+        # could fall back to the StatReloader if this case is detected? For\n+        # now, watching its parent, if possible, is sufficient.\n+        if not root.exists():\n+            if not root.parent.exists():\n+                logger.warning('Unable to watch root dir %s as neither it or its parent exist.', root)\n+                return\n+            root = root.parent\n+        result = self.client.query('watch-project', str(root.absolute()))\n+        if 'warning' in result:\n+            logger.warning('Watchman warning: %s', result['warning'])\n+        logger.debug('Watchman watch-project result: %s', result)\n+        return result['watch'], result.get('relative_path')\n+\n+    @functools.lru_cache()\n+    def _get_clock(self, root):\n+        return self.client.query('clock', root)['clock']\n+\n+    def _subscribe(self, directory, name, expression):\n+        root, rel_path = self._watch_root(directory)\n+        query = {\n+            'expression': expression,\n+            'fields': ['name'],\n+            'since': self._get_clock(root),\n+            'dedup_results': True,\n+        }\n+        if rel_path:\n+            query['relative_root'] = rel_path\n+        logger.debug('Issuing watchman subscription %s, for root %s. Query: %s', name, root, query)\n+        self.client.query('subscribe', root, name, query)\n+\n+    def _subscribe_dir(self, directory, filenames):\n+        if not directory.exists():\n+            if not directory.parent.exists():\n+                logger.warning('Unable to watch directory %s as neither it or its parent exist.', directory)\n+                return\n+            prefix = 'files-parent-%s' % directory.name\n+            filenames = ['%s/%s' % (directory.name, filename) for filename in filenames]\n+            directory = directory.parent\n+            expression = ['name', filenames, 'wholename']\n+        else:\n+            prefix = 'files'\n+            expression = ['name', filenames]\n+        self._subscribe(directory, '%s:%s' % (prefix, directory), expression)\n+\n+    def _watch_glob(self, directory, patterns):\n+        \"\"\"\n+        Watch a directory with a specific glob. If the directory doesn't yet\n+        exist, attempt to watch the parent directory and amend the patterns to\n+        include this. It's important this method isn't called more than one per\n+        directory when updating all subscriptions. Subsequent calls will\n+        overwrite the named subscription, so it must include all possible glob\n+        expressions.\n+        \"\"\"\n+        prefix = 'glob'\n+        if not directory.exists():\n+            if not directory.parent.exists():\n+                logger.warning('Unable to watch directory %s as neither it or its parent exist.', directory)\n+                return\n+            prefix = 'glob-parent-%s' % directory.name\n+            patterns = ['%s/%s' % (directory.name, pattern) for pattern in patterns]\n+            directory = directory.parent\n+\n+        expression = ['anyof']\n+        for pattern in patterns:\n+            expression.append(['match', pattern, 'wholename'])\n+        self._subscribe(directory, '%s:%s' % (prefix, directory), expression)\n+\n+    def watched_roots(self, watched_files):\n+        extra_directories = self.directory_globs.keys()\n+        watched_file_dirs = [f.parent for f in watched_files]\n+        sys_paths = list(sys_path_directories())\n+        return frozenset((*extra_directories, *watched_file_dirs, *sys_paths))\n+\n+    def _update_watches(self):\n+        watched_files = list(self.watched_files(include_globs=False))\n+        found_roots = common_roots(self.watched_roots(watched_files))\n+        logger.debug('Watching %s files', len(watched_files))\n+        logger.debug('Found common roots: %s', found_roots)\n+        # Setup initial roots for performance, shortest roots first.\n+        for root in sorted(found_roots):\n+            self._watch_root(root)\n+        for directory, patterns in self.directory_globs.items():\n+            self._watch_glob(directory, patterns)\n+        # Group sorted watched_files by their parent directory.\n+        sorted_files = sorted(watched_files, key=lambda p: p.parent)\n+        for directory, group in itertools.groupby(sorted_files, key=lambda p: p.parent):\n+            # These paths need to be relative to the parent directory.\n+            self._subscribe_dir(directory, [str(p.relative_to(directory)) for p in group])\n+\n+    def update_watches(self):\n+        try:\n+            self._update_watches()\n+        except Exception as ex:\n+            # If the service is still available, raise the original exception.\n+            if self.check_server_status(ex):\n+                raise\n+\n+    def _check_subscription(self, sub):\n+        subscription = self.client.getSubscription(sub)\n+        if not subscription:\n+            return\n+        logger.debug('Watchman subscription %s has results.', sub)\n+        for result in subscription:\n+            # When using watch-project, it's not simple to get the relative\n+            # directory without storing some specific state. Store the full\n+            # path to the directory in the subscription name, prefixed by its\n+            # type (glob, files).\n+            root_directory = Path(result['subscription'].split(':', 1)[1])\n+            logger.debug('Found root directory %s', root_directory)\n+            for file in result.get('files', []):\n+                self.notify_file_changed(root_directory / file)\n+\n+    def request_processed(self, **kwargs):\n+        logger.debug('Request processed. Setting update_watches event.')\n+        self.processed_request.set()\n+\n+    def tick(self):\n+        request_finished.connect(self.request_processed)\n+        self.update_watches()\n+        while True:\n+            if self.processed_request.is_set():\n+                self.update_watches()\n+                self.processed_request.clear()\n+            try:\n+                self.client.receive()\n+            except pywatchman.SocketTimeout:\n+                pass\n+            except pywatchman.WatchmanError as ex:\n+                logger.debug('Watchman error: %s, checking server status.', ex)\n+                self.check_server_status(ex)\n+            else:\n+                for sub in list(self.client.subs.keys()):\n+                    self._check_subscription(sub)\n+            yield\n+\n+    def stop(self):\n+        self.client.close()\n+        super().stop()\n+\n+    def check_server_status(self, inner_ex=None):\n+        \"\"\"Return True if the server is available.\"\"\"\n+        try:\n+            self.client.query('version')\n+        except Exception:\n+            raise WatchmanUnavailable(str(inner_ex)) from inner_ex\n+        return True\n+\n+    @classmethod\n+    def check_availability(cls):\n+        if not pywatchman:\n+            raise WatchmanUnavailable('pywatchman not installed.')\n+        client = pywatchman.client(timeout=0.1)\n+        try:\n+            result = client.capabilityCheck()\n+        except Exception:\n+            # The service is down?\n+            raise WatchmanUnavailable('Cannot connect to the watchman service.')\n+        version = get_version_tuple(result['version'])\n+        # Watchman 4.9 includes multiple improvements to watching project\n+        # directories as well as case insensitive filesystems.\n+        logger.debug('Watchman version %s', version)\n+        if version < (4, 9):\n+            raise WatchmanUnavailable('Watchman 4.9 or later is required.')\n+\n+\n+def get_reloader():\n+    \"\"\"Return the most suitable reloader for this environment.\"\"\"\n+    try:\n+        WatchmanReloader.check_availability()\n+    except WatchmanUnavailable:\n+        return StatReloader()\n+    return WatchmanReloader()\n+\n+\n+def start_django(reloader, main_func, *args, **kwargs):\n+    ensure_echo_on()\n+\n+    main_func = check_errors(main_func)\n+    django_main_thread = threading.Thread(target=main_func, args=args, kwargs=kwargs, name='django-main-thread')\n+    django_main_thread.setDaemon(True)\n+    django_main_thread.start()\n+\n+    while not reloader.should_stop:\n+        try:\n+            reloader.run(django_main_thread)\n+        except WatchmanUnavailable as ex:\n+            # It's possible that the watchman service shuts down or otherwise\n+            # becomes unavailable. In that case, use the StatReloader.\n+            reloader = StatReloader()\n+            logger.error('Error connecting to Watchman: %s', ex)\n+            logger.info('Watching for file changes with %s', reloader.__class__.__name__)\n+\n+\n+def run_with_reloader(main_func, *args, **kwargs):\n+    signal.signal(signal.SIGTERM, lambda *args: sys.exit(0))\n+    try:\n+        if os.environ.get(DJANGO_AUTORELOAD_ENV) == 'true':\n+            reloader = get_reloader()\n+            logger.info('Watching for file changes with %s', reloader.__class__.__name__)\n+            start_django(reloader, main_func, *args, **kwargs)\n+        else:\n+            exit_code = restart_with_reloader()\n+            sys.exit(exit_code)\n+    except KeyboardInterrupt:\n+        pass\ndiff --git a/django/utils/autoreload.py b/django/utils/autoreload.py\nindex 25c3b44250..8536e506a8 100644\n--- a/django/utils/autoreload.py\n+++ b/django/utils/autoreload.py\n@@ -143,6 +143,11 @@ def iter_modules_and_files(modules, extra_files):\n             # The module could have been removed, don't fail loudly if this\n             # is the case.\n             continue\n+        except ValueError:\n+            # ValueError is raised on Windows and some Unix-like systems\n+            # (e.g. when resolving paths on network mounts) if the path\n+            # contains a null byte. Skip such paths silently.\n+            continue\n         results.add(resolved_path)\n     return frozenset(results)\n \n@@ -189,6 +194,11 @@ def sys_path_directories():\n             resolved_path = path.resolve(strict=True).absolute()\n         except FileNotFoundError:\n             continue\n+        except ValueError:\n+            # ValueError is raised on Windows and some Unix-like systems\n+            # (e.g. when resolving paths on network mounts) if the path\n+            # contains a null byte. Skip such paths silently.\n+            continue\n         # If the path is a file (like a zip file), watch the parent directory.\n         if resolved_path.is_file():\n             yield resolved_path.parent\n"}
{"instance_id": "django__django-11620", "model_name_or_path": "harness/claude-sonnet-4-6", "model_patch": "diff --git a/.harness/checkpoints/12d4761652eb/--Users--huetuanthi--dev--agents--harness--django--urls--resolvers.py b/.harness/checkpoints/12d4761652eb/--Users--huetuanthi--dev--agents--harness--django--urls--resolvers.py\nnew file mode 100644\nindex 0000000000..2154a46320\n--- /dev/null\n+++ b/.harness/checkpoints/12d4761652eb/--Users--huetuanthi--dev--agents--harness--django--urls--resolvers.py\n@@ -0,0 +1,673 @@\n+\"\"\"\n+This module converts requested URLs to callback view functions.\n+\n+URLResolver is the main class here. Its resolve() method takes a URL (as\n+a string) and returns a ResolverMatch object which provides access to all\n+attributes of the resolved URL match.\n+\"\"\"\n+import functools\n+import inspect\n+import re\n+from importlib import import_module\n+from urllib.parse import quote\n+\n+from asgiref.local import Local\n+\n+from django.conf import settings\n+from django.core.checks import Error, Warning\n+from django.core.checks.urls import check_resolver\n+from django.core.exceptions import ImproperlyConfigured, ViewDoesNotExist\n+from django.utils.datastructures import MultiValueDict\n+from django.utils.functional import cached_property\n+from django.utils.http import RFC3986_SUBDELIMS, escape_leading_slashes\n+from django.utils.regex_helper import normalize\n+from django.utils.translation import get_language\n+\n+from .converters import get_converter\n+from .exceptions import NoReverseMatch, Resolver404\n+from .utils import get_callable\n+\n+\n+class ResolverMatch:\n+    def __init__(self, func, args, kwargs, url_name=None, app_names=None, namespaces=None, route=None):\n+        self.func = func\n+        self.args = args\n+        self.kwargs = kwargs\n+        self.url_name = url_name\n+        self.route = route\n+\n+        # If a URLRegexResolver doesn't have a namespace or app_name, it passes\n+        # in an empty value.\n+        self.app_names = [x for x in app_names if x] if app_names else []\n+        self.app_name = ':'.join(self.app_names)\n+        self.namespaces = [x for x in namespaces if x] if namespaces else []\n+        self.namespace = ':'.join(self.namespaces)\n+\n+        if not hasattr(func, '__name__'):\n+            # A class-based view\n+            self._func_path = func.__class__.__module__ + '.' + func.__class__.__name__\n+        else:\n+            # A function-based view\n+            self._func_path = func.__module__ + '.' + func.__name__\n+\n+        view_path = url_name or self._func_path\n+        self.view_name = ':'.join(self.namespaces + [view_path])\n+\n+    def __getitem__(self, index):\n+        return (self.func, self.args, self.kwargs)[index]\n+\n+    def __repr__(self):\n+        return \"ResolverMatch(func=%s, args=%s, kwargs=%s, url_name=%s, app_names=%s, namespaces=%s, route=%s)\" % (\n+            self._func_path, self.args, self.kwargs, self.url_name,\n+            self.app_names, self.namespaces, self.route,\n+        )\n+\n+\n+def get_resolver(urlconf=None):\n+    if urlconf is None:\n+        urlconf = settings.ROOT_URLCONF\n+    return _get_cached_resolver(urlconf)\n+\n+\n+@functools.lru_cache(maxsize=None)\n+def _get_cached_resolver(urlconf=None):\n+    return URLResolver(RegexPattern(r'^/'), urlconf)\n+\n+\n+@functools.lru_cache(maxsize=None)\n+def get_ns_resolver(ns_pattern, resolver, converters):\n+    # Build a namespaced resolver for the given parent URLconf pattern.\n+    # This makes it possible to have captured parameters in the parent\n+    # URLconf pattern.\n+    pattern = RegexPattern(ns_pattern)\n+    pattern.converters = dict(converters)\n+    ns_resolver = URLResolver(pattern, resolver.url_patterns)\n+    return URLResolver(RegexPattern(r'^/'), [ns_resolver])\n+\n+\n+class LocaleRegexDescriptor:\n+    def __init__(self, attr):\n+        self.attr = attr\n+\n+    def __get__(self, instance, cls=None):\n+        \"\"\"\n+        Return a compiled regular expression based on the active language.\n+        \"\"\"\n+        if instance is None:\n+            return self\n+        # As a performance optimization, if the given regex string is a regular\n+        # string (not a lazily-translated string proxy), compile it once and\n+        # avoid per-language compilation.\n+        pattern = getattr(instance, self.attr)\n+        if isinstance(pattern, str):\n+            instance.__dict__['regex'] = instance._compile(pattern)\n+            return instance.__dict__['regex']\n+        language_code = get_language()\n+        if language_code not in instance._regex_dict:\n+            instance._regex_dict[language_code] = instance._compile(str(pattern))\n+        return instance._regex_dict[language_code]\n+\n+\n+class CheckURLMixin:\n+    def describe(self):\n+        \"\"\"\n+        Format the URL pattern for display in warning messages.\n+        \"\"\"\n+        description = \"'{}'\".format(self)\n+        if self.name:\n+            description += \" [name='{}']\".format(self.name)\n+        return description\n+\n+    def _check_pattern_startswith_slash(self):\n+        \"\"\"\n+        Check that the pattern does not begin with a forward slash.\n+        \"\"\"\n+        regex_pattern = self.regex.pattern\n+        if not settings.APPEND_SLASH:\n+            # Skip check as it can be useful to start a URL pattern with a slash\n+            # when APPEND_SLASH=False.\n+            return []\n+        if regex_pattern.startswith(('/', '^/', '^\\\\/')) and not regex_pattern.endswith('/'):\n+            warning = Warning(\n+                \"Your URL pattern {} has a route beginning with a '/'. Remove this \"\n+                \"slash as it is unnecessary. If this pattern is targeted in an \"\n+                \"include(), ensure the include() pattern has a trailing '/'.\".format(\n+                    self.describe()\n+                ),\n+                id=\"urls.W002\",\n+            )\n+            return [warning]\n+        else:\n+            return []\n+\n+\n+class RegexPattern(CheckURLMixin):\n+    regex = LocaleRegexDescriptor('_regex')\n+\n+    def __init__(self, regex, name=None, is_endpoint=False):\n+        self._regex = regex\n+        self._regex_dict = {}\n+        self._is_endpoint = is_endpoint\n+        self.name = name\n+        self.converters = {}\n+\n+    def match(self, path):\n+        match = self.regex.search(path)\n+        if match:\n+            # If there are any named groups, use those as kwargs, ignoring\n+            # non-named groups. Otherwise, pass all non-named arguments as\n+            # positional arguments.\n+            kwargs = {k: v for k, v in match.groupdict().items() if v is not None}\n+            args = () if kwargs else match.groups()\n+            return path[match.end():], args, kwargs\n+        return None\n+\n+    def check(self):\n+        warnings = []\n+        warnings.extend(self._check_pattern_startswith_slash())\n+        if not self._is_endpoint:\n+            warnings.extend(self._check_include_trailing_dollar())\n+        return warnings\n+\n+    def _check_include_trailing_dollar(self):\n+        regex_pattern = self.regex.pattern\n+        if regex_pattern.endswith('$') and not regex_pattern.endswith(r'\\$'):\n+            return [Warning(\n+                \"Your URL pattern {} uses include with a route ending with a '$'. \"\n+                \"Remove the dollar from the route to avoid problems including \"\n+                \"URLs.\".format(self.describe()),\n+                id='urls.W001',\n+            )]\n+        else:\n+            return []\n+\n+    def _compile(self, regex):\n+        \"\"\"Compile and return the given regular expression.\"\"\"\n+        try:\n+            return re.compile(regex)\n+        except re.error as e:\n+            raise ImproperlyConfigured(\n+                '\"%s\" is not a valid regular expression: %s' % (regex, e)\n+            )\n+\n+    def __str__(self):\n+        return str(self._regex)\n+\n+\n+_PATH_PARAMETER_COMPONENT_RE = re.compile(\n+    r'<(?:(?P<converter>[^>:]+):)?(?P<parameter>\\w+)>'\n+)\n+\n+\n+def _route_to_regex(route, is_endpoint=False):\n+    \"\"\"\n+    Convert a path pattern into a regular expression. Return the regular\n+    expression and a dictionary mapping the capture names to the converters.\n+    For example, 'foo/<int:pk>' returns '^foo\\\\/(?P<pk>[0-9]+)'\n+    and {'pk': <django.urls.converters.IntConverter>}.\n+    \"\"\"\n+    original_route = route\n+    parts = ['^']\n+    converters = {}\n+    while True:\n+        match = _PATH_PARAMETER_COMPONENT_RE.search(route)\n+        if not match:\n+            parts.append(re.escape(route))\n+            break\n+        parts.append(re.escape(route[:match.start()]))\n+        route = route[match.end():]\n+        parameter = match.group('parameter')\n+        if not parameter.isidentifier():\n+            raise ImproperlyConfigured(\n+                \"URL route '%s' uses parameter name %r which isn't a valid \"\n+                \"Python identifier.\" % (original_route, parameter)\n+            )\n+        raw_converter = match.group('converter')\n+        if raw_converter is None:\n+            # If a converter isn't specified, the default is `str`.\n+            raw_converter = 'str'\n+        try:\n+            converter = get_converter(raw_converter)\n+        except KeyError as e:\n+            raise ImproperlyConfigured(\n+                \"URL route '%s' uses invalid converter %s.\" % (original_route, e)\n+            )\n+        converters[parameter] = converter\n+        parts.append('(?P<' + parameter + '>' + converter.regex + ')')\n+    if is_endpoint:\n+        parts.append('$')\n+    return ''.join(parts), converters\n+\n+\n+class RoutePattern(CheckURLMixin):\n+    regex = LocaleRegexDescriptor('_route')\n+\n+    def __init__(self, route, name=None, is_endpoint=False):\n+        self._route = route\n+        self._regex_dict = {}\n+        self._is_endpoint = is_endpoint\n+        self.name = name\n+        self.converters = _route_to_regex(str(route), is_endpoint)[1]\n+\n+    def match(self, path):\n+        match = self.regex.search(path)\n+        if match:\n+            # RoutePattern doesn't allow non-named groups so args are ignored.\n+            kwargs = match.groupdict()\n+            for key, value in kwargs.items():\n+                converter = self.converters[key]\n+                try:\n+                    kwargs[key] = converter.to_python(value)\n+                except ValueError:\n+                    return None\n+            return path[match.end():], (), kwargs\n+        return None\n+\n+    def check(self):\n+        warnings = self._check_pattern_startswith_slash()\n+        route = self._route\n+        if '(?P<' in route or route.startswith('^') or route.endswith('$'):\n+            warnings.append(Warning(\n+                \"Your URL pattern {} has a route that contains '(?P<', begins \"\n+                \"with a '^', or ends with a '$'. This was likely an oversight \"\n+                \"when migrating to django.urls.path().\".format(self.describe()),\n+                id='2_0.W001',\n+            ))\n+        return warnings\n+\n+    def _compile(self, route):\n+        return re.compile(_route_to_regex(route, self._is_endpoint)[0])\n+\n+    def __str__(self):\n+        return str(self._route)\n+\n+\n+class LocalePrefixPattern:\n+    def __init__(self, prefix_default_language=True):\n+        self.prefix_default_language = prefix_default_language\n+        self.converters = {}\n+\n+    @property\n+    def regex(self):\n+        # This is only used by reverse() and cached in _reverse_dict.\n+        return re.compile(self.language_prefix)\n+\n+    @property\n+    def language_prefix(self):\n+        language_code = get_language() or settings.LANGUAGE_CODE\n+        if language_code == settings.LANGUAGE_CODE and not self.prefix_default_language:\n+            return ''\n+        else:\n+            return '%s/' % language_code\n+\n+    def match(self, path):\n+        language_prefix = self.language_prefix\n+        if path.startswith(language_prefix):\n+            return path[len(language_prefix):], (), {}\n+        return None\n+\n+    def check(self):\n+        return []\n+\n+    def describe(self):\n+        return \"'{}'\".format(self)\n+\n+    def __str__(self):\n+        return self.language_prefix\n+\n+\n+class URLPattern:\n+    def __init__(self, pattern, callback, default_args=None, name=None):\n+        self.pattern = pattern\n+        self.callback = callback  # the view\n+        self.default_args = default_args or {}\n+        self.name = name\n+\n+    def __repr__(self):\n+        return '<%s %s>' % (self.__class__.__name__, self.pattern.describe())\n+\n+    def check(self):\n+        warnings = self._check_pattern_name()\n+        warnings.extend(self.pattern.check())\n+        return warnings\n+\n+    def _check_pattern_name(self):\n+        \"\"\"\n+        Check that the pattern name does not contain a colon.\n+        \"\"\"\n+        if self.pattern.name is not None and \":\" in self.pattern.name:\n+            warning = Warning(\n+                \"Your URL pattern {} has a name including a ':'. Remove the colon, to \"\n+                \"avoid ambiguous namespace references.\".format(self.pattern.describe()),\n+                id=\"urls.W003\",\n+            )\n+            return [warning]\n+        else:\n+            return []\n+\n+    def resolve(self, path):\n+        match = self.pattern.match(path)\n+        if match:\n+            new_path, args, kwargs = match\n+            # Pass any extra_kwargs as **kwargs.\n+            kwargs.update(self.default_args)\n+            return ResolverMatch(self.callback, args, kwargs, self.pattern.name, route=str(self.pattern))\n+\n+    @cached_property\n+    def lookup_str(self):\n+        \"\"\"\n+        A string that identifies the view (e.g. 'path.to.view_function' or\n+        'path.to.ClassBasedView').\n+        \"\"\"\n+        callback = self.callback\n+        if isinstance(callback, functools.partial):\n+            callback = callback.func\n+        if not hasattr(callback, '__name__'):\n+            return callback.__module__ + \".\" + callback.__class__.__name__\n+        return callback.__module__ + \".\" + callback.__qualname__\n+\n+\n+class URLResolver:\n+    def __init__(self, pattern, urlconf_name, default_kwargs=None, app_name=None, namespace=None):\n+        self.pattern = pattern\n+        # urlconf_name is the dotted Python path to the module defining\n+        # urlpatterns. It may also be an object with an urlpatterns attribute\n+        # or urlpatterns itself.\n+        self.urlconf_name = urlconf_name\n+        self.callback = None\n+        self.default_kwargs = default_kwargs or {}\n+        self.namespace = namespace\n+        self.app_name = app_name\n+        self._reverse_dict = {}\n+        self._namespace_dict = {}\n+        self._app_dict = {}\n+        # set of dotted paths to all functions and classes that are used in\n+        # urlpatterns\n+        self._callback_strs = set()\n+        self._populated = False\n+        self._local = Local()\n+\n+    def __repr__(self):\n+        if isinstance(self.urlconf_name, list) and self.urlconf_name:\n+            # Don't bother to output the whole list, it can be huge\n+            urlconf_repr = '<%s list>' % self.urlconf_name[0].__class__.__name__\n+        else:\n+            urlconf_repr = repr(self.urlconf_name)\n+        return '<%s %s (%s:%s) %s>' % (\n+            self.__class__.__name__, urlconf_repr, self.app_name,\n+            self.namespace, self.pattern.describe(),\n+        )\n+\n+    def check(self):\n+        messages = []\n+        for pattern in self.url_patterns:\n+            messages.extend(check_resolver(pattern))\n+        messages.extend(self._check_custom_error_handlers())\n+        return messages or self.pattern.check()\n+\n+    def _check_custom_error_handlers(self):\n+        messages = []\n+        # All handlers take (request, exception) arguments except handler500\n+        # which takes (request).\n+        for status_code, num_parameters in [(400, 2), (403, 2), (404, 2), (500, 1)]:\n+            try:\n+                handler, param_dict = self.resolve_error_handler(status_code)\n+            except (ImportError, ViewDoesNotExist) as e:\n+                path = getattr(self.urlconf_module, 'handler%s' % status_code)\n+                msg = (\n+                    \"The custom handler{status_code} view '{path}' could not be imported.\"\n+                ).format(status_code=status_code, path=path)\n+                messages.append(Error(msg, hint=str(e), id='urls.E008'))\n+                continue\n+            signature = inspect.signature(handler)\n+            args = [None] * num_parameters\n+            try:\n+                signature.bind(*args)\n+            except TypeError:\n+                msg = (\n+                    \"The custom handler{status_code} view '{path}' does not \"\n+                    \"take the correct number of arguments ({args}).\"\n+                ).format(\n+                    status_code=status_code,\n+                    path=handler.__module__ + '.' + handler.__qualname__,\n+                    args='request, exception' if num_parameters == 2 else 'request',\n+                )\n+                messages.append(Error(msg, id='urls.E007'))\n+        return messages\n+\n+    def _populate(self):\n+        # Short-circuit if called recursively in this thread to prevent\n+        # infinite recursion. Concurrent threads may call this at the same\n+        # time and will need to continue, so set 'populating' on a\n+        # thread-local variable.\n+        if getattr(self._local, 'populating', False):\n+            return\n+        try:\n+            self._local.populating = True\n+            lookups = MultiValueDict()\n+            namespaces = {}\n+            apps = {}\n+            language_code = get_language()\n+            for url_pattern in reversed(self.url_patterns):\n+                p_pattern = url_pattern.pattern.regex.pattern\n+                if p_pattern.startswith('^'):\n+                    p_pattern = p_pattern[1:]\n+                if isinstance(url_pattern, URLPattern):\n+                    self._callback_strs.add(url_pattern.lookup_str)\n+                    bits = normalize(url_pattern.pattern.regex.pattern)\n+                    lookups.appendlist(\n+                        url_pattern.callback,\n+                        (bits, p_pattern, url_pattern.default_args, url_pattern.pattern.converters)\n+                    )\n+                    if url_pattern.name is not None:\n+                        lookups.appendlist(\n+                            url_pattern.name,\n+                            (bits, p_pattern, url_pattern.default_args, url_pattern.pattern.converters)\n+                        )\n+                else:  # url_pattern is a URLResolver.\n+                    url_pattern._populate()\n+                    if url_pattern.app_name:\n+                        apps.setdefault(url_pattern.app_name, []).append(url_pattern.namespace)\n+                        namespaces[url_pattern.namespace] = (p_pattern, url_pattern)\n+                    else:\n+                        for name in url_pattern.reverse_dict:\n+                            for matches, pat, defaults, converters in url_pattern.reverse_dict.getlist(name):\n+                                new_matches = normalize(p_pattern + pat)\n+                                lookups.appendlist(\n+                                    name,\n+                                    (\n+                                        new_matches,\n+                                        p_pattern + pat,\n+                                        {**defaults, **url_pattern.default_kwargs},\n+                                        {**self.pattern.converters, **url_pattern.pattern.converters, **converters}\n+                                    )\n+                                )\n+                        for namespace, (prefix, sub_pattern) in url_pattern.namespace_dict.items():\n+                            current_converters = url_pattern.pattern.converters\n+                            sub_pattern.pattern.converters.update(current_converters)\n+                            namespaces[namespace] = (p_pattern + prefix, sub_pattern)\n+                        for app_name, namespace_list in url_pattern.app_dict.items():\n+                            apps.setdefault(app_name, []).extend(namespace_list)\n+                    self._callback_strs.update(url_pattern._callback_strs)\n+            self._namespace_dict[language_code] = namespaces\n+            self._app_dict[language_code] = apps\n+            self._reverse_dict[language_code] = lookups\n+            self._populated = True\n+        finally:\n+            self._local.populating = False\n+\n+    @property\n+    def reverse_dict(self):\n+        language_code = get_language()\n+        if language_code not in self._reverse_dict:\n+            self._populate()\n+        return self._reverse_dict[language_code]\n+\n+    @property\n+    def namespace_dict(self):\n+        language_code = get_language()\n+        if language_code not in self._namespace_dict:\n+            self._populate()\n+        return self._namespace_dict[language_code]\n+\n+    @property\n+    def app_dict(self):\n+        language_code = get_language()\n+        if language_code not in self._app_dict:\n+            self._populate()\n+        return self._app_dict[language_code]\n+\n+    @staticmethod\n+    def _join_route(route1, route2):\n+        \"\"\"Join two routes, without the starting ^ in the second route.\"\"\"\n+        if not route1:\n+            return route2\n+        if route2.startswith('^'):\n+            route2 = route2[1:]\n+        return route1 + route2\n+\n+    def _is_callback(self, name):\n+        if not self._populated:\n+            self._populate()\n+        return name in self._callback_strs\n+\n+    def resolve(self, path):\n+        path = str(path)  # path may be a reverse_lazy object\n+        tried = []\n+        match = self.pattern.match(path)\n+        if match:\n+            new_path, args, kwargs = match\n+            for pattern in self.url_patterns:\n+                try:\n+                    sub_match = pattern.resolve(new_path)\n+                except Resolver404 as e:\n+                    sub_tried = e.args[0].get('tried')\n+                    if sub_tried is not None:\n+                        tried.extend([pattern] + t for t in sub_tried)\n+                    else:\n+                        tried.append([pattern])\n+                else:\n+                    if sub_match:\n+                        # Merge captured arguments in match with submatch\n+                        sub_match_dict = {**kwargs, **self.default_kwargs}\n+                        # Update the sub_match_dict with the kwargs from the sub_match.\n+                        sub_match_dict.update(sub_match.kwargs)\n+                        # If there are *any* named groups, ignore all non-named groups.\n+                        # Otherwise, pass all non-named arguments as positional arguments.\n+                        sub_match_args = sub_match.args\n+                        if not sub_match_dict:\n+                            sub_match_args = args + sub_match.args\n+                        current_route = '' if isinstance(pattern, URLPattern) else str(pattern.pattern)\n+                        return ResolverMatch(\n+                            sub_match.func,\n+                            sub_match_args,\n+                            sub_match_dict,\n+                            sub_match.url_name,\n+                            [self.app_name] + sub_match.app_names,\n+                            [self.namespace] + sub_match.namespaces,\n+                            self._join_route(current_route, sub_match.route),\n+                        )\n+                    tried.append([pattern])\n+            raise Resolver404({'tried': tried, 'path': new_path})\n+        raise Resolver404({'path': path})\n+\n+    @cached_property\n+    def urlconf_module(self):\n+        if isinstance(self.urlconf_name, str):\n+            return import_module(self.urlconf_name)\n+        else:\n+            return self.urlconf_name\n+\n+    @cached_property\n+    def url_patterns(self):\n+        # urlconf_module might be a valid set of patterns, so we default to it\n+        patterns = getattr(self.urlconf_module, \"urlpatterns\", self.urlconf_module)\n+        try:\n+            iter(patterns)\n+        except TypeError:\n+            msg = (\n+                \"The included URLconf '{name}' does not appear to have any \"\n+                \"patterns in it. If you see valid patterns in the file then \"\n+                \"the issue is probably caused by a circular import.\"\n+            )\n+            raise ImproperlyConfigured(msg.format(name=self.urlconf_name))\n+        return patterns\n+\n+    def resolve_error_handler(self, view_type):\n+        callback = getattr(self.urlconf_module, 'handler%s' % view_type, None)\n+        if not callback:\n+            # No handler specified in file; use lazy import, since\n+            # django.conf.urls imports this file.\n+            from django.conf import urls\n+            callback = getattr(urls, 'handler%s' % view_type)\n+        return get_callable(callback), {}\n+\n+    def reverse(self, lookup_view, *args, **kwargs):\n+        return self._reverse_with_prefix(lookup_view, '', *args, **kwargs)\n+\n+    def _reverse_with_prefix(self, lookup_view, _prefix, *args, **kwargs):\n+        if args and kwargs:\n+            raise ValueError(\"Don't mix *args and **kwargs in call to reverse()!\")\n+\n+        if not self._populated:\n+            self._populate()\n+\n+        possibilities = self.reverse_dict.getlist(lookup_view)\n+\n+        for possibility, pattern, defaults, converters in possibilities:\n+            for result, params in possibility:\n+                if args:\n+                    if len(args) != len(params):\n+                        continue\n+                    candidate_subs = dict(zip(params, args))\n+                else:\n+                    if set(kwargs).symmetric_difference(params).difference(defaults):\n+                        continue\n+                    if any(kwargs.get(k, v) != v for k, v in defaults.items()):\n+                        continue\n+                    candidate_subs = kwargs\n+                # Convert the candidate subs to text using Converter.to_url().\n+                text_candidate_subs = {}\n+                for k, v in candidate_subs.items():\n+                    if k in converters:\n+                        text_candidate_subs[k] = converters[k].to_url(v)\n+                    else:\n+                        text_candidate_subs[k] = str(v)\n+                # WSGI provides decoded URLs, without %xx escapes, and the URL\n+                # resolver operates on such URLs. First substitute arguments\n+                # without quoting to build a decoded URL and look for a match.\n+                # Then, if we have a match, redo the substitution with quoted\n+                # arguments in order to return a properly encoded URL.\n+                candidate_pat = _prefix.replace('%', '%%') + result\n+                if re.search('^%s%s' % (re.escape(_prefix), pattern), candidate_pat % text_candidate_subs):\n+                    # safe characters from `pchar` definition of RFC 3986\n+                    url = quote(candidate_pat % text_candidate_subs, safe=RFC3986_SUBDELIMS + '/~:@')\n+                    # Don't allow construction of scheme relative urls.\n+                    return escape_leading_slashes(url)\n+        # lookup_view can be URL name or callable, but callables are not\n+        # friendly in error messages.\n+        m = getattr(lookup_view, '__module__', None)\n+        n = getattr(lookup_view, '__name__', None)\n+        if m is not None and n is not None:\n+            lookup_view_s = \"%s.%s\" % (m, n)\n+        else:\n+            lookup_view_s = lookup_view\n+\n+        patterns = [pattern for (_, pattern, _, _) in possibilities]\n+        if patterns:\n+            if args:\n+                arg_msg = \"arguments '%s'\" % (args,)\n+            elif kwargs:\n+                arg_msg = \"keyword arguments '%s'\" % (kwargs,)\n+            else:\n+                arg_msg = \"no arguments\"\n+            msg = (\n+                \"Reverse for '%s' with %s not found. %d pattern(s) tried: %s\" %\n+                (lookup_view_s, arg_msg, len(patterns), patterns)\n+            )\n+        else:\n+            msg = (\n+                \"Reverse for '%(view)s' not found. '%(view)s' is not \"\n+                \"a valid view function or pattern name.\" % {'view': lookup_view_s}\n+            )\n+        raise NoReverseMatch(msg)\ndiff --git a/.harness/checkpoints/12d4761652eb/--Users--huetuanthi--dev--agents--harness--django--views--debug.py b/.harness/checkpoints/12d4761652eb/--Users--huetuanthi--dev--agents--harness--django--views--debug.py\nnew file mode 100644\nindex 0000000000..cb3e8c7ec1\n--- /dev/null\n+++ b/.harness/checkpoints/12d4761652eb/--Users--huetuanthi--dev--agents--harness--django--views--debug.py\n@@ -0,0 +1,523 @@\n+import functools\n+import re\n+import sys\n+import types\n+from pathlib import Path\n+\n+from django.conf import settings\n+from django.http import HttpResponse, HttpResponseNotFound\n+from django.template import Context, Engine, TemplateDoesNotExist\n+from django.template.defaultfilters import pprint\n+from django.urls import Resolver404, resolve\n+from django.utils import timezone\n+from django.utils.datastructures import MultiValueDict\n+from django.utils.encoding import force_str\n+from django.utils.module_loading import import_string\n+from django.utils.version import get_docs_version\n+\n+# Minimal Django templates engine to render the error templates\n+# regardless of the project's TEMPLATES setting. Templates are\n+# read directly from the filesystem so that the error handler\n+# works even if the template loader is broken.\n+DEBUG_ENGINE = Engine(\n+    debug=True,\n+    libraries={'i18n': 'django.templatetags.i18n'},\n+)\n+\n+HIDDEN_SETTINGS = re.compile('API|TOKEN|KEY|SECRET|PASS|SIGNATURE', flags=re.IGNORECASE)\n+\n+CLEANSED_SUBSTITUTE = '********************'\n+\n+CURRENT_DIR = Path(__file__).parent\n+\n+\n+class CallableSettingWrapper:\n+    \"\"\"\n+    Object to wrap callable appearing in settings.\n+    * Not to call in the debug page (#21345).\n+    * Not to break the debug page if the callable forbidding to set attributes\n+      (#23070).\n+    \"\"\"\n+    def __init__(self, callable_setting):\n+        self._wrapped = callable_setting\n+\n+    def __repr__(self):\n+        return repr(self._wrapped)\n+\n+\n+def cleanse_setting(key, value):\n+    \"\"\"\n+    Cleanse an individual setting key/value of sensitive content. If the value\n+    is a dictionary, recursively cleanse the keys in that dictionary.\n+    \"\"\"\n+    try:\n+        if HIDDEN_SETTINGS.search(key):\n+            cleansed = CLEANSED_SUBSTITUTE\n+        else:\n+            if isinstance(value, dict):\n+                cleansed = {k: cleanse_setting(k, v) for k, v in value.items()}\n+            else:\n+                cleansed = value\n+    except TypeError:\n+        # If the key isn't regex-able, just return as-is.\n+        cleansed = value\n+\n+    if callable(cleansed):\n+        # For fixing #21345 and #23070\n+        cleansed = CallableSettingWrapper(cleansed)\n+\n+    return cleansed\n+\n+\n+def get_safe_settings():\n+    \"\"\"\n+    Return a dictionary of the settings module with values of sensitive\n+    settings replaced with stars (*********).\n+    \"\"\"\n+    settings_dict = {}\n+    for k in dir(settings):\n+        if k.isupper():\n+            settings_dict[k] = cleanse_setting(k, getattr(settings, k))\n+    return settings_dict\n+\n+\n+def technical_500_response(request, exc_type, exc_value, tb, status_code=500):\n+    \"\"\"\n+    Create a technical server error response. The last three arguments are\n+    the values returned from sys.exc_info() and friends.\n+    \"\"\"\n+    reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n+    if request.is_ajax():\n+        text = reporter.get_traceback_text()\n+        return HttpResponse(text, status=status_code, content_type='text/plain; charset=utf-8')\n+    else:\n+        html = reporter.get_traceback_html()\n+        return HttpResponse(html, status=status_code, content_type='text/html')\n+\n+\n+@functools.lru_cache()\n+def get_default_exception_reporter_filter():\n+    # Instantiate the default filter for the first time and cache it.\n+    return import_string(settings.DEFAULT_EXCEPTION_REPORTER_FILTER)()\n+\n+\n+def get_exception_reporter_filter(request):\n+    default_filter = get_default_exception_reporter_filter()\n+    return getattr(request, 'exception_reporter_filter', default_filter)\n+\n+\n+class ExceptionReporterFilter:\n+    \"\"\"\n+    Base for all exception reporter filter classes. All overridable hooks\n+    contain lenient default behaviors.\n+    \"\"\"\n+\n+    def get_post_parameters(self, request):\n+        if request is None:\n+            return {}\n+        else:\n+            return request.POST\n+\n+    def get_traceback_frame_variables(self, request, tb_frame):\n+        return list(tb_frame.f_locals.items())\n+\n+\n+class SafeExceptionReporterFilter(ExceptionReporterFilter):\n+    \"\"\"\n+    Use annotations made by the sensitive_post_parameters and\n+    sensitive_variables decorators to filter out sensitive information.\n+    \"\"\"\n+\n+    def is_active(self, request):\n+        \"\"\"\n+        This filter is to add safety in production environments (i.e. DEBUG\n+        is False). If DEBUG is True then your site is not safe anyway.\n+        This hook is provided as a convenience to easily activate or\n+        deactivate the filter on a per request basis.\n+        \"\"\"\n+        return settings.DEBUG is False\n+\n+    def get_cleansed_multivaluedict(self, request, multivaluedict):\n+        \"\"\"\n+        Replace the keys in a MultiValueDict marked as sensitive with stars.\n+        This mitigates leaking sensitive POST parameters if something like\n+        request.POST['nonexistent_key'] throws an exception (#21098).\n+        \"\"\"\n+        sensitive_post_parameters = getattr(request, 'sensitive_post_parameters', [])\n+        if self.is_active(request) and sensitive_post_parameters:\n+            multivaluedict = multivaluedict.copy()\n+            for param in sensitive_post_parameters:\n+                if param in multivaluedict:\n+                    multivaluedict[param] = CLEANSED_SUBSTITUTE\n+        return multivaluedict\n+\n+    def get_post_parameters(self, request):\n+        \"\"\"\n+        Replace the values of POST parameters marked as sensitive with\n+        stars (*********).\n+        \"\"\"\n+        if request is None:\n+            return {}\n+        else:\n+            sensitive_post_parameters = getattr(request, 'sensitive_post_parameters', [])\n+            if self.is_active(request) and sensitive_post_parameters:\n+                cleansed = request.POST.copy()\n+                if sensitive_post_parameters == '__ALL__':\n+                    # Cleanse all parameters.\n+                    for k in cleansed:\n+                        cleansed[k] = CLEANSED_SUBSTITUTE\n+                    return cleansed\n+                else:\n+                    # Cleanse only the specified parameters.\n+                    for param in sensitive_post_parameters:\n+                        if param in cleansed:\n+                            cleansed[param] = CLEANSED_SUBSTITUTE\n+                    return cleansed\n+            else:\n+                return request.POST\n+\n+    def cleanse_special_types(self, request, value):\n+        try:\n+            # If value is lazy or a complex object of another kind, this check\n+            # might raise an exception. isinstance checks that lazy\n+            # MultiValueDicts will have a return value.\n+            is_multivalue_dict = isinstance(value, MultiValueDict)\n+        except Exception as e:\n+            return '{!r} while evaluating {!r}'.format(e, value)\n+\n+        if is_multivalue_dict:\n+            # Cleanse MultiValueDicts (request.POST is the one we usually care about)\n+            value = self.get_cleansed_multivaluedict(request, value)\n+        return value\n+\n+    def get_traceback_frame_variables(self, request, tb_frame):\n+        \"\"\"\n+        Replace the values of variables marked as sensitive with\n+        stars (*********).\n+        \"\"\"\n+        # Loop through the frame's callers to see if the sensitive_variables\n+        # decorator was used.\n+        current_frame = tb_frame.f_back\n+        sensitive_variables = None\n+        while current_frame is not None:\n+            if (current_frame.f_code.co_name == 'sensitive_variables_wrapper' and\n+                    'sensitive_variables_wrapper' in current_frame.f_locals):\n+                # The sensitive_variables decorator was used, so we take note\n+                # of the sensitive variables' names.\n+                wrapper = current_frame.f_locals['sensitive_variables_wrapper']\n+                sensitive_variables = getattr(wrapper, 'sensitive_variables', None)\n+                break\n+            current_frame = current_frame.f_back\n+\n+        cleansed = {}\n+        if self.is_active(request) and sensitive_variables:\n+            if sensitive_variables == '__ALL__':\n+                # Cleanse all variables\n+                for name in tb_frame.f_locals:\n+                    cleansed[name] = CLEANSED_SUBSTITUTE\n+            else:\n+                # Cleanse specified variables\n+                for name, value in tb_frame.f_locals.items():\n+                    if name in sensitive_variables:\n+                        value = CLEANSED_SUBSTITUTE\n+                    else:\n+                        value = self.cleanse_special_types(request, value)\n+                    cleansed[name] = value\n+        else:\n+            # Potentially cleanse the request and any MultiValueDicts if they\n+            # are one of the frame variables.\n+            for name, value in tb_frame.f_locals.items():\n+                cleansed[name] = self.cleanse_special_types(request, value)\n+\n+        if (tb_frame.f_code.co_name == 'sensitive_variables_wrapper' and\n+                'sensitive_variables_wrapper' in tb_frame.f_locals):\n+            # For good measure, obfuscate the decorated function's arguments in\n+            # the sensitive_variables decorator's frame, in case the variables\n+            # associated with those arguments were meant to be obfuscated from\n+            # the decorated function's frame.\n+            cleansed['func_args'] = CLEANSED_SUBSTITUTE\n+            cleansed['func_kwargs'] = CLEANSED_SUBSTITUTE\n+\n+        return cleansed.items()\n+\n+\n+class ExceptionReporter:\n+    \"\"\"Organize and coordinate reporting on exceptions.\"\"\"\n+    def __init__(self, request, exc_type, exc_value, tb, is_email=False):\n+        self.request = request\n+        self.filter = get_exception_reporter_filter(self.request)\n+        self.exc_type = exc_type\n+        self.exc_value = exc_value\n+        self.tb = tb\n+        self.is_email = is_email\n+\n+        self.template_info = getattr(self.exc_value, 'template_debug', None)\n+        self.template_does_not_exist = False\n+        self.postmortem = None\n+\n+    def get_traceback_data(self):\n+        \"\"\"Return a dictionary containing traceback information.\"\"\"\n+        if self.exc_type and issubclass(self.exc_type, TemplateDoesNotExist):\n+            self.template_does_not_exist = True\n+            self.postmortem = self.exc_value.chain or [self.exc_value]\n+\n+        frames = self.get_traceback_frames()\n+        for i, frame in enumerate(frames):\n+            if 'vars' in frame:\n+                frame_vars = []\n+                for k, v in frame['vars']:\n+                    v = pprint(v)\n+                    # Trim large blobs of data\n+                    if len(v) > 4096:\n+                        v = '%s\u2026 <trimmed %d bytes string>' % (v[0:4096], len(v))\n+                    frame_vars.append((k, v))\n+                frame['vars'] = frame_vars\n+            frames[i] = frame\n+\n+        unicode_hint = ''\n+        if self.exc_type and issubclass(self.exc_type, UnicodeError):\n+            start = getattr(self.exc_value, 'start', None)\n+            end = getattr(self.exc_value, 'end', None)\n+            if start is not None and end is not None:\n+                unicode_str = self.exc_value.args[1]\n+                unicode_hint = force_str(\n+                    unicode_str[max(start - 5, 0):min(end + 5, len(unicode_str))],\n+                    'ascii', errors='replace'\n+                )\n+        from django import get_version\n+\n+        if self.request is None:\n+            user_str = None\n+        else:\n+            try:\n+                user_str = str(self.request.user)\n+            except Exception:\n+                # request.user may raise OperationalError if the database is\n+                # unavailable, for example.\n+                user_str = '[unable to retrieve the current user]'\n+\n+        c = {\n+            'is_email': self.is_email,\n+            'unicode_hint': unicode_hint,\n+            'frames': frames,\n+            'request': self.request,\n+            'user_str': user_str,\n+            'filtered_POST_items': list(self.filter.get_post_parameters(self.request).items()),\n+            'settings': get_safe_settings(),\n+            'sys_executable': sys.executable,\n+            'sys_version_info': '%d.%d.%d' % sys.version_info[0:3],\n+            'server_time': timezone.now(),\n+            'django_version_info': get_version(),\n+            'sys_path': sys.path,\n+            'template_info': self.template_info,\n+            'template_does_not_exist': self.template_does_not_exist,\n+            'postmortem': self.postmortem,\n+        }\n+        if self.request is not None:\n+            c['request_GET_items'] = self.request.GET.items()\n+            c['request_FILES_items'] = self.request.FILES.items()\n+            c['request_COOKIES_items'] = self.request.COOKIES.items()\n+        # Check whether exception info is available\n+        if self.exc_type:\n+            c['exception_type'] = self.exc_type.__name__\n+        if self.exc_value:\n+            c['exception_value'] = str(self.exc_value)\n+        if frames:\n+            c['lastframe'] = frames[-1]\n+        return c\n+\n+    def get_traceback_html(self):\n+        \"\"\"Return HTML version of debug 500 HTTP error page.\"\"\"\n+        with Path(CURRENT_DIR, 'templates', 'technical_500.html').open(encoding='utf-8') as fh:\n+            t = DEBUG_ENGINE.from_string(fh.read())\n+        c = Context(self.get_traceback_data(), use_l10n=False)\n+        return t.render(c)\n+\n+    def get_traceback_text(self):\n+        \"\"\"Return plain text version of debug 500 HTTP error page.\"\"\"\n+        with Path(CURRENT_DIR, 'templates', 'technical_500.txt').open(encoding='utf-8') as fh:\n+            t = DEBUG_ENGINE.from_string(fh.read())\n+        c = Context(self.get_traceback_data(), autoescape=False, use_l10n=False)\n+        return t.render(c)\n+\n+    def _get_lines_from_file(self, filename, lineno, context_lines, loader=None, module_name=None):\n+        \"\"\"\n+        Return context_lines before and after lineno from file.\n+        Return (pre_context_lineno, pre_context, context_line, post_context).\n+        \"\"\"\n+        source = None\n+        if hasattr(loader, 'get_source'):\n+            try:\n+                source = loader.get_source(module_name)\n+            except ImportError:\n+                pass\n+            if source is not None:\n+                source = source.splitlines()\n+        if source is None:\n+            try:\n+                with open(filename, 'rb') as fp:\n+                    source = fp.read().splitlines()\n+            except OSError:\n+                pass\n+        if source is None:\n+            return None, [], None, []\n+\n+        # If we just read the source from a file, or if the loader did not\n+        # apply tokenize.detect_encoding to decode the source into a\n+        # string, then we should do that ourselves.\n+        if isinstance(source[0], bytes):\n+            encoding = 'ascii'\n+            for line in source[:2]:\n+                # File coding may be specified. Match pattern from PEP-263\n+                # (https://www.python.org/dev/peps/pep-0263/)\n+                match = re.search(br'coding[:=]\\s*([-\\w.]+)', line)\n+                if match:\n+                    encoding = match.group(1).decode('ascii')\n+                    break\n+            source = [str(sline, encoding, 'replace') for sline in source]\n+\n+        lower_bound = max(0, lineno - context_lines)\n+        upper_bound = lineno + context_lines\n+\n+        pre_context = source[lower_bound:lineno]\n+        context_line = source[lineno]\n+        post_context = source[lineno + 1:upper_bound]\n+\n+        return lower_bound, pre_context, context_line, post_context\n+\n+    def get_traceback_frames(self):\n+        def explicit_or_implicit_cause(exc_value):\n+            explicit = getattr(exc_value, '__cause__', None)\n+            implicit = getattr(exc_value, '__context__', None)\n+            return explicit or implicit\n+\n+        # Get the exception and all its causes\n+        exceptions = []\n+        exc_value = self.exc_value\n+        while exc_value:\n+            exceptions.append(exc_value)\n+            exc_value = explicit_or_implicit_cause(exc_value)\n+            if exc_value in exceptions:\n+                # Avoid infinite loop if there's a cyclic reference (#29393).\n+                break\n+\n+        frames = []\n+        # No exceptions were supplied to ExceptionReporter\n+        if not exceptions:\n+            return frames\n+\n+        # In case there's just one exception, take the traceback from self.tb\n+        exc_value = exceptions.pop()\n+        tb = self.tb if not exceptions else exc_value.__traceback__\n+\n+        while tb is not None:\n+            # Support for __traceback_hide__ which is used by a few libraries\n+            # to hide internal frames.\n+            if tb.tb_frame.f_locals.get('__traceback_hide__'):\n+                tb = tb.tb_next\n+                continue\n+            filename = tb.tb_frame.f_code.co_filename\n+            function = tb.tb_frame.f_code.co_name\n+            lineno = tb.tb_lineno - 1\n+            loader = tb.tb_frame.f_globals.get('__loader__')\n+            module_name = tb.tb_frame.f_globals.get('__name__') or ''\n+            pre_context_lineno, pre_context, context_line, post_context = self._get_lines_from_file(\n+                filename, lineno, 7, loader, module_name,\n+            )\n+            if pre_context_lineno is None:\n+                pre_context_lineno = lineno\n+                pre_context = []\n+                context_line = '<source code not available>'\n+                post_context = []\n+            frames.append({\n+                'exc_cause': explicit_or_implicit_cause(exc_value),\n+                'exc_cause_explicit': getattr(exc_value, '__cause__', True),\n+                'tb': tb,\n+                'type': 'django' if module_name.startswith('django.') else 'user',\n+                'filename': filename,\n+                'function': function,\n+                'lineno': lineno + 1,\n+                'vars': self.filter.get_traceback_frame_variables(self.request, tb.tb_frame),\n+                'id': id(tb),\n+                'pre_context': pre_context,\n+                'context_line': context_line,\n+                'post_context': post_context,\n+                'pre_context_lineno': pre_context_lineno + 1,\n+            })\n+\n+            # If the traceback for current exception is consumed, try the\n+            # other exception.\n+            if not tb.tb_next and exceptions:\n+                exc_value = exceptions.pop()\n+                tb = exc_value.__traceback__\n+            else:\n+                tb = tb.tb_next\n+\n+        return frames\n+\n+\n+def technical_404_response(request, exception):\n+    \"\"\"Create a technical 404 error response. `exception` is the Http404.\"\"\"\n+    try:\n+        error_url = exception.args[0]['path']\n+    except (IndexError, TypeError, KeyError):\n+        error_url = request.path_info[1:]  # Trim leading slash\n+\n+    try:\n+        tried = exception.args[0]['tried']\n+    except (IndexError, TypeError, KeyError):\n+        tried = []\n+    else:\n+        if (not tried or (                  # empty URLconf\n+            request.path == '/' and\n+            len(tried) == 1 and             # default URLconf\n+            len(tried[0]) == 1 and\n+            getattr(tried[0][0], 'app_name', '') == getattr(tried[0][0], 'namespace', '') == 'admin'\n+        )):\n+            return default_urlconf(request)\n+\n+    urlconf = getattr(request, 'urlconf', settings.ROOT_URLCONF)\n+    if isinstance(urlconf, types.ModuleType):\n+        urlconf = urlconf.__name__\n+\n+    caller = ''\n+    try:\n+        resolver_match = resolve(request.path)\n+    except Resolver404:\n+        pass\n+    else:\n+        obj = resolver_match.func\n+\n+        if hasattr(obj, '__name__'):\n+            caller = obj.__name__\n+        elif hasattr(obj, '__class__') and hasattr(obj.__class__, '__name__'):\n+            caller = obj.__class__.__name__\n+\n+        if hasattr(obj, '__module__'):\n+            module = obj.__module__\n+            caller = '%s.%s' % (module, caller)\n+\n+    with Path(CURRENT_DIR, 'templates', 'technical_404.html').open(encoding='utf-8') as fh:\n+        t = DEBUG_ENGINE.from_string(fh.read())\n+    c = Context({\n+        'urlconf': urlconf,\n+        'root_urlconf': settings.ROOT_URLCONF,\n+        'request_path': error_url,\n+        'urlpatterns': tried,\n+        'reason': str(exception),\n+        'request': request,\n+        'settings': get_safe_settings(),\n+        'raising_view_name': caller,\n+    })\n+    return HttpResponseNotFound(t.render(c), content_type='text/html')\n+\n+\n+def default_urlconf(request):\n+    \"\"\"Create an empty URLconf 404 error response.\"\"\"\n+    with Path(CURRENT_DIR, 'templates', 'default_urlconf.html').open(encoding='utf-8') as fh:\n+        t = DEBUG_ENGINE.from_string(fh.read())\n+    c = Context({\n+        'version': get_docs_version(),\n+    })\n+\n+    return HttpResponse(t.render(c), content_type='text/html')\ndiff --git a/django/urls/resolvers.py b/django/urls/resolvers.py\nindex 2154a46320..c719742c28 100644\n--- a/django/urls/resolvers.py\n+++ b/django/urls/resolvers.py\n@@ -17,6 +17,7 @@ from django.conf import settings\n from django.core.checks import Error, Warning\n from django.core.checks.urls import check_resolver\n from django.core.exceptions import ImproperlyConfigured, ViewDoesNotExist\n+from django.http import Http404\n from django.utils.datastructures import MultiValueDict\n from django.utils.functional import cached_property\n from django.utils.http import RFC3986_SUBDELIMS, escape_leading_slashes\n@@ -260,6 +261,8 @@ class RoutePattern(CheckURLMixin):\n                     kwargs[key] = converter.to_python(value)\n                 except ValueError:\n                     return None\n+                except Http404:\n+                    raise\n             return path[match.end():], (), kwargs\n         return None\n \ndiff --git a/django/views/debug.py b/django/views/debug.py\nindex cb3e8c7ec1..08bc43cf0e 100644\n--- a/django/views/debug.py\n+++ b/django/views/debug.py\n@@ -5,7 +5,7 @@ import types\n from pathlib import Path\n \n from django.conf import settings\n-from django.http import HttpResponse, HttpResponseNotFound\n+from django.http import Http404, HttpResponse, HttpResponseNotFound\n from django.template import Context, Engine, TemplateDoesNotExist\n from django.template.defaultfilters import pprint\n from django.urls import Resolver404, resolve\n@@ -483,7 +483,7 @@ def technical_404_response(request, exception):\n     caller = ''\n     try:\n         resolver_match = resolve(request.path)\n-    except Resolver404:\n+    except Http404:\n         pass\n     else:\n         obj = resolver_match.func\ndiff --git a/tests/urlpatterns/tests.py b/tests/urlpatterns/tests.py\nindex 214739f678..6d84fdbcb1 100644\n--- a/tests/urlpatterns/tests.py\n+++ b/tests/urlpatterns/tests.py\n@@ -1,6 +1,7 @@\n import uuid\n \n from django.core.exceptions import ImproperlyConfigured\n+from django.http import Http404\n from django.test import SimpleTestCase\n from django.test.utils import override_settings\n from django.urls import Resolver404, path, resolve, reverse\n@@ -219,6 +220,13 @@ class ConversionExceptionTests(SimpleTestCase):\n         with self.assertRaisesMessage(TypeError, 'This type error propagates.'):\n             resolve('/dynamic/abc/')\n \n+    def test_resolve_http404_propagates(self):\n+        @DynamicConverter.register_to_python\n+        def raises_http404(value):\n+            raise Http404('This Http404 propagates.')\n+        with self.assertRaisesMessage(Http404, 'This Http404 propagates.'):\n+            resolve('/dynamic/abc/')\n+\n     def test_reverse_value_error_propagates(self):\n         @DynamicConverter.register_to_url\n         def raises_value_error(value):\n"}
{"instance_id": "django__django-11630", "model_name_or_path": "harness/claude-sonnet-4-6", "model_patch": "diff --git a/.harness/checkpoints/04528095a088/django--core--checks--model_checks.py b/.harness/checkpoints/04528095a088/django--core--checks--model_checks.py\nnew file mode 100644\nindex 0000000000..5c2266ca1d\n--- /dev/null\n+++ b/.harness/checkpoints/04528095a088/django--core--checks--model_checks.py\n@@ -0,0 +1,198 @@\n+import inspect\n+import types\n+from collections import defaultdict\n+from itertools import chain\n+\n+from django.apps import apps\n+from django.core.checks import Error, Tags, register\n+\n+\n+@register(Tags.models)\n+def check_all_models(app_configs=None, **kwargs):\n+    db_table_models = defaultdict(list)\n+    indexes = defaultdict(list)\n+    constraints = defaultdict(list)\n+    errors = []\n+    if app_configs is None:\n+        models = apps.get_models()\n+    else:\n+        models = chain.from_iterable(app_config.get_models() for app_config in app_configs)\n+    for model in models:\n+        if model._meta.managed and not model._meta.proxy:\n+            db_table_models[model._meta.db_table].append(model._meta.label)\n+        if not inspect.ismethod(model.check):\n+            errors.append(\n+                Error(\n+                    \"The '%s.check()' class method is currently overridden by %r.\"\n+                    % (model.__name__, model.check),\n+                    obj=model,\n+                    id='models.E020'\n+                )\n+            )\n+        else:\n+            errors.extend(model.check(**kwargs))\n+        for model_index in model._meta.indexes:\n+            indexes[model_index.name].append(model._meta.label)\n+        for model_constraint in model._meta.constraints:\n+            constraints[model_constraint.name].append(model._meta.label)\n+    for db_table, model_labels in db_table_models.items():\n+        if len(model_labels) != 1:\n+            errors.append(\n+                Error(\n+                    \"db_table '%s' is used by multiple models: %s.\"\n+                    % (db_table, ', '.join(db_table_models[db_table])),\n+                    obj=db_table,\n+                    id='models.E028',\n+                )\n+            )\n+    for index_name, model_labels in indexes.items():\n+        if len(model_labels) > 1:\n+            model_labels = set(model_labels)\n+            errors.append(\n+                Error(\n+                    \"index name '%s' is not unique %s %s.\" % (\n+                        index_name,\n+                        'for model' if len(model_labels) == 1 else 'amongst models:',\n+                        ', '.join(sorted(model_labels)),\n+                    ),\n+                    id='models.E029' if len(model_labels) == 1 else 'models.E030',\n+                ),\n+            )\n+    for constraint_name, model_labels in constraints.items():\n+        if len(model_labels) > 1:\n+            model_labels = set(model_labels)\n+            errors.append(\n+                Error(\n+                    \"constraint name '%s' is not unique %s %s.\" % (\n+                        constraint_name,\n+                        'for model' if len(model_labels) == 1 else 'amongst models:',\n+                        ', '.join(sorted(model_labels)),\n+                    ),\n+                    id='models.E031' if len(model_labels) == 1 else 'models.E032',\n+                ),\n+            )\n+    return errors\n+\n+\n+def _check_lazy_references(apps, ignore=None):\n+    \"\"\"\n+    Ensure all lazy (i.e. string) model references have been resolved.\n+\n+    Lazy references are used in various places throughout Django, primarily in\n+    related fields and model signals. Identify those common cases and provide\n+    more helpful error messages for them.\n+\n+    The ignore parameter is used by StateApps to exclude swappable models from\n+    this check.\n+    \"\"\"\n+    pending_models = set(apps._pending_operations) - (ignore or set())\n+\n+    # Short circuit if there aren't any errors.\n+    if not pending_models:\n+        return []\n+\n+    from django.db.models import signals\n+    model_signals = {\n+        signal: name for name, signal in vars(signals).items()\n+        if isinstance(signal, signals.ModelSignal)\n+    }\n+\n+    def extract_operation(obj):\n+        \"\"\"\n+        Take a callable found in Apps._pending_operations and identify the\n+        original callable passed to Apps.lazy_model_operation(). If that\n+        callable was a partial, return the inner, non-partial function and\n+        any arguments and keyword arguments that were supplied with it.\n+\n+        obj is a callback defined locally in Apps.lazy_model_operation() and\n+        annotated there with a `func` attribute so as to imitate a partial.\n+        \"\"\"\n+        operation, args, keywords = obj, [], {}\n+        while hasattr(operation, 'func'):\n+            args.extend(getattr(operation, 'args', []))\n+            keywords.update(getattr(operation, 'keywords', {}))\n+            operation = operation.func\n+        return operation, args, keywords\n+\n+    def app_model_error(model_key):\n+        try:\n+            apps.get_app_config(model_key[0])\n+            model_error = \"app '%s' doesn't provide model '%s'\" % model_key\n+        except LookupError:\n+            model_error = \"app '%s' isn't installed\" % model_key[0]\n+        return model_error\n+\n+    # Here are several functions which return CheckMessage instances for the\n+    # most common usages of lazy operations throughout Django. These functions\n+    # take the model that was being waited on as an (app_label, modelname)\n+    # pair, the original lazy function, and its positional and keyword args as\n+    # determined by extract_operation().\n+\n+    def field_error(model_key, func, args, keywords):\n+        error_msg = (\n+            \"The field %(field)s was declared with a lazy reference \"\n+            \"to '%(model)s', but %(model_error)s.\"\n+        )\n+        params = {\n+            'model': '.'.join(model_key),\n+            'field': keywords['field'],\n+            'model_error': app_model_error(model_key),\n+        }\n+        return Error(error_msg % params, obj=keywords['field'], id='fields.E307')\n+\n+    def signal_connect_error(model_key, func, args, keywords):\n+        error_msg = (\n+            \"%(receiver)s was connected to the '%(signal)s' signal with a \"\n+            \"lazy reference to the sender '%(model)s', but %(model_error)s.\"\n+        )\n+        receiver = args[0]\n+        # The receiver is either a function or an instance of class\n+        # defining a `__call__` method.\n+        if isinstance(receiver, types.FunctionType):\n+            description = \"The function '%s'\" % receiver.__name__\n+        elif isinstance(receiver, types.MethodType):\n+            description = \"Bound method '%s.%s'\" % (receiver.__self__.__class__.__name__, receiver.__name__)\n+        else:\n+            description = \"An instance of class '%s'\" % receiver.__class__.__name__\n+        signal_name = model_signals.get(func.__self__, 'unknown')\n+        params = {\n+            'model': '.'.join(model_key),\n+            'receiver': description,\n+            'signal': signal_name,\n+            'model_error': app_model_error(model_key),\n+        }\n+        return Error(error_msg % params, obj=receiver.__module__, id='signals.E001')\n+\n+    def default_error(model_key, func, args, keywords):\n+        error_msg = \"%(op)s contains a lazy reference to %(model)s, but %(model_error)s.\"\n+        params = {\n+            'op': func,\n+            'model': '.'.join(model_key),\n+            'model_error': app_model_error(model_key),\n+        }\n+        return Error(error_msg % params, obj=func, id='models.E022')\n+\n+    # Maps common uses of lazy operations to corresponding error functions\n+    # defined above. If a key maps to None, no error will be produced.\n+    # default_error() will be used for usages that don't appear in this dict.\n+    known_lazy = {\n+        ('django.db.models.fields.related', 'resolve_related_class'): field_error,\n+        ('django.db.models.fields.related', 'set_managed'): None,\n+        ('django.dispatch.dispatcher', 'connect'): signal_connect_error,\n+    }\n+\n+    def build_error(model_key, func, args, keywords):\n+        key = (func.__module__, func.__name__)\n+        error_fn = known_lazy.get(key, default_error)\n+        return error_fn(model_key, func, args, keywords) if error_fn else None\n+\n+    return sorted(filter(None, (\n+        build_error(model_key, *extract_operation(func))\n+        for model_key in pending_models\n+        for func in apps._pending_operations[model_key]\n+    )), key=lambda error: error.msg)\n+\n+\n+@register(Tags.models)\n+def check_lazy_references(app_configs=None, **kwargs):\n+    return _check_lazy_references(apps)\ndiff --git a/django/core/checks/model_checks.py b/django/core/checks/model_checks.py\nindex 5c2266ca1d..affba1eb37 100644\n--- a/django/core/checks/model_checks.py\n+++ b/django/core/checks/model_checks.py\n@@ -19,7 +19,7 @@ def check_all_models(app_configs=None, **kwargs):\n         models = chain.from_iterable(app_config.get_models() for app_config in app_configs)\n     for model in models:\n         if model._meta.managed and not model._meta.proxy:\n-            db_table_models[model._meta.db_table].append(model._meta.label)\n+            db_table_models[model._meta.db_table].append(model)\n         if not inspect.ismethod(model.check):\n             errors.append(\n                 Error(\n@@ -35,16 +35,40 @@ def check_all_models(app_configs=None, **kwargs):\n             indexes[model_index.name].append(model._meta.label)\n         for model_constraint in model._meta.constraints:\n             constraints[model_constraint.name].append(model._meta.label)\n-    for db_table, model_labels in db_table_models.items():\n-        if len(model_labels) != 1:\n-            errors.append(\n-                Error(\n-                    \"db_table '%s' is used by multiple models: %s.\"\n-                    % (db_table, ', '.join(db_table_models[db_table])),\n-                    obj=db_table,\n-                    id='models.E028',\n+    for db_table, table_models in db_table_models.items():\n+        if len(table_models) != 1:\n+            from django.db import router\n+            from django.conf import settings\n+            # Only report an error if the models sharing a table name can be\n+            # routed to the same database. Models on different databases can\n+            # legitimately share a table name without conflict.\n+            # Build a set of (db alias, model) pairs allowed by the router.\n+            all_dbs = list(settings.DATABASES)\n+            # Collect the database aliases each model is allowed on.\n+            models_dbs = [\n+                {db for db in all_dbs if router.allow_migrate_model(db, m)}\n+                for m in table_models\n+            ]\n+            # If no two models share a common allowed database, there is no\n+            # real collision and we should not raise an error.\n+            collision = False\n+            for i, dbs_i in enumerate(models_dbs):\n+                for dbs_j in models_dbs[i + 1:]:\n+                    if dbs_i & dbs_j:\n+                        collision = True\n+                        break\n+                if collision:\n+                    break\n+            if collision:\n+                model_labels = [m._meta.label for m in table_models]\n+                errors.append(\n+                    Error(\n+                        \"db_table '%s' is used by multiple models: %s.\"\n+                        % (db_table, ', '.join(model_labels)),\n+                        obj=db_table,\n+                        id='models.E028',\n+                    )\n                 )\n-            )\n     for index_name, model_labels in indexes.items():\n         if len(model_labels) > 1:\n             model_labels = set(model_labels)\ndiff --git a/tests/check_framework/test_model_checks.py b/tests/check_framework/test_model_checks.py\nindex 79177e38f7..e2a36c877a 100644\n--- a/tests/check_framework/test_model_checks.py\n+++ b/tests/check_framework/test_model_checks.py\n@@ -3,7 +3,7 @@ from django.core.checks import Error\n from django.db import models\n from django.test import SimpleTestCase, TestCase, skipUnlessDBFeature\n from django.test.utils import (\n-    isolate_apps, modify_settings, override_system_checks,\n+    isolate_apps, modify_settings, override_settings, override_system_checks,\n )\n \n \n@@ -74,6 +74,70 @@ class DuplicateDBTableTests(SimpleTestCase):\n         self.assertEqual(Model._meta.db_table, ProxyModel._meta.db_table)\n         self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [])\n \n+    @modify_settings(INSTALLED_APPS={'append': 'basic'})\n+    @isolate_apps('basic', 'check_framework', kwarg_name='apps')\n+    def test_no_collision_across_apps_with_different_db_via_router(self, apps):\n+        \"\"\"\n+        Models in different apps that share a db_table but are routed to\n+        different databases via DATABASE_ROUTERS should not raise E028.\n+        \"\"\"\n+        class RouterAllowBasicOnly:\n+            \"\"\"Route 'basic' app to 'other' DB, everything else to 'default'.\"\"\"\n+            def allow_migrate(self, db, app_label, **hints):\n+                if app_label == 'basic':\n+                    return db == 'other'\n+                return db == 'default'\n+\n+        class Model1(models.Model):\n+            class Meta:\n+                app_label = 'basic'\n+                db_table = 'test_table'\n+\n+        class Model2(models.Model):\n+            class Meta:\n+                app_label = 'check_framework'\n+                db_table = 'test_table'\n+\n+        with self.settings(DATABASE_ROUTERS=['check_framework.test_model_checks.RouterAllowBasicOnly']):\n+            # Patch the router into the module scope so the dotted-path lookup works.\n+            import check_framework.test_model_checks as _mod\n+            _mod.RouterAllowBasicOnly = RouterAllowBasicOnly\n+            self.assertEqual(checks.run_checks(app_configs=apps.get_app_configs()), [])\n+\n+    @modify_settings(INSTALLED_APPS={'append': 'basic'})\n+    @isolate_apps('basic', 'check_framework', kwarg_name='apps')\n+    def test_collision_across_apps_with_same_db_via_router(self, apps):\n+        \"\"\"\n+        Models in different apps that share a db_table AND can be migrated to\n+        the same database via DATABASE_ROUTERS should still raise E028.\n+        \"\"\"\n+        class RouterAllowAll:\n+            \"\"\"Allow everything on every database.\"\"\"\n+            def allow_migrate(self, db, app_label, **hints):\n+                return True\n+\n+        class Model1(models.Model):\n+            class Meta:\n+                app_label = 'basic'\n+                db_table = 'test_table'\n+\n+        class Model2(models.Model):\n+            class Meta:\n+                app_label = 'check_framework'\n+                db_table = 'test_table'\n+\n+        import check_framework.test_model_checks as _mod\n+        _mod.RouterAllowAll = RouterAllowAll\n+        with self.settings(DATABASE_ROUTERS=['check_framework.test_model_checks.RouterAllowAll']):\n+            self.assertEqual(checks.run_checks(app_configs=apps.get_app_configs()), [\n+                Error(\n+                    \"db_table 'test_table' is used by multiple models: \"\n+                    \"basic.Model1, check_framework.Model2.\",\n+                    obj='test_table',\n+                    id='models.E028',\n+                )\n+            ])\n+\n \n @isolate_apps('check_framework', attr_name='apps')\n @override_system_checks([checks.model_checks.check_all_models])\n"}
{"instance_id": "django__django-11742", "model_name_or_path": "harness/claude-sonnet-4-6", "model_patch": "diff --git a/.harness/checkpoints/2bdd67450864/--Users--huetuanthi--dev--agents--harness--django--db--models--fields--__init__.py b/.harness/checkpoints/2bdd67450864/--Users--huetuanthi--dev--agents--harness--django--db--models--fields--__init__.py\nnew file mode 100644\nindex 0000000000..9ce46e3f73\n--- /dev/null\n+++ b/.harness/checkpoints/2bdd67450864/--Users--huetuanthi--dev--agents--harness--django--db--models--fields--__init__.py\n@@ -0,0 +1,2399 @@\n+import collections.abc\n+import copy\n+import datetime\n+import decimal\n+import operator\n+import uuid\n+import warnings\n+from base64 import b64decode, b64encode\n+from functools import partialmethod, total_ordering\n+\n+from django import forms\n+from django.apps import apps\n+from django.conf import settings\n+from django.core import checks, exceptions, validators\n+# When the _meta object was formalized, this exception was moved to\n+# django.core.exceptions. It is retained here for backwards compatibility\n+# purposes.\n+from django.core.exceptions import FieldDoesNotExist  # NOQA\n+from django.db import connection, connections, router\n+from django.db.models.constants import LOOKUP_SEP\n+from django.db.models.query_utils import DeferredAttribute, RegisterLookupMixin\n+from django.utils import timezone\n+from django.utils.datastructures import DictWrapper\n+from django.utils.dateparse import (\n+    parse_date, parse_datetime, parse_duration, parse_time,\n+)\n+from django.utils.duration import duration_microseconds, duration_string\n+from django.utils.functional import Promise, cached_property\n+from django.utils.ipv6 import clean_ipv6_address\n+from django.utils.itercompat import is_iterable\n+from django.utils.text import capfirst\n+from django.utils.translation import gettext_lazy as _\n+\n+__all__ = [\n+    'AutoField', 'BLANK_CHOICE_DASH', 'BigAutoField', 'BigIntegerField',\n+    'BinaryField', 'BooleanField', 'CharField', 'CommaSeparatedIntegerField',\n+    'DateField', 'DateTimeField', 'DecimalField', 'DurationField',\n+    'EmailField', 'Empty', 'Field', 'FieldDoesNotExist', 'FilePathField',\n+    'FloatField', 'GenericIPAddressField', 'IPAddressField', 'IntegerField',\n+    'NOT_PROVIDED', 'NullBooleanField', 'PositiveIntegerField',\n+    'PositiveSmallIntegerField', 'SlugField', 'SmallAutoField',\n+    'SmallIntegerField', 'TextField', 'TimeField', 'URLField', 'UUIDField',\n+]\n+\n+\n+class Empty:\n+    pass\n+\n+\n+class NOT_PROVIDED:\n+    pass\n+\n+\n+# The values to use for \"blank\" in SelectFields. Will be appended to the start\n+# of most \"choices\" lists.\n+BLANK_CHOICE_DASH = [(\"\", \"---------\")]\n+\n+\n+def _load_field(app_label, model_name, field_name):\n+    return apps.get_model(app_label, model_name)._meta.get_field(field_name)\n+\n+\n+# A guide to Field parameters:\n+#\n+#   * name:      The name of the field specified in the model.\n+#   * attname:   The attribute to use on the model object. This is the same as\n+#                \"name\", except in the case of ForeignKeys, where \"_id\" is\n+#                appended.\n+#   * db_column: The db_column specified in the model (or None).\n+#   * column:    The database column for this field. This is the same as\n+#                \"attname\", except if db_column is specified.\n+#\n+# Code that introspects values, or does other dynamic things, should use\n+# attname. For example, this gets the primary key value of object \"obj\":\n+#\n+#     getattr(obj, opts.pk.attname)\n+\n+def _empty(of_cls):\n+    new = Empty()\n+    new.__class__ = of_cls\n+    return new\n+\n+\n+def return_None():\n+    return None\n+\n+\n+@total_ordering\n+class Field(RegisterLookupMixin):\n+    \"\"\"Base class for all field types\"\"\"\n+\n+    # Designates whether empty strings fundamentally are allowed at the\n+    # database level.\n+    empty_strings_allowed = True\n+    empty_values = list(validators.EMPTY_VALUES)\n+\n+    # These track each time a Field instance is created. Used to retain order.\n+    # The auto_creation_counter is used for fields that Django implicitly\n+    # creates, creation_counter is used for all user-specified fields.\n+    creation_counter = 0\n+    auto_creation_counter = -1\n+    default_validators = []  # Default set of validators\n+    default_error_messages = {\n+        'invalid_choice': _('Value %(value)r is not a valid choice.'),\n+        'null': _('This field cannot be null.'),\n+        'blank': _('This field cannot be blank.'),\n+        'unique': _('%(model_name)s with this %(field_label)s '\n+                    'already exists.'),\n+        # Translators: The 'lookup_type' is one of 'date', 'year' or 'month'.\n+        # Eg: \"Title must be unique for pub_date year\"\n+        'unique_for_date': _(\"%(field_label)s must be unique for \"\n+                             \"%(date_field_label)s %(lookup_type)s.\"),\n+    }\n+    system_check_deprecated_details = None\n+    system_check_removed_details = None\n+\n+    # Field flags\n+    hidden = False\n+\n+    many_to_many = None\n+    many_to_one = None\n+    one_to_many = None\n+    one_to_one = None\n+    related_model = None\n+\n+    descriptor_class = DeferredAttribute\n+\n+    # Generic field type description, usually overridden by subclasses\n+    def _description(self):\n+        return _('Field of type: %(field_type)s') % {\n+            'field_type': self.__class__.__name__\n+        }\n+    description = property(_description)\n+\n+    def __init__(self, verbose_name=None, name=None, primary_key=False,\n+                 max_length=None, unique=False, blank=False, null=False,\n+                 db_index=False, rel=None, default=NOT_PROVIDED, editable=True,\n+                 serialize=True, unique_for_date=None, unique_for_month=None,\n+                 unique_for_year=None, choices=None, help_text='', db_column=None,\n+                 db_tablespace=None, auto_created=False, validators=(),\n+                 error_messages=None):\n+        self.name = name\n+        self.verbose_name = verbose_name  # May be set by set_attributes_from_name\n+        self._verbose_name = verbose_name  # Store original for deconstruction\n+        self.primary_key = primary_key\n+        self.max_length, self._unique = max_length, unique\n+        self.blank, self.null = blank, null\n+        self.remote_field = rel\n+        self.is_relation = self.remote_field is not None\n+        self.default = default\n+        self.editable = editable\n+        self.serialize = serialize\n+        self.unique_for_date = unique_for_date\n+        self.unique_for_month = unique_for_month\n+        self.unique_for_year = unique_for_year\n+        if isinstance(choices, collections.abc.Iterator):\n+            choices = list(choices)\n+        self.choices = choices\n+        self.help_text = help_text\n+        self.db_index = db_index\n+        self.db_column = db_column\n+        self._db_tablespace = db_tablespace\n+        self.auto_created = auto_created\n+\n+        # Adjust the appropriate creation counter, and save our local copy.\n+        if auto_created:\n+            self.creation_counter = Field.auto_creation_counter\n+            Field.auto_creation_counter -= 1\n+        else:\n+            self.creation_counter = Field.creation_counter\n+            Field.creation_counter += 1\n+\n+        self._validators = list(validators)  # Store for deconstruction later\n+\n+        messages = {}\n+        for c in reversed(self.__class__.__mro__):\n+            messages.update(getattr(c, 'default_error_messages', {}))\n+        messages.update(error_messages or {})\n+        self._error_messages = error_messages  # Store for deconstruction later\n+        self.error_messages = messages\n+\n+    def __str__(self):\n+        \"\"\"\n+        Return \"app_label.model_label.field_name\" for fields attached to\n+        models.\n+        \"\"\"\n+        if not hasattr(self, 'model'):\n+            return super().__str__()\n+        model = self.model\n+        app = model._meta.app_label\n+        return '%s.%s.%s' % (app, model._meta.object_name, self.name)\n+\n+    def __repr__(self):\n+        \"\"\"Display the module, class, and name of the field.\"\"\"\n+        path = '%s.%s' % (self.__class__.__module__, self.__class__.__qualname__)\n+        name = getattr(self, 'name', None)\n+        if name is not None:\n+            return '<%s: %s>' % (path, name)\n+        return '<%s>' % path\n+\n+    def check(self, **kwargs):\n+        return [\n+            *self._check_field_name(),\n+            *self._check_choices(),\n+            *self._check_db_index(),\n+            *self._check_null_allowed_for_primary_keys(),\n+            *self._check_backend_specific_checks(**kwargs),\n+            *self._check_validators(),\n+            *self._check_deprecation_details(),\n+        ]\n+\n+    def _check_field_name(self):\n+        \"\"\"\n+        Check if field name is valid, i.e. 1) does not end with an\n+        underscore, 2) does not contain \"__\" and 3) is not \"pk\".\n+        \"\"\"\n+        if self.name.endswith('_'):\n+            return [\n+                checks.Error(\n+                    'Field names must not end with an underscore.',\n+                    obj=self,\n+                    id='fields.E001',\n+                )\n+            ]\n+        elif LOOKUP_SEP in self.name:\n+            return [\n+                checks.Error(\n+                    'Field names must not contain \"%s\".' % (LOOKUP_SEP,),\n+                    obj=self,\n+                    id='fields.E002',\n+                )\n+            ]\n+        elif self.name == 'pk':\n+            return [\n+                checks.Error(\n+                    \"'pk' is a reserved word that cannot be used as a field name.\",\n+                    obj=self,\n+                    id='fields.E003',\n+                )\n+            ]\n+        else:\n+            return []\n+\n+    def _check_choices(self):\n+        if not self.choices:\n+            return []\n+\n+        def is_value(value, accept_promise=True):\n+            return isinstance(value, (str, Promise) if accept_promise else str) or not is_iterable(value)\n+\n+        if is_value(self.choices, accept_promise=False):\n+            return [\n+                checks.Error(\n+                    \"'choices' must be an iterable (e.g., a list or tuple).\",\n+                    obj=self,\n+                    id='fields.E004',\n+                )\n+            ]\n+\n+        # Expect [group_name, [value, display]]\n+        for choices_group in self.choices:\n+            try:\n+                group_name, group_choices = choices_group\n+            except (TypeError, ValueError):\n+                # Containing non-pairs\n+                break\n+            try:\n+                if not all(\n+                    is_value(value) and is_value(human_name)\n+                    for value, human_name in group_choices\n+                ):\n+                    break\n+            except (TypeError, ValueError):\n+                # No groups, choices in the form [value, display]\n+                value, human_name = group_name, group_choices\n+                if not is_value(value) or not is_value(human_name):\n+                    break\n+\n+            # Special case: choices=['ab']\n+            if isinstance(choices_group, str):\n+                break\n+        else:\n+            return []\n+\n+        return [\n+            checks.Error(\n+                \"'choices' must be an iterable containing \"\n+                \"(actual value, human readable name) tuples.\",\n+                obj=self,\n+                id='fields.E005',\n+            )\n+        ]\n+\n+    def _check_db_index(self):\n+        if self.db_index not in (None, True, False):\n+            return [\n+                checks.Error(\n+                    \"'db_index' must be None, True or False.\",\n+                    obj=self,\n+                    id='fields.E006',\n+                )\n+            ]\n+        else:\n+            return []\n+\n+    def _check_null_allowed_for_primary_keys(self):\n+        if (self.primary_key and self.null and\n+                not connection.features.interprets_empty_strings_as_nulls):\n+            # We cannot reliably check this for backends like Oracle which\n+            # consider NULL and '' to be equal (and thus set up\n+            # character-based fields a little differently).\n+            return [\n+                checks.Error(\n+                    'Primary keys must not have null=True.',\n+                    hint=('Set null=False on the field, or '\n+                          'remove primary_key=True argument.'),\n+                    obj=self,\n+                    id='fields.E007',\n+                )\n+            ]\n+        else:\n+            return []\n+\n+    def _check_backend_specific_checks(self, **kwargs):\n+        app_label = self.model._meta.app_label\n+        for db in connections:\n+            if router.allow_migrate(db, app_label, model_name=self.model._meta.model_name):\n+                return connections[db].validation.check_field(self, **kwargs)\n+        return []\n+\n+    def _check_validators(self):\n+        errors = []\n+        for i, validator in enumerate(self.validators):\n+            if not callable(validator):\n+                errors.append(\n+                    checks.Error(\n+                        \"All 'validators' must be callable.\",\n+                        hint=(\n+                            \"validators[{i}] ({repr}) isn't a function or \"\n+                            \"instance of a validator class.\".format(\n+                                i=i, repr=repr(validator),\n+                            )\n+                        ),\n+                        obj=self,\n+                        id='fields.E008',\n+                    )\n+                )\n+        return errors\n+\n+    def _check_deprecation_details(self):\n+        if self.system_check_removed_details is not None:\n+            return [\n+                checks.Error(\n+                    self.system_check_removed_details.get(\n+                        'msg',\n+                        '%s has been removed except for support in historical '\n+                        'migrations.' % self.__class__.__name__\n+                    ),\n+                    hint=self.system_check_removed_details.get('hint'),\n+                    obj=self,\n+                    id=self.system_check_removed_details.get('id', 'fields.EXXX'),\n+                )\n+            ]\n+        elif self.system_check_deprecated_details is not None:\n+            return [\n+                checks.Warning(\n+                    self.system_check_deprecated_details.get(\n+                        'msg',\n+                        '%s has been deprecated.' % self.__class__.__name__\n+                    ),\n+                    hint=self.system_check_deprecated_details.get('hint'),\n+                    obj=self,\n+                    id=self.system_check_deprecated_details.get('id', 'fields.WXXX'),\n+                )\n+            ]\n+        return []\n+\n+    def get_col(self, alias, output_field=None):\n+        if output_field is None:\n+            output_field = self\n+        if alias != self.model._meta.db_table or output_field != self:\n+            from django.db.models.expressions import Col\n+            return Col(alias, self, output_field)\n+        else:\n+            return self.cached_col\n+\n+    @cached_property\n+    def cached_col(self):\n+        from django.db.models.expressions import Col\n+        return Col(self.model._meta.db_table, self)\n+\n+    def select_format(self, compiler, sql, params):\n+        \"\"\"\n+        Custom format for select clauses. For example, GIS columns need to be\n+        selected as AsText(table.col) on MySQL as the table.col data can't be\n+        used by Django.\n+        \"\"\"\n+        return sql, params\n+\n+    def deconstruct(self):\n+        \"\"\"\n+        Return enough information to recreate the field as a 4-tuple:\n+\n+         * The name of the field on the model, if contribute_to_class() has\n+           been run.\n+         * The import path of the field, including the class:e.g.\n+           django.db.models.IntegerField This should be the most portable\n+           version, so less specific may be better.\n+         * A list of positional arguments.\n+         * A dict of keyword arguments.\n+\n+        Note that the positional or keyword arguments must contain values of\n+        the following types (including inner values of collection types):\n+\n+         * None, bool, str, int, float, complex, set, frozenset, list, tuple,\n+           dict\n+         * UUID\n+         * datetime.datetime (naive), datetime.date\n+         * top-level classes, top-level functions - will be referenced by their\n+           full import path\n+         * Storage instances - these have their own deconstruct() method\n+\n+        This is because the values here must be serialized into a text format\n+        (possibly new Python code, possibly JSON) and these are the only types\n+        with encoding handlers defined.\n+\n+        There's no need to return the exact way the field was instantiated this\n+        time, just ensure that the resulting field is the same - prefer keyword\n+        arguments over positional ones, and omit parameters with their default\n+        values.\n+        \"\"\"\n+        # Short-form way of fetching all the default parameters\n+        keywords = {}\n+        possibles = {\n+            \"verbose_name\": None,\n+            \"primary_key\": False,\n+            \"max_length\": None,\n+            \"unique\": False,\n+            \"blank\": False,\n+            \"null\": False,\n+            \"db_index\": False,\n+            \"default\": NOT_PROVIDED,\n+            \"editable\": True,\n+            \"serialize\": True,\n+            \"unique_for_date\": None,\n+            \"unique_for_month\": None,\n+            \"unique_for_year\": None,\n+            \"choices\": None,\n+            \"help_text\": '',\n+            \"db_column\": None,\n+            \"db_tablespace\": None,\n+            \"auto_created\": False,\n+            \"validators\": [],\n+            \"error_messages\": None,\n+        }\n+        attr_overrides = {\n+            \"unique\": \"_unique\",\n+            \"error_messages\": \"_error_messages\",\n+            \"validators\": \"_validators\",\n+            \"verbose_name\": \"_verbose_name\",\n+            \"db_tablespace\": \"_db_tablespace\",\n+        }\n+        equals_comparison = {\"choices\", \"validators\"}\n+        for name, default in possibles.items():\n+            value = getattr(self, attr_overrides.get(name, name))\n+            # Unroll anything iterable for choices into a concrete list\n+            if name == \"choices\" and isinstance(value, collections.abc.Iterable):\n+                value = list(value)\n+            # Do correct kind of comparison\n+            if name in equals_comparison:\n+                if value != default:\n+                    keywords[name] = value\n+            else:\n+                if value is not default:\n+                    keywords[name] = value\n+        # Work out path - we shorten it for known Django core fields\n+        path = \"%s.%s\" % (self.__class__.__module__, self.__class__.__qualname__)\n+        if path.startswith(\"django.db.models.fields.related\"):\n+            path = path.replace(\"django.db.models.fields.related\", \"django.db.models\")\n+        elif path.startswith(\"django.db.models.fields.files\"):\n+            path = path.replace(\"django.db.models.fields.files\", \"django.db.models\")\n+        elif path.startswith(\"django.db.models.fields.proxy\"):\n+            path = path.replace(\"django.db.models.fields.proxy\", \"django.db.models\")\n+        elif path.startswith(\"django.db.models.fields\"):\n+            path = path.replace(\"django.db.models.fields\", \"django.db.models\")\n+        # Return basic info - other fields should override this.\n+        return (self.name, path, [], keywords)\n+\n+    def clone(self):\n+        \"\"\"\n+        Uses deconstruct() to clone a new copy of this Field.\n+        Will not preserve any class attachments/attribute names.\n+        \"\"\"\n+        name, path, args, kwargs = self.deconstruct()\n+        return self.__class__(*args, **kwargs)\n+\n+    def __eq__(self, other):\n+        # Needed for @total_ordering\n+        if isinstance(other, Field):\n+            return self.creation_counter == other.creation_counter\n+        return NotImplemented\n+\n+    def __lt__(self, other):\n+        # This is needed because bisect does not take a comparison function.\n+        if isinstance(other, Field):\n+            return self.creation_counter < other.creation_counter\n+        return NotImplemented\n+\n+    def __hash__(self):\n+        return hash(self.creation_counter)\n+\n+    def __deepcopy__(self, memodict):\n+        # We don't have to deepcopy very much here, since most things are not\n+        # intended to be altered after initial creation.\n+        obj = copy.copy(self)\n+        if self.remote_field:\n+            obj.remote_field = copy.copy(self.remote_field)\n+            if hasattr(self.remote_field, 'field') and self.remote_field.field is self:\n+                obj.remote_field.field = obj\n+        memodict[id(self)] = obj\n+        return obj\n+\n+    def __copy__(self):\n+        # We need to avoid hitting __reduce__, so define this\n+        # slightly weird copy construct.\n+        obj = Empty()\n+        obj.__class__ = self.__class__\n+        obj.__dict__ = self.__dict__.copy()\n+        return obj\n+\n+    def __reduce__(self):\n+        \"\"\"\n+        Pickling should return the model._meta.fields instance of the field,\n+        not a new copy of that field. So, use the app registry to load the\n+        model and then the field back.\n+        \"\"\"\n+        if not hasattr(self, 'model'):\n+            # Fields are sometimes used without attaching them to models (for\n+            # example in aggregation). In this case give back a plain field\n+            # instance. The code below will create a new empty instance of\n+            # class self.__class__, then update its dict with self.__dict__\n+            # values - so, this is very close to normal pickle.\n+            state = self.__dict__.copy()\n+            # The _get_default cached_property can't be pickled due to lambda\n+            # usage.\n+            state.pop('_get_default', None)\n+            return _empty, (self.__class__,), state\n+        return _load_field, (self.model._meta.app_label, self.model._meta.object_name,\n+                             self.name)\n+\n+    def get_pk_value_on_save(self, instance):\n+        \"\"\"\n+        Hook to generate new PK values on save. This method is called when\n+        saving instances with no primary key value set. If this method returns\n+        something else than None, then the returned value is used when saving\n+        the new instance.\n+        \"\"\"\n+        if self.default:\n+            return self.get_default()\n+        return None\n+\n+    def to_python(self, value):\n+        \"\"\"\n+        Convert the input value into the expected Python data type, raising\n+        django.core.exceptions.ValidationError if the data can't be converted.\n+        Return the converted value. Subclasses should override this.\n+        \"\"\"\n+        return value\n+\n+    @cached_property\n+    def validators(self):\n+        \"\"\"\n+        Some validators can't be created at field initialization time.\n+        This method provides a way to delay their creation until required.\n+        \"\"\"\n+        return [*self.default_validators, *self._validators]\n+\n+    def run_validators(self, value):\n+        if value in self.empty_values:\n+            return\n+\n+        errors = []\n+        for v in self.validators:\n+            try:\n+                v(value)\n+            except exceptions.ValidationError as e:\n+                if hasattr(e, 'code') and e.code in self.error_messages:\n+                    e.message = self.error_messages[e.code]\n+                errors.extend(e.error_list)\n+\n+        if errors:\n+            raise exceptions.ValidationError(errors)\n+\n+    def validate(self, value, model_instance):\n+        \"\"\"\n+        Validate value and raise ValidationError if necessary. Subclasses\n+        should override this to provide validation logic.\n+        \"\"\"\n+        if not self.editable:\n+            # Skip validation for non-editable fields.\n+            return\n+\n+        if self.choices is not None and value not in self.empty_values:\n+            for option_key, option_value in self.choices:\n+                if isinstance(option_value, (list, tuple)):\n+                    # This is an optgroup, so look inside the group for\n+                    # options.\n+                    for optgroup_key, optgroup_value in option_value:\n+                        if value == optgroup_key:\n+                            return\n+                elif value == option_key:\n+                    return\n+            raise exceptions.ValidationError(\n+                self.error_messages['invalid_choice'],\n+                code='invalid_choice',\n+                params={'value': value},\n+            )\n+\n+        if value is None and not self.null:\n+            raise exceptions.ValidationError(self.error_messages['null'], code='null')\n+\n+        if not self.blank and value in self.empty_values:\n+            raise exceptions.ValidationError(self.error_messages['blank'], code='blank')\n+\n+    def clean(self, value, model_instance):\n+        \"\"\"\n+        Convert the value's type and run validation. Validation errors\n+        from to_python() and validate() are propagated. Return the correct\n+        value if no error is raised.\n+        \"\"\"\n+        value = self.to_python(value)\n+        self.validate(value, model_instance)\n+        self.run_validators(value)\n+        return value\n+\n+    def db_type_parameters(self, connection):\n+        return DictWrapper(self.__dict__, connection.ops.quote_name, 'qn_')\n+\n+    def db_check(self, connection):\n+        \"\"\"\n+        Return the database column check constraint for this field, for the\n+        provided connection. Works the same way as db_type() for the case that\n+        get_internal_type() does not map to a preexisting model field.\n+        \"\"\"\n+        data = self.db_type_parameters(connection)\n+        try:\n+            return connection.data_type_check_constraints[self.get_internal_type()] % data\n+        except KeyError:\n+            return None\n+\n+    def db_type(self, connection):\n+        \"\"\"\n+        Return the database column data type for this field, for the provided\n+        connection.\n+        \"\"\"\n+        # The default implementation of this method looks at the\n+        # backend-specific data_types dictionary, looking up the field by its\n+        # \"internal type\".\n+        #\n+        # A Field class can implement the get_internal_type() method to specify\n+        # which *preexisting* Django Field class it's most similar to -- i.e.,\n+        # a custom field might be represented by a TEXT column type, which is\n+        # the same as the TextField Django field type, which means the custom\n+        # field's get_internal_type() returns 'TextField'.\n+        #\n+        # But the limitation of the get_internal_type() / data_types approach\n+        # is that it cannot handle database column types that aren't already\n+        # mapped to one of the built-in Django field types. In this case, you\n+        # can implement db_type() instead of get_internal_type() to specify\n+        # exactly which wacky database column type you want to use.\n+        data = self.db_type_parameters(connection)\n+        try:\n+            return connection.data_types[self.get_internal_type()] % data\n+        except KeyError:\n+            return None\n+\n+    def rel_db_type(self, connection):\n+        \"\"\"\n+        Return the data type that a related field pointing to this field should\n+        use. For example, this method is called by ForeignKey and OneToOneField\n+        to determine its data type.\n+        \"\"\"\n+        return self.db_type(connection)\n+\n+    def cast_db_type(self, connection):\n+        \"\"\"Return the data type to use in the Cast() function.\"\"\"\n+        db_type = connection.ops.cast_data_types.get(self.get_internal_type())\n+        if db_type:\n+            return db_type % self.db_type_parameters(connection)\n+        return self.db_type(connection)\n+\n+    def db_parameters(self, connection):\n+        \"\"\"\n+        Extension of db_type(), providing a range of different return values\n+        (type, checks). This will look at db_type(), allowing custom model\n+        fields to override it.\n+        \"\"\"\n+        type_string = self.db_type(connection)\n+        check_string = self.db_check(connection)\n+        return {\n+            \"type\": type_string,\n+            \"check\": check_string,\n+        }\n+\n+    def db_type_suffix(self, connection):\n+        return connection.data_types_suffix.get(self.get_internal_type())\n+\n+    def get_db_converters(self, connection):\n+        if hasattr(self, 'from_db_value'):\n+            return [self.from_db_value]\n+        return []\n+\n+    @property\n+    def unique(self):\n+        return self._unique or self.primary_key\n+\n+    @property\n+    def db_tablespace(self):\n+        return self._db_tablespace or settings.DEFAULT_INDEX_TABLESPACE\n+\n+    def set_attributes_from_name(self, name):\n+        self.name = self.name or name\n+        self.attname, self.column = self.get_attname_column()\n+        self.concrete = self.column is not None\n+        if self.verbose_name is None and self.name:\n+            self.verbose_name = self.name.replace('_', ' ')\n+\n+    def contribute_to_class(self, cls, name, private_only=False):\n+        \"\"\"\n+        Register the field with the model class it belongs to.\n+\n+        If private_only is True, create a separate instance of this field\n+        for every subclass of cls, even if cls is not an abstract model.\n+        \"\"\"\n+        self.set_attributes_from_name(name)\n+        self.model = cls\n+        cls._meta.add_field(self, private=private_only)\n+        if self.column:\n+            # Don't override classmethods with the descriptor. This means that\n+            # if you have a classmethod and a field with the same name, then\n+            # such fields can't be deferred (we don't have a check for this).\n+            if not getattr(cls, self.attname, None):\n+                setattr(cls, self.attname, self.descriptor_class(self))\n+        if self.choices is not None:\n+            setattr(cls, 'get_%s_display' % self.name,\n+                    partialmethod(cls._get_FIELD_display, field=self))\n+\n+    def get_filter_kwargs_for_object(self, obj):\n+        \"\"\"\n+        Return a dict that when passed as kwargs to self.model.filter(), would\n+        yield all instances having the same value for this field as obj has.\n+        \"\"\"\n+        return {self.name: getattr(obj, self.attname)}\n+\n+    def get_attname(self):\n+        return self.name\n+\n+    def get_attname_column(self):\n+        attname = self.get_attname()\n+        column = self.db_column or attname\n+        return attname, column\n+\n+    def get_internal_type(self):\n+        return self.__class__.__name__\n+\n+    def pre_save(self, model_instance, add):\n+        \"\"\"Return field's value just before saving.\"\"\"\n+        return getattr(model_instance, self.attname)\n+\n+    def get_prep_value(self, value):\n+        \"\"\"Perform preliminary non-db specific value checks and conversions.\"\"\"\n+        if isinstance(value, Promise):\n+            value = value._proxy____cast()\n+        return value\n+\n+    def get_db_prep_value(self, value, connection, prepared=False):\n+        \"\"\"\n+        Return field's value prepared for interacting with the database backend.\n+\n+        Used by the default implementations of get_db_prep_save().\n+        \"\"\"\n+        if not prepared:\n+            value = self.get_prep_value(value)\n+        return value\n+\n+    def get_db_prep_save(self, value, connection):\n+        \"\"\"Return field's value prepared for saving into a database.\"\"\"\n+        return self.get_db_prep_value(value, connection=connection, prepared=False)\n+\n+    def has_default(self):\n+        \"\"\"Return a boolean of whether this field has a default value.\"\"\"\n+        return self.default is not NOT_PROVIDED\n+\n+    def get_default(self):\n+        \"\"\"Return the default value for this field.\"\"\"\n+        return self._get_default()\n+\n+    @cached_property\n+    def _get_default(self):\n+        if self.has_default():\n+            if callable(self.default):\n+                return self.default\n+            return lambda: self.default\n+\n+        if not self.empty_strings_allowed or self.null and not connection.features.interprets_empty_strings_as_nulls:\n+            return return_None\n+        return str  # return empty string\n+\n+    def get_choices(self, include_blank=True, blank_choice=BLANK_CHOICE_DASH, limit_choices_to=None, ordering=()):\n+        \"\"\"\n+        Return choices with a default blank choices included, for use\n+        as <select> choices for this field.\n+        \"\"\"\n+        if self.choices is not None:\n+            choices = list(self.choices)\n+            if include_blank:\n+                blank_defined = any(choice in ('', None) for choice, _ in self.flatchoices)\n+                if not blank_defined:\n+                    choices = blank_choice + choices\n+            return choices\n+        rel_model = self.remote_field.model\n+        limit_choices_to = limit_choices_to or self.get_limit_choices_to()\n+        choice_func = operator.attrgetter(\n+            self.remote_field.get_related_field().attname\n+            if hasattr(self.remote_field, 'get_related_field')\n+            else 'pk'\n+        )\n+        qs = rel_model._default_manager.complex_filter(limit_choices_to)\n+        if ordering:\n+            qs = qs.order_by(*ordering)\n+        return (blank_choice if include_blank else []) + [\n+            (choice_func(x), str(x)) for x in qs\n+        ]\n+\n+    def value_to_string(self, obj):\n+        \"\"\"\n+        Return a string value of this field from the passed obj.\n+        This is used by the serialization framework.\n+        \"\"\"\n+        return str(self.value_from_object(obj))\n+\n+    def _get_flatchoices(self):\n+        \"\"\"Flattened version of choices tuple.\"\"\"\n+        if self.choices is None:\n+            return []\n+        flat = []\n+        for choice, value in self.choices:\n+            if isinstance(value, (list, tuple)):\n+                flat.extend(value)\n+            else:\n+                flat.append((choice, value))\n+        return flat\n+    flatchoices = property(_get_flatchoices)\n+\n+    def save_form_data(self, instance, data):\n+        setattr(instance, self.name, data)\n+\n+    def formfield(self, form_class=None, choices_form_class=None, **kwargs):\n+        \"\"\"Return a django.forms.Field instance for this field.\"\"\"\n+        defaults = {\n+            'required': not self.blank,\n+            'label': capfirst(self.verbose_name),\n+            'help_text': self.help_text,\n+        }\n+        if self.has_default():\n+            if callable(self.default):\n+                defaults['initial'] = self.default\n+                defaults['show_hidden_initial'] = True\n+            else:\n+                defaults['initial'] = self.get_default()\n+        if self.choices is not None:\n+            # Fields with choices get special treatment.\n+            include_blank = (self.blank or\n+                             not (self.has_default() or 'initial' in kwargs))\n+            defaults['choices'] = self.get_choices(include_blank=include_blank)\n+            defaults['coerce'] = self.to_python\n+            if self.null:\n+                defaults['empty_value'] = None\n+            if choices_form_class is not None:\n+                form_class = choices_form_class\n+            else:\n+                form_class = forms.TypedChoiceField\n+            # Many of the subclass-specific formfield arguments (min_value,\n+            # max_value) don't apply for choice fields, so be sure to only pass\n+            # the values that TypedChoiceField will understand.\n+            for k in list(kwargs):\n+                if k not in ('coerce', 'empty_value', 'choices', 'required',\n+                             'widget', 'label', 'initial', 'help_text',\n+                             'error_messages', 'show_hidden_initial', 'disabled'):\n+                    del kwargs[k]\n+        defaults.update(kwargs)\n+        if form_class is None:\n+            form_class = forms.CharField\n+        return form_class(**defaults)\n+\n+    def value_from_object(self, obj):\n+        \"\"\"Return the value of this field in the given model instance.\"\"\"\n+        return getattr(obj, self.attname)\n+\n+\n+class BooleanField(Field):\n+    empty_strings_allowed = False\n+    default_error_messages = {\n+        'invalid': _('\u201c%(value)s\u201d value must be either True or False.'),\n+        'invalid_nullable': _('\u201c%(value)s\u201d value must be either True, False, or None.'),\n+    }\n+    description = _(\"Boolean (Either True or False)\")\n+\n+    def get_internal_type(self):\n+        return \"BooleanField\"\n+\n+    def to_python(self, value):\n+        if self.null and value in self.empty_values:\n+            return None\n+        if value in (True, False):\n+            # 1/0 are equal to True/False. bool() converts former to latter.\n+            return bool(value)\n+        if value in ('t', 'True', '1'):\n+            return True\n+        if value in ('f', 'False', '0'):\n+            return False\n+        raise exceptions.ValidationError(\n+            self.error_messages['invalid_nullable' if self.null else 'invalid'],\n+            code='invalid',\n+            params={'value': value},\n+        )\n+\n+    def get_prep_value(self, value):\n+        value = super().get_prep_value(value)\n+        if value is None:\n+            return None\n+        return self.to_python(value)\n+\n+    def formfield(self, **kwargs):\n+        if self.choices is not None:\n+            include_blank = not (self.has_default() or 'initial' in kwargs)\n+            defaults = {'choices': self.get_choices(include_blank=include_blank)}\n+        else:\n+            form_class = forms.NullBooleanField if self.null else forms.BooleanField\n+            # In HTML checkboxes, 'required' means \"must be checked\" which is\n+            # different from the choices case (\"must select some value\").\n+            # required=False allows unchecked checkboxes.\n+            defaults = {'form_class': form_class, 'required': False}\n+        return super().formfield(**{**defaults, **kwargs})\n+\n+\n+class CharField(Field):\n+    description = _(\"String (up to %(max_length)s)\")\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.validators.append(validators.MaxLengthValidator(self.max_length))\n+\n+    def check(self, **kwargs):\n+        return [\n+            *super().check(**kwargs),\n+            *self._check_max_length_attribute(**kwargs),\n+        ]\n+\n+    def _check_max_length_attribute(self, **kwargs):\n+        if self.max_length is None:\n+            return [\n+                checks.Error(\n+                    \"CharFields must define a 'max_length' attribute.\",\n+                    obj=self,\n+                    id='fields.E120',\n+                )\n+            ]\n+        elif (not isinstance(self.max_length, int) or isinstance(self.max_length, bool) or\n+                self.max_length <= 0):\n+            return [\n+                checks.Error(\n+                    \"'max_length' must be a positive integer.\",\n+                    obj=self,\n+                    id='fields.E121',\n+                )\n+            ]\n+        else:\n+            return []\n+\n+    def cast_db_type(self, connection):\n+        if self.max_length is None:\n+            return connection.ops.cast_char_field_without_max_length\n+        return super().cast_db_type(connection)\n+\n+    def get_internal_type(self):\n+        return \"CharField\"\n+\n+    def to_python(self, value):\n+        if isinstance(value, str) or value is None:\n+            return value\n+        return str(value)\n+\n+    def get_prep_value(self, value):\n+        value = super().get_prep_value(value)\n+        return self.to_python(value)\n+\n+    def formfield(self, **kwargs):\n+        # Passing max_length to forms.CharField means that the value's length\n+        # will be validated twice. This is considered acceptable since we want\n+        # the value in the form field (to pass into widget for example).\n+        defaults = {'max_length': self.max_length}\n+        # TODO: Handle multiple backends with different feature flags.\n+        if self.null and not connection.features.interprets_empty_strings_as_nulls:\n+            defaults['empty_value'] = None\n+        defaults.update(kwargs)\n+        return super().formfield(**defaults)\n+\n+\n+class CommaSeparatedIntegerField(CharField):\n+    default_validators = [validators.validate_comma_separated_integer_list]\n+    description = _(\"Comma-separated integers\")\n+    system_check_removed_details = {\n+        'msg': (\n+            'CommaSeparatedIntegerField is removed except for support in '\n+            'historical migrations.'\n+        ),\n+        'hint': (\n+            'Use CharField(validators=[validate_comma_separated_integer_list]) '\n+            'instead.'\n+        ),\n+        'id': 'fields.E901',\n+    }\n+\n+\n+class DateTimeCheckMixin:\n+\n+    def check(self, **kwargs):\n+        return [\n+            *super().check(**kwargs),\n+            *self._check_mutually_exclusive_options(),\n+            *self._check_fix_default_value(),\n+        ]\n+\n+    def _check_mutually_exclusive_options(self):\n+        # auto_now, auto_now_add, and default are mutually exclusive\n+        # options. The use of more than one of these options together\n+        # will trigger an Error\n+        mutually_exclusive_options = [self.auto_now_add, self.auto_now, self.has_default()]\n+        enabled_options = [option not in (None, False) for option in mutually_exclusive_options].count(True)\n+        if enabled_options > 1:\n+            return [\n+                checks.Error(\n+                    \"The options auto_now, auto_now_add, and default \"\n+                    \"are mutually exclusive. Only one of these options \"\n+                    \"may be present.\",\n+                    obj=self,\n+                    id='fields.E160',\n+                )\n+            ]\n+        else:\n+            return []\n+\n+    def _check_fix_default_value(self):\n+        return []\n+\n+\n+class DateField(DateTimeCheckMixin, Field):\n+    empty_strings_allowed = False\n+    default_error_messages = {\n+        'invalid': _('\u201c%(value)s\u201d value has an invalid date format. It must be '\n+                     'in YYYY-MM-DD format.'),\n+        'invalid_date': _('\u201c%(value)s\u201d value has the correct format (YYYY-MM-DD) '\n+                          'but it is an invalid date.'),\n+    }\n+    description = _(\"Date (without time)\")\n+\n+    def __init__(self, verbose_name=None, name=None, auto_now=False,\n+                 auto_now_add=False, **kwargs):\n+        self.auto_now, self.auto_now_add = auto_now, auto_now_add\n+        if auto_now or auto_now_add:\n+            kwargs['editable'] = False\n+            kwargs['blank'] = True\n+        super().__init__(verbose_name, name, **kwargs)\n+\n+    def _check_fix_default_value(self):\n+        \"\"\"\n+        Warn that using an actual date or datetime value is probably wrong;\n+        it's only evaluated on server startup.\n+        \"\"\"\n+        if not self.has_default():\n+            return []\n+\n+        now = timezone.now()\n+        if not timezone.is_naive(now):\n+            now = timezone.make_naive(now, timezone.utc)\n+        value = self.default\n+        if isinstance(value, datetime.datetime):\n+            if not timezone.is_naive(value):\n+                value = timezone.make_naive(value, timezone.utc)\n+            value = value.date()\n+        elif isinstance(value, datetime.date):\n+            # Nothing to do, as dates don't have tz information\n+            pass\n+        else:\n+            # No explicit date / datetime value -- no checks necessary\n+            return []\n+        offset = datetime.timedelta(days=1)\n+        lower = (now - offset).date()\n+        upper = (now + offset).date()\n+        if lower <= value <= upper:\n+            return [\n+                checks.Warning(\n+                    'Fixed default value provided.',\n+                    hint='It seems you set a fixed date / time / datetime '\n+                         'value as default for this field. This may not be '\n+                         'what you want. If you want to have the current date '\n+                         'as default, use `django.utils.timezone.now`',\n+                    obj=self,\n+                    id='fields.W161',\n+                )\n+            ]\n+\n+        return []\n+\n+    def deconstruct(self):\n+        name, path, args, kwargs = super().deconstruct()\n+        if self.auto_now:\n+            kwargs['auto_now'] = True\n+        if self.auto_now_add:\n+            kwargs['auto_now_add'] = True\n+        if self.auto_now or self.auto_now_add:\n+            del kwargs['editable']\n+            del kwargs['blank']\n+        return name, path, args, kwargs\n+\n+    def get_internal_type(self):\n+        return \"DateField\"\n+\n+    def to_python(self, value):\n+        if value is None:\n+            return value\n+        if isinstance(value, datetime.datetime):\n+            if settings.USE_TZ and timezone.is_aware(value):\n+                # Convert aware datetimes to the default time zone\n+                # before casting them to dates (#17742).\n+                default_timezone = timezone.get_default_timezone()\n+                value = timezone.make_naive(value, default_timezone)\n+            return value.date()\n+        if isinstance(value, datetime.date):\n+            return value\n+\n+        try:\n+            parsed = parse_date(value)\n+            if parsed is not None:\n+                return parsed\n+        except ValueError:\n+            raise exceptions.ValidationError(\n+                self.error_messages['invalid_date'],\n+                code='invalid_date',\n+                params={'value': value},\n+            )\n+\n+        raise exceptions.ValidationError(\n+            self.error_messages['invalid'],\n+            code='invalid',\n+            params={'value': value},\n+        )\n+\n+    def pre_save(self, model_instance, add):\n+        if self.auto_now or (self.auto_now_add and add):\n+            value = datetime.date.today()\n+            setattr(model_instance, self.attname, value)\n+            return value\n+        else:\n+            return super().pre_save(model_instance, add)\n+\n+    def contribute_to_class(self, cls, name, **kwargs):\n+        super().contribute_to_class(cls, name, **kwargs)\n+        if not self.null:\n+            setattr(\n+                cls, 'get_next_by_%s' % self.name,\n+                partialmethod(cls._get_next_or_previous_by_FIELD, field=self, is_next=True)\n+            )\n+            setattr(\n+                cls, 'get_previous_by_%s' % self.name,\n+                partialmethod(cls._get_next_or_previous_by_FIELD, field=self, is_next=False)\n+            )\n+\n+    def get_prep_value(self, value):\n+        value = super().get_prep_value(value)\n+        return self.to_python(value)\n+\n+    def get_db_prep_value(self, value, connection, prepared=False):\n+        # Casts dates into the format expected by the backend\n+        if not prepared:\n+            value = self.get_prep_value(value)\n+        return connection.ops.adapt_datefield_value(value)\n+\n+    def value_to_string(self, obj):\n+        val = self.value_from_object(obj)\n+        return '' if val is None else val.isoformat()\n+\n+    def formfield(self, **kwargs):\n+        return super().formfield(**{\n+            'form_class': forms.DateField,\n+            **kwargs,\n+        })\n+\n+\n+class DateTimeField(DateField):\n+    empty_strings_allowed = False\n+    default_error_messages = {\n+        'invalid': _('\u201c%(value)s\u201d value has an invalid format. It must be in '\n+                     'YYYY-MM-DD HH:MM[:ss[.uuuuuu]][TZ] format.'),\n+        'invalid_date': _(\"\u201c%(value)s\u201d value has the correct format \"\n+                          \"(YYYY-MM-DD) but it is an invalid date.\"),\n+        'invalid_datetime': _('\u201c%(value)s\u201d value has the correct format '\n+                              '(YYYY-MM-DD HH:MM[:ss[.uuuuuu]][TZ]) '\n+                              'but it is an invalid date/time.'),\n+    }\n+    description = _(\"Date (with time)\")\n+\n+    # __init__ is inherited from DateField\n+\n+    def _check_fix_default_value(self):\n+        \"\"\"\n+        Warn that using an actual date or datetime value is probably wrong;\n+        it's only evaluated on server startup.\n+        \"\"\"\n+        if not self.has_default():\n+            return []\n+\n+        now = timezone.now()\n+        if not timezone.is_naive(now):\n+            now = timezone.make_naive(now, timezone.utc)\n+        value = self.default\n+        if isinstance(value, datetime.datetime):\n+            second_offset = datetime.timedelta(seconds=10)\n+            lower = now - second_offset\n+            upper = now + second_offset\n+            if timezone.is_aware(value):\n+                value = timezone.make_naive(value, timezone.utc)\n+        elif isinstance(value, datetime.date):\n+            second_offset = datetime.timedelta(seconds=10)\n+            lower = now - second_offset\n+            lower = datetime.datetime(lower.year, lower.month, lower.day)\n+            upper = now + second_offset\n+            upper = datetime.datetime(upper.year, upper.month, upper.day)\n+            value = datetime.datetime(value.year, value.month, value.day)\n+        else:\n+            # No explicit date / datetime value -- no checks necessary\n+            return []\n+        if lower <= value <= upper:\n+            return [\n+                checks.Warning(\n+                    'Fixed default value provided.',\n+                    hint='It seems you set a fixed date / time / datetime '\n+                         'value as default for this field. This may not be '\n+                         'what you want. If you want to have the current date '\n+                         'as default, use `django.utils.timezone.now`',\n+                    obj=self,\n+                    id='fields.W161',\n+                )\n+            ]\n+\n+        return []\n+\n+    def get_internal_type(self):\n+        return \"DateTimeField\"\n+\n+    def to_python(self, value):\n+        if value is None:\n+            return value\n+        if isinstance(value, datetime.datetime):\n+            return value\n+        if isinstance(value, datetime.date):\n+            value = datetime.datetime(value.year, value.month, value.day)\n+            if settings.USE_TZ:\n+                # For backwards compatibility, interpret naive datetimes in\n+                # local time. This won't work during DST change, but we can't\n+                # do much about it, so we let the exceptions percolate up the\n+                # call stack.\n+                warnings.warn(\"DateTimeField %s.%s received a naive datetime \"\n+                              \"(%s) while time zone support is active.\" %\n+                              (self.model.__name__, self.name, value),\n+                              RuntimeWarning)\n+                default_timezone = timezone.get_default_timezone()\n+                value = timezone.make_aware(value, default_timezone)\n+            return value\n+\n+        try:\n+            parsed = parse_datetime(value)\n+            if parsed is not None:\n+                return parsed\n+        except ValueError:\n+            raise exceptions.ValidationError(\n+                self.error_messages['invalid_datetime'],\n+                code='invalid_datetime',\n+                params={'value': value},\n+            )\n+\n+        try:\n+            parsed = parse_date(value)\n+            if parsed is not None:\n+                return datetime.datetime(parsed.year, parsed.month, parsed.day)\n+        except ValueError:\n+            raise exceptions.ValidationError(\n+                self.error_messages['invalid_date'],\n+                code='invalid_date',\n+                params={'value': value},\n+            )\n+\n+        raise exceptions.ValidationError(\n+            self.error_messages['invalid'],\n+            code='invalid',\n+            params={'value': value},\n+        )\n+\n+    def pre_save(self, model_instance, add):\n+        if self.auto_now or (self.auto_now_add and add):\n+            value = timezone.now()\n+            setattr(model_instance, self.attname, value)\n+            return value\n+        else:\n+            return super().pre_save(model_instance, add)\n+\n+    # contribute_to_class is inherited from DateField, it registers\n+    # get_next_by_FOO and get_prev_by_FOO\n+\n+    def get_prep_value(self, value):\n+        value = super().get_prep_value(value)\n+        value = self.to_python(value)\n+        if value is not None and settings.USE_TZ and timezone.is_naive(value):\n+            # For backwards compatibility, interpret naive datetimes in local\n+            # time. This won't work during DST change, but we can't do much\n+            # about it, so we let the exceptions percolate up the call stack.\n+            try:\n+                name = '%s.%s' % (self.model.__name__, self.name)\n+            except AttributeError:\n+                name = '(unbound)'\n+            warnings.warn(\"DateTimeField %s received a naive datetime (%s)\"\n+                          \" while time zone support is active.\" %\n+                          (name, value),\n+                          RuntimeWarning)\n+            default_timezone = timezone.get_default_timezone()\n+            value = timezone.make_aware(value, default_timezone)\n+        return value\n+\n+    def get_db_prep_value(self, value, connection, prepared=False):\n+        # Casts datetimes into the format expected by the backend\n+        if not prepared:\n+            value = self.get_prep_value(value)\n+        return connection.ops.adapt_datetimefield_value(value)\n+\n+    def value_to_string(self, obj):\n+        val = self.value_from_object(obj)\n+        return '' if val is None else val.isoformat()\n+\n+    def formfield(self, **kwargs):\n+        return super().formfield(**{\n+            'form_class': forms.DateTimeField,\n+            **kwargs,\n+        })\n+\n+\n+class DecimalField(Field):\n+    empty_strings_allowed = False\n+    default_error_messages = {\n+        'invalid': _('\u201c%(value)s\u201d value must be a decimal number.'),\n+    }\n+    description = _(\"Decimal number\")\n+\n+    def __init__(self, verbose_name=None, name=None, max_digits=None,\n+                 decimal_places=None, **kwargs):\n+        self.max_digits, self.decimal_places = max_digits, decimal_places\n+        super().__init__(verbose_name, name, **kwargs)\n+\n+    def check(self, **kwargs):\n+        errors = super().check(**kwargs)\n+\n+        digits_errors = [\n+            *self._check_decimal_places(),\n+            *self._check_max_digits(),\n+        ]\n+        if not digits_errors:\n+            errors.extend(self._check_decimal_places_and_max_digits(**kwargs))\n+        else:\n+            errors.extend(digits_errors)\n+        return errors\n+\n+    def _check_decimal_places(self):\n+        try:\n+            decimal_places = int(self.decimal_places)\n+            if decimal_places < 0:\n+                raise ValueError()\n+        except TypeError:\n+            return [\n+                checks.Error(\n+                    \"DecimalFields must define a 'decimal_places' attribute.\",\n+                    obj=self,\n+                    id='fields.E130',\n+                )\n+            ]\n+        except ValueError:\n+            return [\n+                checks.Error(\n+                    \"'decimal_places' must be a non-negative integer.\",\n+                    obj=self,\n+                    id='fields.E131',\n+                )\n+            ]\n+        else:\n+            return []\n+\n+    def _check_max_digits(self):\n+        try:\n+            max_digits = int(self.max_digits)\n+            if max_digits <= 0:\n+                raise ValueError()\n+        except TypeError:\n+            return [\n+                checks.Error(\n+                    \"DecimalFields must define a 'max_digits' attribute.\",\n+                    obj=self,\n+                    id='fields.E132',\n+                )\n+            ]\n+        except ValueError:\n+            return [\n+                checks.Error(\n+                    \"'max_digits' must be a positive integer.\",\n+                    obj=self,\n+                    id='fields.E133',\n+                )\n+            ]\n+        else:\n+            return []\n+\n+    def _check_decimal_places_and_max_digits(self, **kwargs):\n+        if int(self.decimal_places) > int(self.max_digits):\n+            return [\n+                checks.Error(\n+                    \"'max_digits' must be greater or equal to 'decimal_places'.\",\n+                    obj=self,\n+                    id='fields.E134',\n+                )\n+            ]\n+        return []\n+\n+    @cached_property\n+    def validators(self):\n+        return super().validators + [\n+            validators.DecimalValidator(self.max_digits, self.decimal_places)\n+        ]\n+\n+    @cached_property\n+    def context(self):\n+        return decimal.Context(prec=self.max_digits)\n+\n+    def deconstruct(self):\n+        name, path, args, kwargs = super().deconstruct()\n+        if self.max_digits is not None:\n+            kwargs['max_digits'] = self.max_digits\n+        if self.decimal_places is not None:\n+            kwargs['decimal_places'] = self.decimal_places\n+        return name, path, args, kwargs\n+\n+    def get_internal_type(self):\n+        return \"DecimalField\"\n+\n+    def to_python(self, value):\n+        if value is None:\n+            return value\n+        if isinstance(value, float):\n+            return self.context.create_decimal_from_float(value)\n+        try:\n+            return decimal.Decimal(value)\n+        except decimal.InvalidOperation:\n+            raise exceptions.ValidationError(\n+                self.error_messages['invalid'],\n+                code='invalid',\n+                params={'value': value},\n+            )\n+\n+    def get_db_prep_save(self, value, connection):\n+        return connection.ops.adapt_decimalfield_value(self.to_python(value), self.max_digits, self.decimal_places)\n+\n+    def get_prep_value(self, value):\n+        value = super().get_prep_value(value)\n+        return self.to_python(value)\n+\n+    def formfield(self, **kwargs):\n+        return super().formfield(**{\n+            'max_digits': self.max_digits,\n+            'decimal_places': self.decimal_places,\n+            'form_class': forms.DecimalField,\n+            **kwargs,\n+        })\n+\n+\n+class DurationField(Field):\n+    \"\"\"\n+    Store timedelta objects.\n+\n+    Use interval on PostgreSQL, INTERVAL DAY TO SECOND on Oracle, and bigint\n+    of microseconds on other databases.\n+    \"\"\"\n+    empty_strings_allowed = False\n+    default_error_messages = {\n+        'invalid': _('\u201c%(value)s\u201d value has an invalid format. It must be in '\n+                     '[DD] [[HH:]MM:]ss[.uuuuuu] format.')\n+    }\n+    description = _(\"Duration\")\n+\n+    def get_internal_type(self):\n+        return \"DurationField\"\n+\n+    def to_python(self, value):\n+        if value is None:\n+            return value\n+        if isinstance(value, datetime.timedelta):\n+            return value\n+        try:\n+            parsed = parse_duration(value)\n+        except ValueError:\n+            pass\n+        else:\n+            if parsed is not None:\n+                return parsed\n+\n+        raise exceptions.ValidationError(\n+            self.error_messages['invalid'],\n+            code='invalid',\n+            params={'value': value},\n+        )\n+\n+    def get_db_prep_value(self, value, connection, prepared=False):\n+        if connection.features.has_native_duration_field:\n+            return value\n+        if value is None:\n+            return None\n+        return duration_microseconds(value)\n+\n+    def get_db_converters(self, connection):\n+        converters = []\n+        if not connection.features.has_native_duration_field:\n+            converters.append(connection.ops.convert_durationfield_value)\n+        return converters + super().get_db_converters(connection)\n+\n+    def value_to_string(self, obj):\n+        val = self.value_from_object(obj)\n+        return '' if val is None else duration_string(val)\n+\n+    def formfield(self, **kwargs):\n+        return super().formfield(**{\n+            'form_class': forms.DurationField,\n+            **kwargs,\n+        })\n+\n+\n+class EmailField(CharField):\n+    default_validators = [validators.validate_email]\n+    description = _(\"Email address\")\n+\n+    def __init__(self, *args, **kwargs):\n+        # max_length=254 to be compliant with RFCs 3696 and 5321\n+        kwargs.setdefault('max_length', 254)\n+        super().__init__(*args, **kwargs)\n+\n+    def deconstruct(self):\n+        name, path, args, kwargs = super().deconstruct()\n+        # We do not exclude max_length if it matches default as we want to change\n+        # the default in future.\n+        return name, path, args, kwargs\n+\n+    def formfield(self, **kwargs):\n+        # As with CharField, this will cause email validation to be performed\n+        # twice.\n+        return super().formfield(**{\n+            'form_class': forms.EmailField,\n+            **kwargs,\n+        })\n+\n+\n+class FilePathField(Field):\n+    description = _(\"File path\")\n+\n+    def __init__(self, verbose_name=None, name=None, path='', match=None,\n+                 recursive=False, allow_files=True, allow_folders=False, **kwargs):\n+        self.path, self.match, self.recursive = path, match, recursive\n+        self.allow_files, self.allow_folders = allow_files, allow_folders\n+        kwargs.setdefault('max_length', 100)\n+        super().__init__(verbose_name, name, **kwargs)\n+\n+    def check(self, **kwargs):\n+        return [\n+            *super().check(**kwargs),\n+            *self._check_allowing_files_or_folders(**kwargs),\n+        ]\n+\n+    def _check_allowing_files_or_folders(self, **kwargs):\n+        if not self.allow_files and not self.allow_folders:\n+            return [\n+                checks.Error(\n+                    \"FilePathFields must have either 'allow_files' or 'allow_folders' set to True.\",\n+                    obj=self,\n+                    id='fields.E140',\n+                )\n+            ]\n+        return []\n+\n+    def deconstruct(self):\n+        name, path, args, kwargs = super().deconstruct()\n+        if self.path != '':\n+            kwargs['path'] = self.path\n+        if self.match is not None:\n+            kwargs['match'] = self.match\n+        if self.recursive is not False:\n+            kwargs['recursive'] = self.recursive\n+        if self.allow_files is not True:\n+            kwargs['allow_files'] = self.allow_files\n+        if self.allow_folders is not False:\n+            kwargs['allow_folders'] = self.allow_folders\n+        if kwargs.get(\"max_length\") == 100:\n+            del kwargs[\"max_length\"]\n+        return name, path, args, kwargs\n+\n+    def get_prep_value(self, value):\n+        value = super().get_prep_value(value)\n+        if value is None:\n+            return None\n+        return str(value)\n+\n+    def formfield(self, **kwargs):\n+        return super().formfield(**{\n+            'path': self.path() if callable(self.path) else self.path,\n+            'match': self.match,\n+            'recursive': self.recursive,\n+            'form_class': forms.FilePathField,\n+            'allow_files': self.allow_files,\n+            'allow_folders': self.allow_folders,\n+            **kwargs,\n+        })\n+\n+    def get_internal_type(self):\n+        return \"FilePathField\"\n+\n+\n+class FloatField(Field):\n+    empty_strings_allowed = False\n+    default_error_messages = {\n+        'invalid': _('\u201c%(value)s\u201d value must be a float.'),\n+    }\n+    description = _(\"Floating point number\")\n+\n+    def get_prep_value(self, value):\n+        value = super().get_prep_value(value)\n+        if value is None:\n+            return None\n+        try:\n+            return float(value)\n+        except (TypeError, ValueError) as e:\n+            raise e.__class__(\n+                \"Field '%s' expected a number but got %r.\" % (self.name, value),\n+            ) from e\n+\n+    def get_internal_type(self):\n+        return \"FloatField\"\n+\n+    def to_python(self, value):\n+        if value is None:\n+            return value\n+        try:\n+            return float(value)\n+        except (TypeError, ValueError):\n+            raise exceptions.ValidationError(\n+                self.error_messages['invalid'],\n+                code='invalid',\n+                params={'value': value},\n+            )\n+\n+    def formfield(self, **kwargs):\n+        return super().formfield(**{\n+            'form_class': forms.FloatField,\n+            **kwargs,\n+        })\n+\n+\n+class IntegerField(Field):\n+    empty_strings_allowed = False\n+    default_error_messages = {\n+        'invalid': _('\u201c%(value)s\u201d value must be an integer.'),\n+    }\n+    description = _(\"Integer\")\n+\n+    def check(self, **kwargs):\n+        return [\n+            *super().check(**kwargs),\n+            *self._check_max_length_warning(),\n+        ]\n+\n+    def _check_max_length_warning(self):\n+        if self.max_length is not None:\n+            return [\n+                checks.Warning(\n+                    \"'max_length' is ignored when used with %s.\" % self.__class__.__name__,\n+                    hint=\"Remove 'max_length' from field\",\n+                    obj=self,\n+                    id='fields.W122',\n+                )\n+            ]\n+        return []\n+\n+    @cached_property\n+    def validators(self):\n+        # These validators can't be added at field initialization time since\n+        # they're based on values retrieved from `connection`.\n+        validators_ = super().validators\n+        internal_type = self.get_internal_type()\n+        min_value, max_value = connection.ops.integer_field_range(internal_type)\n+        if min_value is not None and not any(\n+            (\n+                isinstance(validator, validators.MinValueValidator) and (\n+                    validator.limit_value()\n+                    if callable(validator.limit_value)\n+                    else validator.limit_value\n+                ) >= min_value\n+            ) for validator in validators_\n+        ):\n+            validators_.append(validators.MinValueValidator(min_value))\n+        if max_value is not None and not any(\n+            (\n+                isinstance(validator, validators.MaxValueValidator) and (\n+                    validator.limit_value()\n+                    if callable(validator.limit_value)\n+                    else validator.limit_value\n+                ) <= max_value\n+            ) for validator in validators_\n+        ):\n+            validators_.append(validators.MaxValueValidator(max_value))\n+        return validators_\n+\n+    def get_prep_value(self, value):\n+        value = super().get_prep_value(value)\n+        if value is None:\n+            return None\n+        try:\n+            return int(value)\n+        except (TypeError, ValueError) as e:\n+            raise e.__class__(\n+                \"Field '%s' expected a number but got %r.\" % (self.name, value),\n+            ) from e\n+\n+    def get_internal_type(self):\n+        return \"IntegerField\"\n+\n+    def to_python(self, value):\n+        if value is None:\n+            return value\n+        try:\n+            return int(value)\n+        except (TypeError, ValueError):\n+            raise exceptions.ValidationError(\n+                self.error_messages['invalid'],\n+                code='invalid',\n+                params={'value': value},\n+            )\n+\n+    def formfield(self, **kwargs):\n+        return super().formfield(**{\n+            'form_class': forms.IntegerField,\n+            **kwargs,\n+        })\n+\n+\n+class BigIntegerField(IntegerField):\n+    description = _(\"Big (8 byte) integer\")\n+    MAX_BIGINT = 9223372036854775807\n+\n+    def get_internal_type(self):\n+        return \"BigIntegerField\"\n+\n+    def formfield(self, **kwargs):\n+        return super().formfield(**{\n+            'min_value': -BigIntegerField.MAX_BIGINT - 1,\n+            'max_value': BigIntegerField.MAX_BIGINT,\n+            **kwargs,\n+        })\n+\n+\n+class IPAddressField(Field):\n+    empty_strings_allowed = False\n+    description = _(\"IPv4 address\")\n+    system_check_removed_details = {\n+        'msg': (\n+            'IPAddressField has been removed except for support in '\n+            'historical migrations.'\n+        ),\n+        'hint': 'Use GenericIPAddressField instead.',\n+        'id': 'fields.E900',\n+    }\n+\n+    def __init__(self, *args, **kwargs):\n+        kwargs['max_length'] = 15\n+        super().__init__(*args, **kwargs)\n+\n+    def deconstruct(self):\n+        name, path, args, kwargs = super().deconstruct()\n+        del kwargs['max_length']\n+        return name, path, args, kwargs\n+\n+    def get_prep_value(self, value):\n+        value = super().get_prep_value(value)\n+        if value is None:\n+            return None\n+        return str(value)\n+\n+    def get_internal_type(self):\n+        return \"IPAddressField\"\n+\n+\n+class GenericIPAddressField(Field):\n+    empty_strings_allowed = False\n+    description = _(\"IP address\")\n+    default_error_messages = {}\n+\n+    def __init__(self, verbose_name=None, name=None, protocol='both',\n+                 unpack_ipv4=False, *args, **kwargs):\n+        self.unpack_ipv4 = unpack_ipv4\n+        self.protocol = protocol\n+        self.default_validators, invalid_error_message = \\\n+            validators.ip_address_validators(protocol, unpack_ipv4)\n+        self.default_error_messages['invalid'] = invalid_error_message\n+        kwargs['max_length'] = 39\n+        super().__init__(verbose_name, name, *args, **kwargs)\n+\n+    def check(self, **kwargs):\n+        return [\n+            *super().check(**kwargs),\n+            *self._check_blank_and_null_values(**kwargs),\n+        ]\n+\n+    def _check_blank_and_null_values(self, **kwargs):\n+        if not getattr(self, 'null', False) and getattr(self, 'blank', False):\n+            return [\n+                checks.Error(\n+                    'GenericIPAddressFields cannot have blank=True if null=False, '\n+                    'as blank values are stored as nulls.',\n+                    obj=self,\n+                    id='fields.E150',\n+                )\n+            ]\n+        return []\n+\n+    def deconstruct(self):\n+        name, path, args, kwargs = super().deconstruct()\n+        if self.unpack_ipv4 is not False:\n+            kwargs['unpack_ipv4'] = self.unpack_ipv4\n+        if self.protocol != \"both\":\n+            kwargs['protocol'] = self.protocol\n+        if kwargs.get(\"max_length\") == 39:\n+            del kwargs['max_length']\n+        return name, path, args, kwargs\n+\n+    def get_internal_type(self):\n+        return \"GenericIPAddressField\"\n+\n+    def to_python(self, value):\n+        if value is None:\n+            return None\n+        if not isinstance(value, str):\n+            value = str(value)\n+        value = value.strip()\n+        if ':' in value:\n+            return clean_ipv6_address(value, self.unpack_ipv4, self.error_messages['invalid'])\n+        return value\n+\n+    def get_db_prep_value(self, value, connection, prepared=False):\n+        if not prepared:\n+            value = self.get_prep_value(value)\n+        return connection.ops.adapt_ipaddressfield_value(value)\n+\n+    def get_prep_value(self, value):\n+        value = super().get_prep_value(value)\n+        if value is None:\n+            return None\n+        if value and ':' in value:\n+            try:\n+                return clean_ipv6_address(value, self.unpack_ipv4)\n+            except exceptions.ValidationError:\n+                pass\n+        return str(value)\n+\n+    def formfield(self, **kwargs):\n+        return super().formfield(**{\n+            'protocol': self.protocol,\n+            'form_class': forms.GenericIPAddressField,\n+            **kwargs,\n+        })\n+\n+\n+class NullBooleanField(BooleanField):\n+    default_error_messages = {\n+        'invalid': _('\u201c%(value)s\u201d value must be either None, True or False.'),\n+        'invalid_nullable': _('\u201c%(value)s\u201d value must be either None, True or False.'),\n+    }\n+    description = _(\"Boolean (Either True, False or None)\")\n+\n+    def __init__(self, *args, **kwargs):\n+        kwargs['null'] = True\n+        kwargs['blank'] = True\n+        super().__init__(*args, **kwargs)\n+\n+    def deconstruct(self):\n+        name, path, args, kwargs = super().deconstruct()\n+        del kwargs['null']\n+        del kwargs['blank']\n+        return name, path, args, kwargs\n+\n+    def get_internal_type(self):\n+        return \"NullBooleanField\"\n+\n+\n+class PositiveIntegerRelDbTypeMixin:\n+\n+    def rel_db_type(self, connection):\n+        \"\"\"\n+        Return the data type that a related field pointing to this field should\n+        use. In most cases, a foreign key pointing to a positive integer\n+        primary key will have an integer column data type but some databases\n+        (e.g. MySQL) have an unsigned integer type. In that case\n+        (related_fields_match_type=True), the primary key should return its\n+        db_type.\n+        \"\"\"\n+        if connection.features.related_fields_match_type:\n+            return self.db_type(connection)\n+        else:\n+            return IntegerField().db_type(connection=connection)\n+\n+\n+class PositiveIntegerField(PositiveIntegerRelDbTypeMixin, IntegerField):\n+    description = _(\"Positive integer\")\n+\n+    def get_internal_type(self):\n+        return \"PositiveIntegerField\"\n+\n+    def formfield(self, **kwargs):\n+        return super().formfield(**{\n+            'min_value': 0,\n+            **kwargs,\n+        })\n+\n+\n+class PositiveSmallIntegerField(PositiveIntegerRelDbTypeMixin, IntegerField):\n+    description = _(\"Positive small integer\")\n+\n+    def get_internal_type(self):\n+        return \"PositiveSmallIntegerField\"\n+\n+    def formfield(self, **kwargs):\n+        return super().formfield(**{\n+            'min_value': 0,\n+            **kwargs,\n+        })\n+\n+\n+class SlugField(CharField):\n+    default_validators = [validators.validate_slug]\n+    description = _(\"Slug (up to %(max_length)s)\")\n+\n+    def __init__(self, *args, max_length=50, db_index=True, allow_unicode=False, **kwargs):\n+        self.allow_unicode = allow_unicode\n+        if self.allow_unicode:\n+            self.default_validators = [validators.validate_unicode_slug]\n+        super().__init__(*args, max_length=max_length, db_index=db_index, **kwargs)\n+\n+    def deconstruct(self):\n+        name, path, args, kwargs = super().deconstruct()\n+        if kwargs.get(\"max_length\") == 50:\n+            del kwargs['max_length']\n+        if self.db_index is False:\n+            kwargs['db_index'] = False\n+        else:\n+            del kwargs['db_index']\n+        if self.allow_unicode is not False:\n+            kwargs['allow_unicode'] = self.allow_unicode\n+        return name, path, args, kwargs\n+\n+    def get_internal_type(self):\n+        return \"SlugField\"\n+\n+    def formfield(self, **kwargs):\n+        return super().formfield(**{\n+            'form_class': forms.SlugField,\n+            'allow_unicode': self.allow_unicode,\n+            **kwargs,\n+        })\n+\n+\n+class SmallIntegerField(IntegerField):\n+    description = _(\"Small integer\")\n+\n+    def get_internal_type(self):\n+        return \"SmallIntegerField\"\n+\n+\n+class TextField(Field):\n+    description = _(\"Text\")\n+\n+    def get_internal_type(self):\n+        return \"TextField\"\n+\n+    def to_python(self, value):\n+        if isinstance(value, str) or value is None:\n+            return value\n+        return str(value)\n+\n+    def get_prep_value(self, value):\n+        value = super().get_prep_value(value)\n+        return self.to_python(value)\n+\n+    def formfield(self, **kwargs):\n+        # Passing max_length to forms.CharField means that the value's length\n+        # will be validated twice. This is considered acceptable since we want\n+        # the value in the form field (to pass into widget for example).\n+        return super().formfield(**{\n+            'max_length': self.max_length,\n+            **({} if self.choices is not None else {'widget': forms.Textarea}),\n+            **kwargs,\n+        })\n+\n+\n+class TimeField(DateTimeCheckMixin, Field):\n+    empty_strings_allowed = False\n+    default_error_messages = {\n+        'invalid': _('\u201c%(value)s\u201d value has an invalid format. It must be in '\n+                     'HH:MM[:ss[.uuuuuu]] format.'),\n+        'invalid_time': _('\u201c%(value)s\u201d value has the correct format '\n+                          '(HH:MM[:ss[.uuuuuu]]) but it is an invalid time.'),\n+    }\n+    description = _(\"Time\")\n+\n+    def __init__(self, verbose_name=None, name=None, auto_now=False,\n+                 auto_now_add=False, **kwargs):\n+        self.auto_now, self.auto_now_add = auto_now, auto_now_add\n+        if auto_now or auto_now_add:\n+            kwargs['editable'] = False\n+            kwargs['blank'] = True\n+        super().__init__(verbose_name, name, **kwargs)\n+\n+    def _check_fix_default_value(self):\n+        \"\"\"\n+        Warn that using an actual date or datetime value is probably wrong;\n+        it's only evaluated on server startup.\n+        \"\"\"\n+        if not self.has_default():\n+            return []\n+\n+        now = timezone.now()\n+        if not timezone.is_naive(now):\n+            now = timezone.make_naive(now, timezone.utc)\n+        value = self.default\n+        if isinstance(value, datetime.datetime):\n+            second_offset = datetime.timedelta(seconds=10)\n+            lower = now - second_offset\n+            upper = now + second_offset\n+            if timezone.is_aware(value):\n+                value = timezone.make_naive(value, timezone.utc)\n+        elif isinstance(value, datetime.time):\n+            second_offset = datetime.timedelta(seconds=10)\n+            lower = now - second_offset\n+            upper = now + second_offset\n+            value = datetime.datetime.combine(now.date(), value)\n+            if timezone.is_aware(value):\n+                value = timezone.make_naive(value, timezone.utc).time()\n+        else:\n+            # No explicit time / datetime value -- no checks necessary\n+            return []\n+        if lower <= value <= upper:\n+            return [\n+                checks.Warning(\n+                    'Fixed default value provided.',\n+                    hint='It seems you set a fixed date / time / datetime '\n+                         'value as default for this field. This may not be '\n+                         'what you want. If you want to have the current date '\n+                         'as default, use `django.utils.timezone.now`',\n+                    obj=self,\n+                    id='fields.W161',\n+                )\n+            ]\n+\n+        return []\n+\n+    def deconstruct(self):\n+        name, path, args, kwargs = super().deconstruct()\n+        if self.auto_now is not False:\n+            kwargs[\"auto_now\"] = self.auto_now\n+        if self.auto_now_add is not False:\n+            kwargs[\"auto_now_add\"] = self.auto_now_add\n+        if self.auto_now or self.auto_now_add:\n+            del kwargs['blank']\n+            del kwargs['editable']\n+        return name, path, args, kwargs\n+\n+    def get_internal_type(self):\n+        return \"TimeField\"\n+\n+    def to_python(self, value):\n+        if value is None:\n+            return None\n+        if isinstance(value, datetime.time):\n+            return value\n+        if isinstance(value, datetime.datetime):\n+            # Not usually a good idea to pass in a datetime here (it loses\n+            # information), but this can be a side-effect of interacting with a\n+            # database backend (e.g. Oracle), so we'll be accommodating.\n+            return value.time()\n+\n+        try:\n+            parsed = parse_time(value)\n+            if parsed is not None:\n+                return parsed\n+        except ValueError:\n+            raise exceptions.ValidationError(\n+                self.error_messages['invalid_time'],\n+                code='invalid_time',\n+                params={'value': value},\n+            )\n+\n+        raise exceptions.ValidationError(\n+            self.error_messages['invalid'],\n+            code='invalid',\n+            params={'value': value},\n+        )\n+\n+    def pre_save(self, model_instance, add):\n+        if self.auto_now or (self.auto_now_add and add):\n+            value = datetime.datetime.now().time()\n+            setattr(model_instance, self.attname, value)\n+            return value\n+        else:\n+            return super().pre_save(model_instance, add)\n+\n+    def get_prep_value(self, value):\n+        value = super().get_prep_value(value)\n+        return self.to_python(value)\n+\n+    def get_db_prep_value(self, value, connection, prepared=False):\n+        # Casts times into the format expected by the backend\n+        if not prepared:\n+            value = self.get_prep_value(value)\n+        return connection.ops.adapt_timefield_value(value)\n+\n+    def value_to_string(self, obj):\n+        val = self.value_from_object(obj)\n+        return '' if val is None else val.isoformat()\n+\n+    def formfield(self, **kwargs):\n+        return super().formfield(**{\n+            'form_class': forms.TimeField,\n+            **kwargs,\n+        })\n+\n+\n+class URLField(CharField):\n+    default_validators = [validators.URLValidator()]\n+    description = _(\"URL\")\n+\n+    def __init__(self, verbose_name=None, name=None, **kwargs):\n+        kwargs.setdefault('max_length', 200)\n+        super().__init__(verbose_name, name, **kwargs)\n+\n+    def deconstruct(self):\n+        name, path, args, kwargs = super().deconstruct()\n+        if kwargs.get(\"max_length\") == 200:\n+            del kwargs['max_length']\n+        return name, path, args, kwargs\n+\n+    def formfield(self, **kwargs):\n+        # As with CharField, this will cause URL validation to be performed\n+        # twice.\n+        return super().formfield(**{\n+            'form_class': forms.URLField,\n+            **kwargs,\n+        })\n+\n+\n+class BinaryField(Field):\n+    description = _(\"Raw binary data\")\n+    empty_values = [None, b'']\n+\n+    def __init__(self, *args, **kwargs):\n+        kwargs.setdefault('editable', False)\n+        super().__init__(*args, **kwargs)\n+        if self.max_length is not None:\n+            self.validators.append(validators.MaxLengthValidator(self.max_length))\n+\n+    def check(self, **kwargs):\n+        return [*super().check(**kwargs), *self._check_str_default_value()]\n+\n+    def _check_str_default_value(self):\n+        if self.has_default() and isinstance(self.default, str):\n+            return [\n+                checks.Error(\n+                    \"BinaryField's default cannot be a string. Use bytes \"\n+                    \"content instead.\",\n+                    obj=self,\n+                    id='fields.E170',\n+                )\n+            ]\n+        return []\n+\n+    def deconstruct(self):\n+        name, path, args, kwargs = super().deconstruct()\n+        if self.editable:\n+            kwargs['editable'] = True\n+        else:\n+            del kwargs['editable']\n+        return name, path, args, kwargs\n+\n+    def get_internal_type(self):\n+        return \"BinaryField\"\n+\n+    def get_placeholder(self, value, compiler, connection):\n+        return connection.ops.binary_placeholder_sql(value)\n+\n+    def get_default(self):\n+        if self.has_default() and not callable(self.default):\n+            return self.default\n+        default = super().get_default()\n+        if default == '':\n+            return b''\n+        return default\n+\n+    def get_db_prep_value(self, value, connection, prepared=False):\n+        value = super().get_db_prep_value(value, connection, prepared)\n+        if value is not None:\n+            return connection.Database.Binary(value)\n+        return value\n+\n+    def value_to_string(self, obj):\n+        \"\"\"Binary data is serialized as base64\"\"\"\n+        return b64encode(self.value_from_object(obj)).decode('ascii')\n+\n+    def to_python(self, value):\n+        # If it's a string, it should be base64-encoded data\n+        if isinstance(value, str):\n+            return memoryview(b64decode(value.encode('ascii')))\n+        return value\n+\n+\n+class UUIDField(Field):\n+    default_error_messages = {\n+        'invalid': _('\u201c%(value)s\u201d is not a valid UUID.'),\n+    }\n+    description = _('Universally unique identifier')\n+    empty_strings_allowed = False\n+\n+    def __init__(self, verbose_name=None, **kwargs):\n+        kwargs['max_length'] = 32\n+        super().__init__(verbose_name, **kwargs)\n+\n+    def deconstruct(self):\n+        name, path, args, kwargs = super().deconstruct()\n+        del kwargs['max_length']\n+        return name, path, args, kwargs\n+\n+    def get_internal_type(self):\n+        return \"UUIDField\"\n+\n+    def get_prep_value(self, value):\n+        value = super().get_prep_value(value)\n+        return self.to_python(value)\n+\n+    def get_db_prep_value(self, value, connection, prepared=False):\n+        if value is None:\n+            return None\n+        if not isinstance(value, uuid.UUID):\n+            value = self.to_python(value)\n+\n+        if connection.features.has_native_uuid_field:\n+            return value\n+        return value.hex\n+\n+    def to_python(self, value):\n+        if value is not None and not isinstance(value, uuid.UUID):\n+            input_form = 'int' if isinstance(value, int) else 'hex'\n+            try:\n+                return uuid.UUID(**{input_form: value})\n+            except (AttributeError, ValueError):\n+                raise exceptions.ValidationError(\n+                    self.error_messages['invalid'],\n+                    code='invalid',\n+                    params={'value': value},\n+                )\n+        return value\n+\n+    def formfield(self, **kwargs):\n+        return super().formfield(**{\n+            'form_class': forms.UUIDField,\n+            **kwargs,\n+        })\n+\n+\n+class AutoFieldMixin:\n+\n+    def __init__(self, *args, **kwargs):\n+        kwargs['blank'] = True\n+        super().__init__(*args, **kwargs)\n+\n+    def check(self, **kwargs):\n+        return [\n+            *super().check(**kwargs),\n+            *self._check_primary_key(),\n+        ]\n+\n+    def _check_primary_key(self):\n+        if not self.primary_key:\n+            return [\n+                checks.Error(\n+                    'AutoFields must set primary_key=True.',\n+                    obj=self,\n+                    id='fields.E100',\n+                ),\n+            ]\n+        else:\n+            return []\n+\n+    def deconstruct(self):\n+        name, path, args, kwargs = super().deconstruct()\n+        del kwargs['blank']\n+        kwargs['primary_key'] = True\n+        return name, path, args, kwargs\n+\n+    def validate(self, value, model_instance):\n+        pass\n+\n+    def get_db_prep_value(self, value, connection, prepared=False):\n+        if not prepared:\n+            value = self.get_prep_value(value)\n+            value = connection.ops.validate_autopk_value(value)\n+        return value\n+\n+    def contribute_to_class(self, cls, name, **kwargs):\n+        assert not cls._meta.auto_field, (\n+            \"Model %s can't have more than one auto-generated field.\"\n+            % cls._meta.label\n+        )\n+        super().contribute_to_class(cls, name, **kwargs)\n+        cls._meta.auto_field = self\n+\n+    def formfield(self, **kwargs):\n+        return None\n+\n+\n+class AutoFieldMeta(type):\n+    \"\"\"\n+    Metaclass to maintain backward inheritance compatibility for AutoField.\n+\n+    It is intended that AutoFieldMixin become public API when it is possible to\n+    create a non-integer automatically-generated field using column defaults\n+    stored in the database.\n+\n+    In many areas Django also relies on using isinstance() to check for an\n+    automatically-generated field as a subclass of AutoField. A new flag needs\n+    to be implemented on Field to be used instead.\n+\n+    When these issues have been addressed, this metaclass could be used to\n+    deprecate inheritance from AutoField and use of isinstance() with AutoField\n+    for detecting automatically-generated fields.\n+    \"\"\"\n+\n+    @property\n+    def _subclasses(self):\n+        return (BigAutoField, SmallAutoField)\n+\n+    def __instancecheck__(self, instance):\n+        return isinstance(instance, self._subclasses) or super().__instancecheck__(instance)\n+\n+    def __subclasscheck__(self, subclass):\n+        return subclass in self._subclasses or super().__subclasscheck__(subclass)\n+\n+\n+class AutoField(AutoFieldMixin, IntegerField, metaclass=AutoFieldMeta):\n+\n+    def get_internal_type(self):\n+        return 'AutoField'\n+\n+    def rel_db_type(self, connection):\n+        return IntegerField().db_type(connection=connection)\n+\n+\n+class BigAutoField(AutoFieldMixin, BigIntegerField):\n+\n+    def get_internal_type(self):\n+        return 'BigAutoField'\n+\n+    def rel_db_type(self, connection):\n+        return BigIntegerField().db_type(connection=connection)\n+\n+\n+class SmallAutoField(AutoFieldMixin, SmallIntegerField):\n+\n+    def get_internal_type(self):\n+        return 'SmallAutoField'\n+\n+    def rel_db_type(self, connection):\n+        return SmallIntegerField().db_type(connection=connection)\ndiff --git a/.harness/checkpoints/2bdd67450864/--Users--huetuanthi--dev--agents--harness--tests--invalid_models_tests--test_ordinary_fields.py b/.harness/checkpoints/2bdd67450864/--Users--huetuanthi--dev--agents--harness--tests--invalid_models_tests--test_ordinary_fields.py\nnew file mode 100644\nindex 0000000000..5bb1847a70\n--- /dev/null\n+++ b/.harness/checkpoints/2bdd67450864/--Users--huetuanthi--dev--agents--harness--tests--invalid_models_tests--test_ordinary_fields.py\n@@ -0,0 +1,744 @@\n+import unittest\n+\n+from django.core.checks import Error, Warning as DjangoWarning\n+from django.db import connection, models\n+from django.test import SimpleTestCase, TestCase, skipIfDBFeature\n+from django.test.utils import isolate_apps, override_settings\n+from django.utils.functional import lazy\n+from django.utils.timezone import now\n+from django.utils.translation import gettext_lazy as _\n+\n+\n+@isolate_apps('invalid_models_tests')\n+class AutoFieldTests(SimpleTestCase):\n+\n+    def test_valid_case(self):\n+        class Model(models.Model):\n+            id = models.AutoField(primary_key=True)\n+\n+        field = Model._meta.get_field('id')\n+        self.assertEqual(field.check(), [])\n+\n+    def test_primary_key(self):\n+        # primary_key must be True. Refs #12467.\n+        class Model(models.Model):\n+            field = models.AutoField(primary_key=False)\n+\n+            # Prevent Django from autocreating `id` AutoField, which would\n+            # result in an error, because a model must have exactly one\n+            # AutoField.\n+            another = models.IntegerField(primary_key=True)\n+\n+        field = Model._meta.get_field('field')\n+        self.assertEqual(field.check(), [\n+            Error(\n+                'AutoFields must set primary_key=True.',\n+                obj=field,\n+                id='fields.E100',\n+            ),\n+        ])\n+\n+    def test_max_length_warning(self):\n+        class Model(models.Model):\n+            auto = models.AutoField(primary_key=True, max_length=2)\n+\n+        field = Model._meta.get_field('auto')\n+        self.assertEqual(field.check(), [\n+            DjangoWarning(\n+                \"'max_length' is ignored when used with %s.\"\n+                % field.__class__.__name__,\n+                hint=\"Remove 'max_length' from field\",\n+                obj=field,\n+                id='fields.W122',\n+            ),\n+        ])\n+\n+\n+@isolate_apps('invalid_models_tests')\n+class BinaryFieldTests(SimpleTestCase):\n+\n+    def test_valid_default_value(self):\n+        class Model(models.Model):\n+            field1 = models.BinaryField(default=b'test')\n+            field2 = models.BinaryField(default=None)\n+\n+        for field_name in ('field1', 'field2'):\n+            field = Model._meta.get_field(field_name)\n+            self.assertEqual(field.check(), [])\n+\n+    def test_str_default_value(self):\n+        class Model(models.Model):\n+            field = models.BinaryField(default='test')\n+\n+        field = Model._meta.get_field('field')\n+        self.assertEqual(field.check(), [\n+            Error(\n+                \"BinaryField's default cannot be a string. Use bytes content \"\n+                \"instead.\",\n+                obj=field,\n+                id='fields.E170',\n+            ),\n+        ])\n+\n+\n+@isolate_apps('invalid_models_tests')\n+class CharFieldTests(SimpleTestCase):\n+\n+    def test_valid_field(self):\n+        class Model(models.Model):\n+            field = models.CharField(\n+                max_length=255,\n+                choices=[\n+                    ('1', 'item1'),\n+                    ('2', 'item2'),\n+                ],\n+                db_index=True,\n+            )\n+\n+        field = Model._meta.get_field('field')\n+        self.assertEqual(field.check(), [])\n+\n+    def test_missing_max_length(self):\n+        class Model(models.Model):\n+            field = models.CharField()\n+\n+        field = Model._meta.get_field('field')\n+        self.assertEqual(field.check(), [\n+            Error(\n+                \"CharFields must define a 'max_length' attribute.\",\n+                obj=field,\n+                id='fields.E120',\n+            ),\n+        ])\n+\n+    def test_negative_max_length(self):\n+        class Model(models.Model):\n+            field = models.CharField(max_length=-1)\n+\n+        field = Model._meta.get_field('field')\n+        self.assertEqual(field.check(), [\n+            Error(\n+                \"'max_length' must be a positive integer.\",\n+                obj=field,\n+                id='fields.E121',\n+            ),\n+        ])\n+\n+    def test_bad_max_length_value(self):\n+        class Model(models.Model):\n+            field = models.CharField(max_length=\"bad\")\n+\n+        field = Model._meta.get_field('field')\n+        self.assertEqual(field.check(), [\n+            Error(\n+                \"'max_length' must be a positive integer.\",\n+                obj=field,\n+                id='fields.E121',\n+            ),\n+        ])\n+\n+    def test_str_max_length_value(self):\n+        class Model(models.Model):\n+            field = models.CharField(max_length='20')\n+\n+        field = Model._meta.get_field('field')\n+        self.assertEqual(field.check(), [\n+            Error(\n+                \"'max_length' must be a positive integer.\",\n+                obj=field,\n+                id='fields.E121',\n+            ),\n+        ])\n+\n+    def test_str_max_length_type(self):\n+        class Model(models.Model):\n+            field = models.CharField(max_length=True)\n+\n+        field = Model._meta.get_field('field')\n+        self.assertEqual(field.check(), [\n+            Error(\n+                \"'max_length' must be a positive integer.\",\n+                obj=field,\n+                id='fields.E121'\n+            ),\n+        ])\n+\n+    def test_non_iterable_choices(self):\n+        class Model(models.Model):\n+            field = models.CharField(max_length=10, choices='bad')\n+\n+        field = Model._meta.get_field('field')\n+        self.assertEqual(field.check(), [\n+            Error(\n+                \"'choices' must be an iterable (e.g., a list or tuple).\",\n+                obj=field,\n+                id='fields.E004',\n+            ),\n+        ])\n+\n+    def test_non_iterable_choices_two_letters(self):\n+        \"\"\"Two letters isn't a valid choice pair.\"\"\"\n+        class Model(models.Model):\n+            field = models.CharField(max_length=10, choices=['ab'])\n+\n+        field = Model._meta.get_field('field')\n+        self.assertEqual(field.check(), [\n+            Error(\n+                \"'choices' must be an iterable containing (actual value, \"\n+                \"human readable name) tuples.\",\n+                obj=field,\n+                id='fields.E005',\n+            ),\n+        ])\n+\n+    def test_iterable_of_iterable_choices(self):\n+        class ThingItem:\n+            def __init__(self, value, display):\n+                self.value = value\n+                self.display = display\n+\n+            def __iter__(self):\n+                return iter((self.value, self.display))\n+\n+            def __len__(self):\n+                return 2\n+\n+        class Things:\n+            def __iter__(self):\n+                return iter((ThingItem(1, 2), ThingItem(3, 4)))\n+\n+        class ThingWithIterableChoices(models.Model):\n+            thing = models.CharField(max_length=100, blank=True, choices=Things())\n+\n+        self.assertEqual(ThingWithIterableChoices._meta.get_field('thing').check(), [])\n+\n+    def test_choices_containing_non_pairs(self):\n+        class Model(models.Model):\n+            field = models.CharField(max_length=10, choices=[(1, 2, 3), (1, 2, 3)])\n+\n+        class Model2(models.Model):\n+            field = models.IntegerField(choices=[0])\n+\n+        for model in (Model, Model2):\n+            with self.subTest(model.__name__):\n+                field = model._meta.get_field('field')\n+                self.assertEqual(field.check(), [\n+                    Error(\n+                        \"'choices' must be an iterable containing (actual \"\n+                        \"value, human readable name) tuples.\",\n+                        obj=field,\n+                        id='fields.E005',\n+                    ),\n+                ])\n+\n+    def test_choices_containing_lazy(self):\n+        class Model(models.Model):\n+            field = models.CharField(max_length=10, choices=[['1', _('1')], ['2', _('2')]])\n+\n+        self.assertEqual(Model._meta.get_field('field').check(), [])\n+\n+    def test_lazy_choices(self):\n+        class Model(models.Model):\n+            field = models.CharField(max_length=10, choices=lazy(lambda: [[1, '1'], [2, '2']], tuple)())\n+\n+        self.assertEqual(Model._meta.get_field('field').check(), [])\n+\n+    def test_choices_named_group(self):\n+        class Model(models.Model):\n+            field = models.CharField(\n+                max_length=10, choices=[\n+                    ['knights', [['L', 'Lancelot'], ['G', 'Galahad']]],\n+                    ['wizards', [['T', 'Tim the Enchanter']]],\n+                    ['R', 'Random character'],\n+                ],\n+            )\n+\n+        self.assertEqual(Model._meta.get_field('field').check(), [])\n+\n+    def test_choices_named_group_non_pairs(self):\n+        class Model(models.Model):\n+            field = models.CharField(\n+                max_length=10,\n+                choices=[['knights', [['L', 'Lancelot', 'Du Lac']]]],\n+            )\n+\n+        field = Model._meta.get_field('field')\n+        self.assertEqual(field.check(), [\n+            Error(\n+                \"'choices' must be an iterable containing (actual value, \"\n+                \"human readable name) tuples.\",\n+                obj=field,\n+                id='fields.E005',\n+            ),\n+        ])\n+\n+    def test_choices_named_group_bad_structure(self):\n+        class Model(models.Model):\n+            field = models.CharField(\n+                max_length=10, choices=[\n+                    ['knights', [\n+                        ['Noble', [['G', 'Galahad']]],\n+                        ['Combative', [['L', 'Lancelot']]],\n+                    ]],\n+                ],\n+            )\n+\n+        field = Model._meta.get_field('field')\n+        self.assertEqual(field.check(), [\n+            Error(\n+                \"'choices' must be an iterable containing (actual value, \"\n+                \"human readable name) tuples.\",\n+                obj=field,\n+                id='fields.E005',\n+            ),\n+        ])\n+\n+    def test_choices_named_group_lazy(self):\n+        class Model(models.Model):\n+            field = models.CharField(\n+                max_length=10, choices=[\n+                    [_('knights'), [['L', _('Lancelot')], ['G', _('Galahad')]]],\n+                    ['R', _('Random character')],\n+                ],\n+            )\n+\n+        self.assertEqual(Model._meta.get_field('field').check(), [])\n+\n+    def test_bad_db_index_value(self):\n+        class Model(models.Model):\n+            field = models.CharField(max_length=10, db_index='bad')\n+\n+        field = Model._meta.get_field('field')\n+        self.assertEqual(field.check(), [\n+            Error(\n+                \"'db_index' must be None, True or False.\",\n+                obj=field,\n+                id='fields.E006',\n+            ),\n+        ])\n+\n+    def test_bad_validators(self):\n+        class Model(models.Model):\n+            field = models.CharField(max_length=10, validators=[True])\n+\n+        field = Model._meta.get_field('field')\n+        self.assertEqual(field.check(), [\n+            Error(\n+                \"All 'validators' must be callable.\",\n+                hint=(\n+                    \"validators[0] (True) isn't a function or instance of a \"\n+                    \"validator class.\"\n+                ),\n+                obj=field,\n+                id='fields.E008',\n+            ),\n+        ])\n+\n+    @unittest.skipUnless(connection.vendor == 'mysql',\n+                         \"Test valid only for MySQL\")\n+    def test_too_long_char_field_under_mysql(self):\n+        from django.db.backends.mysql.validation import DatabaseValidation\n+\n+        class Model(models.Model):\n+            field = models.CharField(unique=True, max_length=256)\n+\n+        field = Model._meta.get_field('field')\n+        validator = DatabaseValidation(connection=connection)\n+        self.assertEqual(validator.check_field(field), [\n+            Error(\n+                'MySQL does not allow unique CharFields to have a max_length > 255.',\n+                obj=field,\n+                id='mysql.E001',\n+            )\n+        ])\n+\n+\n+@isolate_apps('invalid_models_tests')\n+class DateFieldTests(SimpleTestCase):\n+    maxDiff = None\n+\n+    def test_auto_now_and_auto_now_add_raise_error(self):\n+        class Model(models.Model):\n+            field0 = models.DateTimeField(auto_now=True, auto_now_add=True, default=now)\n+            field1 = models.DateTimeField(auto_now=True, auto_now_add=False, default=now)\n+            field2 = models.DateTimeField(auto_now=False, auto_now_add=True, default=now)\n+            field3 = models.DateTimeField(auto_now=True, auto_now_add=True, default=None)\n+\n+        expected = []\n+        checks = []\n+        for i in range(4):\n+            field = Model._meta.get_field('field%d' % i)\n+            expected.append(Error(\n+                \"The options auto_now, auto_now_add, and default \"\n+                \"are mutually exclusive. Only one of these options \"\n+                \"may be present.\",\n+                obj=field,\n+                id='fields.E160',\n+            ))\n+            checks.extend(field.check())\n+            self.assertEqual(checks, expected)\n+\n+    def test_fix_default_value(self):\n+        class Model(models.Model):\n+            field_dt = models.DateField(default=now())\n+            field_d = models.DateField(default=now().date())\n+            field_now = models.DateField(default=now)\n+\n+        field_dt = Model._meta.get_field('field_dt')\n+        field_d = Model._meta.get_field('field_d')\n+        field_now = Model._meta.get_field('field_now')\n+        errors = field_dt.check()\n+        errors.extend(field_d.check())\n+        errors.extend(field_now.check())  # doesn't raise a warning\n+        self.assertEqual(errors, [\n+            DjangoWarning(\n+                'Fixed default value provided.',\n+                hint='It seems you set a fixed date / time / datetime '\n+                     'value as default for this field. This may not be '\n+                     'what you want. If you want to have the current date '\n+                     'as default, use `django.utils.timezone.now`',\n+                obj=field_dt,\n+                id='fields.W161',\n+            ),\n+            DjangoWarning(\n+                'Fixed default value provided.',\n+                hint='It seems you set a fixed date / time / datetime '\n+                     'value as default for this field. This may not be '\n+                     'what you want. If you want to have the current date '\n+                     'as default, use `django.utils.timezone.now`',\n+                obj=field_d,\n+                id='fields.W161',\n+            )\n+        ])\n+\n+    @override_settings(USE_TZ=True)\n+    def test_fix_default_value_tz(self):\n+        self.test_fix_default_value()\n+\n+\n+@isolate_apps('invalid_models_tests')\n+class DateTimeFieldTests(SimpleTestCase):\n+    maxDiff = None\n+\n+    def test_fix_default_value(self):\n+        class Model(models.Model):\n+            field_dt = models.DateTimeField(default=now())\n+            field_d = models.DateTimeField(default=now().date())\n+            field_now = models.DateTimeField(default=now)\n+\n+        field_dt = Model._meta.get_field('field_dt')\n+        field_d = Model._meta.get_field('field_d')\n+        field_now = Model._meta.get_field('field_now')\n+        errors = field_dt.check()\n+        errors.extend(field_d.check())\n+        errors.extend(field_now.check())  # doesn't raise a warning\n+        self.assertEqual(errors, [\n+            DjangoWarning(\n+                'Fixed default value provided.',\n+                hint='It seems you set a fixed date / time / datetime '\n+                     'value as default for this field. This may not be '\n+                     'what you want. If you want to have the current date '\n+                     'as default, use `django.utils.timezone.now`',\n+                obj=field_dt,\n+                id='fields.W161',\n+            ),\n+            DjangoWarning(\n+                'Fixed default value provided.',\n+                hint='It seems you set a fixed date / time / datetime '\n+                     'value as default for this field. This may not be '\n+                     'what you want. If you want to have the current date '\n+                     'as default, use `django.utils.timezone.now`',\n+                obj=field_d,\n+                id='fields.W161',\n+            )\n+        ])\n+\n+    @override_settings(USE_TZ=True)\n+    def test_fix_default_value_tz(self):\n+        self.test_fix_default_value()\n+\n+\n+@isolate_apps('invalid_models_tests')\n+class DecimalFieldTests(SimpleTestCase):\n+\n+    def test_required_attributes(self):\n+        class Model(models.Model):\n+            field = models.DecimalField()\n+\n+        field = Model._meta.get_field('field')\n+        self.assertEqual(field.check(), [\n+            Error(\n+                \"DecimalFields must define a 'decimal_places' attribute.\",\n+                obj=field,\n+                id='fields.E130',\n+            ),\n+            Error(\n+                \"DecimalFields must define a 'max_digits' attribute.\",\n+                obj=field,\n+                id='fields.E132',\n+            ),\n+        ])\n+\n+    def test_negative_max_digits_and_decimal_places(self):\n+        class Model(models.Model):\n+            field = models.DecimalField(max_digits=-1, decimal_places=-1)\n+\n+        field = Model._meta.get_field('field')\n+        self.assertEqual(field.check(), [\n+            Error(\n+                \"'decimal_places' must be a non-negative integer.\",\n+                obj=field,\n+                id='fields.E131',\n+            ),\n+            Error(\n+                \"'max_digits' must be a positive integer.\",\n+                obj=field,\n+                id='fields.E133',\n+            ),\n+        ])\n+\n+    def test_bad_values_of_max_digits_and_decimal_places(self):\n+        class Model(models.Model):\n+            field = models.DecimalField(max_digits=\"bad\", decimal_places=\"bad\")\n+\n+        field = Model._meta.get_field('field')\n+        self.assertEqual(field.check(), [\n+            Error(\n+                \"'decimal_places' must be a non-negative integer.\",\n+                obj=field,\n+                id='fields.E131',\n+            ),\n+            Error(\n+                \"'max_digits' must be a positive integer.\",\n+                obj=field,\n+                id='fields.E133',\n+            ),\n+        ])\n+\n+    def test_decimal_places_greater_than_max_digits(self):\n+        class Model(models.Model):\n+            field = models.DecimalField(max_digits=9, decimal_places=10)\n+\n+        field = Model._meta.get_field('field')\n+        self.assertEqual(field.check(), [\n+            Error(\n+                \"'max_digits' must be greater or equal to 'decimal_places'.\",\n+                obj=field,\n+                id='fields.E134',\n+            ),\n+        ])\n+\n+    def test_valid_field(self):\n+        class Model(models.Model):\n+            field = models.DecimalField(max_digits=10, decimal_places=10)\n+\n+        field = Model._meta.get_field('field')\n+        self.assertEqual(field.check(), [])\n+\n+\n+@isolate_apps('invalid_models_tests')\n+class FileFieldTests(SimpleTestCase):\n+\n+    def test_valid_default_case(self):\n+        class Model(models.Model):\n+            field = models.FileField()\n+\n+        self.assertEqual(Model._meta.get_field('field').check(), [])\n+\n+    def test_valid_case(self):\n+        class Model(models.Model):\n+            field = models.FileField(upload_to='somewhere')\n+\n+        field = Model._meta.get_field('field')\n+        self.assertEqual(field.check(), [])\n+\n+    def test_primary_key(self):\n+        class Model(models.Model):\n+            field = models.FileField(primary_key=False, upload_to='somewhere')\n+\n+        field = Model._meta.get_field('field')\n+        self.assertEqual(field.check(), [\n+            Error(\n+                \"'primary_key' is not a valid argument for a FileField.\",\n+                obj=field,\n+                id='fields.E201',\n+            )\n+        ])\n+\n+    def test_upload_to_starts_with_slash(self):\n+        class Model(models.Model):\n+            field = models.FileField(upload_to='/somewhere')\n+\n+        field = Model._meta.get_field('field')\n+        self.assertEqual(field.check(), [\n+            Error(\n+                \"FileField's 'upload_to' argument must be a relative path, not \"\n+                \"an absolute path.\",\n+                obj=field,\n+                id='fields.E202',\n+                hint='Remove the leading slash.',\n+            )\n+        ])\n+\n+    def test_upload_to_callable_not_checked(self):\n+        def callable(instance, filename):\n+            return '/' + filename\n+\n+        class Model(models.Model):\n+            field = models.FileField(upload_to=callable)\n+\n+        field = Model._meta.get_field('field')\n+        self.assertEqual(field.check(), [])\n+\n+\n+@isolate_apps('invalid_models_tests')\n+class FilePathFieldTests(SimpleTestCase):\n+\n+    def test_forbidden_files_and_folders(self):\n+        class Model(models.Model):\n+            field = models.FilePathField(allow_files=False, allow_folders=False)\n+\n+        field = Model._meta.get_field('field')\n+        self.assertEqual(field.check(), [\n+            Error(\n+                \"FilePathFields must have either 'allow_files' or 'allow_folders' set to True.\",\n+                obj=field,\n+                id='fields.E140',\n+            ),\n+        ])\n+\n+\n+@isolate_apps('invalid_models_tests')\n+class GenericIPAddressFieldTests(SimpleTestCase):\n+\n+    def test_non_nullable_blank(self):\n+        class Model(models.Model):\n+            field = models.GenericIPAddressField(null=False, blank=True)\n+\n+        field = Model._meta.get_field('field')\n+        self.assertEqual(field.check(), [\n+            Error(\n+                ('GenericIPAddressFields cannot have blank=True if null=False, '\n+                 'as blank values are stored as nulls.'),\n+                obj=field,\n+                id='fields.E150',\n+            ),\n+        ])\n+\n+\n+@isolate_apps('invalid_models_tests')\n+class ImageFieldTests(SimpleTestCase):\n+\n+    def test_pillow_installed(self):\n+        try:\n+            from PIL import Image  # NOQA\n+        except ImportError:\n+            pillow_installed = False\n+        else:\n+            pillow_installed = True\n+\n+        class Model(models.Model):\n+            field = models.ImageField(upload_to='somewhere')\n+\n+        field = Model._meta.get_field('field')\n+        errors = field.check()\n+        expected = [] if pillow_installed else [\n+            Error(\n+                'Cannot use ImageField because Pillow is not installed.',\n+                hint=('Get Pillow at https://pypi.org/project/Pillow/ '\n+                      'or run command \"python -m pip install Pillow\".'),\n+                obj=field,\n+                id='fields.E210',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+\n+@isolate_apps('invalid_models_tests')\n+class IntegerFieldTests(SimpleTestCase):\n+\n+    def test_max_length_warning(self):\n+        class Model(models.Model):\n+            integer = models.IntegerField(max_length=2)\n+            biginteger = models.BigIntegerField(max_length=2)\n+            smallinteger = models.SmallIntegerField(max_length=2)\n+            positiveinteger = models.PositiveIntegerField(max_length=2)\n+            positivesmallinteger = models.PositiveSmallIntegerField(max_length=2)\n+\n+        for field in Model._meta.get_fields():\n+            if field.auto_created:\n+                continue\n+            with self.subTest(name=field.name):\n+                self.assertEqual(field.check(), [\n+                    DjangoWarning(\n+                        \"'max_length' is ignored when used with %s.\" % field.__class__.__name__,\n+                        hint=\"Remove 'max_length' from field\",\n+                        obj=field,\n+                        id='fields.W122',\n+                    )\n+                ])\n+\n+\n+@isolate_apps('invalid_models_tests')\n+class TimeFieldTests(SimpleTestCase):\n+    maxDiff = None\n+\n+    def test_fix_default_value(self):\n+        class Model(models.Model):\n+            field_dt = models.TimeField(default=now())\n+            field_t = models.TimeField(default=now().time())\n+            field_now = models.DateField(default=now)\n+\n+        field_dt = Model._meta.get_field('field_dt')\n+        field_t = Model._meta.get_field('field_t')\n+        field_now = Model._meta.get_field('field_now')\n+        errors = field_dt.check()\n+        errors.extend(field_t.check())\n+        errors.extend(field_now.check())  # doesn't raise a warning\n+        self.assertEqual(errors, [\n+            DjangoWarning(\n+                'Fixed default value provided.',\n+                hint='It seems you set a fixed date / time / datetime '\n+                     'value as default for this field. This may not be '\n+                     'what you want. If you want to have the current date '\n+                     'as default, use `django.utils.timezone.now`',\n+                obj=field_dt,\n+                id='fields.W161',\n+            ),\n+            DjangoWarning(\n+                'Fixed default value provided.',\n+                hint='It seems you set a fixed date / time / datetime '\n+                     'value as default for this field. This may not be '\n+                     'what you want. If you want to have the current date '\n+                     'as default, use `django.utils.timezone.now`',\n+                obj=field_t,\n+                id='fields.W161',\n+            )\n+        ])\n+\n+    @override_settings(USE_TZ=True)\n+    def test_fix_default_value_tz(self):\n+        self.test_fix_default_value()\n+\n+\n+@isolate_apps('invalid_models_tests')\n+class TextFieldTests(TestCase):\n+\n+    @skipIfDBFeature('supports_index_on_text_field')\n+    def test_max_length_warning(self):\n+        class Model(models.Model):\n+            value = models.TextField(db_index=True)\n+        field = Model._meta.get_field('value')\n+        field_type = field.db_type(connection)\n+        self.assertEqual(field.check(), [\n+            DjangoWarning(\n+                '%s does not support a database index on %s columns.'\n+                % (connection.display_name, field_type),\n+                hint=(\n+                    \"An index won't be created. Silence this warning if you \"\n+                    \"don't care about it.\"\n+                ),\n+                obj=field,\n+                id='fields.W162',\n+            )\n+        ])\ndiff --git a/django/db/models/fields/__init__.py b/django/db/models/fields/__init__.py\nindex 9ce46e3f73..f2ee177ad5 100644\n--- a/django/db/models/fields/__init__.py\n+++ b/django/db/models/fields/__init__.py\n@@ -207,6 +207,7 @@ class Field(RegisterLookupMixin):\n             *self._check_backend_specific_checks(**kwargs),\n             *self._check_validators(),\n             *self._check_deprecation_details(),\n+            *self._check_max_length_fits_choices(),\n         ]\n \n     def _check_field_name(self):\n@@ -291,6 +292,42 @@ class Field(RegisterLookupMixin):\n             )\n         ]\n \n+    def _check_max_length_fits_choices(self):\n+        if not self.choices or not isinstance(self.max_length, int) or isinstance(self.max_length, bool):\n+            return []\n+\n+        def get_choice_values(choices):\n+            \"\"\"Recursively yield actual choice values from flat or grouped choices.\"\"\"\n+            for choice in choices:\n+                try:\n+                    group_name, group_choices = choice\n+                except (TypeError, ValueError):\n+                    # Not a pair \u2014 skip; _check_choices() will catch this.\n+                    continue\n+                if is_iterable(group_choices) and not isinstance(group_choices, str):\n+                    # Grouped choices: (group_name, [(value, display), ...])\n+                    for value, _ in group_choices:\n+                        yield value\n+                else:\n+                    # Flat choice: (value, display)\n+                    yield group_name\n+\n+        invalid_choices = [\n+            value for value in get_choice_values(self.choices)\n+            if isinstance(value, str) and len(value) > self.max_length\n+        ]\n+        if invalid_choices:\n+            return [\n+                checks.Error(\n+                    \"'max_length' is too small to fit the longest value \"\n+                    \"in 'choices' (%d < %d).\"\n+                    % (self.max_length, max(len(v) for v in invalid_choices)),\n+                    obj=self,\n+                    id='fields.E009',\n+                )\n+            ]\n+        return []\n+\n     def _check_db_index(self):\n         if self.db_index not in (None, True, False):\n             return [\ndiff --git a/docs/ref/checks.txt b/docs/ref/checks.txt\nindex 4da932720e..6c5f5ec809 100644\n--- a/docs/ref/checks.txt\n+++ b/docs/ref/checks.txt\n@@ -138,6 +138,8 @@ Model fields\n * **fields.E006**: ``db_index`` must be ``None``, ``True`` or ``False``.\n * **fields.E007**: Primary keys must not have ``null=True``.\n * **fields.E008**: All ``validators`` must be callable.\n+* **fields.E009**: ``max_length`` is too small to fit the longest value in\n+  ``choices``.\n * **fields.E100**: ``AutoField``\\s must set primary_key=True.\n * **fields.E110**: ``BooleanField``\\s do not accept null values. *This check\n   appeared before support for null values was added in Django 2.1.*\ndiff --git a/tests/invalid_models_tests/test_ordinary_fields.py b/tests/invalid_models_tests/test_ordinary_fields.py\nindex 5bb1847a70..fcd74352ff 100644\n--- a/tests/invalid_models_tests/test_ordinary_fields.py\n+++ b/tests/invalid_models_tests/test_ordinary_fields.py\n@@ -304,6 +304,73 @@ class CharFieldTests(SimpleTestCase):\n \n         self.assertEqual(Model._meta.get_field('field').check(), [])\n \n+    def test_max_length_is_too_small_for_choices(self):\n+        class Model(models.Model):\n+            field = models.CharField(\n+                max_length=2,\n+                choices=[('ABC', 'Value ABC'), ('XY', 'Value XY')],\n+            )\n+\n+        field = Model._meta.get_field('field')\n+        self.assertEqual(field.check(), [\n+            Error(\n+                \"'max_length' is too small to fit the longest value \"\n+                \"in 'choices' (2 < 3).\",\n+                obj=field,\n+                id='fields.E009',\n+            ),\n+        ])\n+\n+    def test_max_length_is_too_small_for_grouped_choices(self):\n+        class Model(models.Model):\n+            field = models.CharField(\n+                max_length=2,\n+                choices=[\n+                    ['Group', [('ABC', 'Value ABC'), ('XY', 'Value XY')]],\n+                ],\n+            )\n+\n+        field = Model._meta.get_field('field')\n+        self.assertEqual(field.check(), [\n+            Error(\n+                \"'max_length' is too small to fit the longest value \"\n+                \"in 'choices' (2 < 3).\",\n+                obj=field,\n+                id='fields.E009',\n+            ),\n+        ])\n+\n+    def test_max_length_fits_longest_choice(self):\n+        class Model(models.Model):\n+            field = models.CharField(\n+                max_length=3,\n+                choices=[('ABC', 'Value ABC'), ('XY', 'Value XY')],\n+            )\n+\n+        self.assertEqual(Model._meta.get_field('field').check(), [])\n+\n+    def test_max_length_fits_longest_choice_in_named_group(self):\n+        class Model(models.Model):\n+            field = models.CharField(\n+                max_length=3,\n+                choices=[\n+                    ['Group', [('ABC', 'Value ABC'), ('XY', 'Value XY')]],\n+                    ['R', 'Random character'],\n+                ],\n+            )\n+\n+        self.assertEqual(Model._meta.get_field('field').check(), [])\n+\n+    def test_non_string_choices_not_checked_for_max_length(self):\n+        \"\"\"Non-string choice values are not validated against max_length.\"\"\"\n+        class Model(models.Model):\n+            field = models.CharField(\n+                max_length=2,\n+                choices=[(1234, 'Long integer value')],\n+            )\n+\n+        self.assertEqual(Model._meta.get_field('field').check(), [])\n+\n     def test_bad_db_index_value(self):\n         class Model(models.Model):\n             field = models.CharField(max_length=10, db_index='bad')\n"}
{"instance_id": "django__django-11797", "model_name_or_path": "harness/claude-sonnet-4-6", "model_patch": ""}
{"instance_id": "django__django-11815", "model_name_or_path": "harness/claude-sonnet-4-6", "model_patch": "diff --git a/.harness/checkpoints/3b82b70aad53/--Users--huetuanthi--dev--agents--harness--django--db--migrations--serializer.py b/.harness/checkpoints/3b82b70aad53/--Users--huetuanthi--dev--agents--harness--django--db--migrations--serializer.py\nnew file mode 100644\nindex 0000000000..27b5cbd379\n--- /dev/null\n+++ b/.harness/checkpoints/3b82b70aad53/--Users--huetuanthi--dev--agents--harness--django--db--migrations--serializer.py\n@@ -0,0 +1,339 @@\n+import builtins\n+import collections.abc\n+import datetime\n+import decimal\n+import enum\n+import functools\n+import math\n+import re\n+import types\n+import uuid\n+\n+from django.conf import SettingsReference\n+from django.db import models\n+from django.db.migrations.operations.base import Operation\n+from django.db.migrations.utils import COMPILED_REGEX_TYPE, RegexObject\n+from django.utils.functional import LazyObject, Promise\n+from django.utils.timezone import utc\n+from django.utils.version import get_docs_version\n+\n+\n+class BaseSerializer:\n+    def __init__(self, value):\n+        self.value = value\n+\n+    def serialize(self):\n+        raise NotImplementedError('Subclasses of BaseSerializer must implement the serialize() method.')\n+\n+\n+class BaseSequenceSerializer(BaseSerializer):\n+    def _format(self):\n+        raise NotImplementedError('Subclasses of BaseSequenceSerializer must implement the _format() method.')\n+\n+    def serialize(self):\n+        imports = set()\n+        strings = []\n+        for item in self.value:\n+            item_string, item_imports = serializer_factory(item).serialize()\n+            imports.update(item_imports)\n+            strings.append(item_string)\n+        value = self._format()\n+        return value % (\", \".join(strings)), imports\n+\n+\n+class BaseSimpleSerializer(BaseSerializer):\n+    def serialize(self):\n+        return repr(self.value), set()\n+\n+\n+class ChoicesSerializer(BaseSerializer):\n+    def serialize(self):\n+        return serializer_factory(self.value.value).serialize()\n+\n+\n+class DateTimeSerializer(BaseSerializer):\n+    \"\"\"For datetime.*, except datetime.datetime.\"\"\"\n+    def serialize(self):\n+        return repr(self.value), {'import datetime'}\n+\n+\n+class DatetimeDatetimeSerializer(BaseSerializer):\n+    \"\"\"For datetime.datetime.\"\"\"\n+    def serialize(self):\n+        if self.value.tzinfo is not None and self.value.tzinfo != utc:\n+            self.value = self.value.astimezone(utc)\n+        imports = [\"import datetime\"]\n+        if self.value.tzinfo is not None:\n+            imports.append(\"from django.utils.timezone import utc\")\n+        return repr(self.value).replace('<UTC>', 'utc'), set(imports)\n+\n+\n+class DecimalSerializer(BaseSerializer):\n+    def serialize(self):\n+        return repr(self.value), {\"from decimal import Decimal\"}\n+\n+\n+class DeconstructableSerializer(BaseSerializer):\n+    @staticmethod\n+    def serialize_deconstructed(path, args, kwargs):\n+        name, imports = DeconstructableSerializer._serialize_path(path)\n+        strings = []\n+        for arg in args:\n+            arg_string, arg_imports = serializer_factory(arg).serialize()\n+            strings.append(arg_string)\n+            imports.update(arg_imports)\n+        for kw, arg in sorted(kwargs.items()):\n+            arg_string, arg_imports = serializer_factory(arg).serialize()\n+            imports.update(arg_imports)\n+            strings.append(\"%s=%s\" % (kw, arg_string))\n+        return \"%s(%s)\" % (name, \", \".join(strings)), imports\n+\n+    @staticmethod\n+    def _serialize_path(path):\n+        module, name = path.rsplit(\".\", 1)\n+        if module == \"django.db.models\":\n+            imports = {\"from django.db import models\"}\n+            name = \"models.%s\" % name\n+        else:\n+            imports = {\"import %s\" % module}\n+            name = path\n+        return name, imports\n+\n+    def serialize(self):\n+        return self.serialize_deconstructed(*self.value.deconstruct())\n+\n+\n+class DictionarySerializer(BaseSerializer):\n+    def serialize(self):\n+        imports = set()\n+        strings = []\n+        for k, v in sorted(self.value.items()):\n+            k_string, k_imports = serializer_factory(k).serialize()\n+            v_string, v_imports = serializer_factory(v).serialize()\n+            imports.update(k_imports)\n+            imports.update(v_imports)\n+            strings.append((k_string, v_string))\n+        return \"{%s}\" % (\", \".join(\"%s: %s\" % (k, v) for k, v in strings)), imports\n+\n+\n+class EnumSerializer(BaseSerializer):\n+    def serialize(self):\n+        enum_class = self.value.__class__\n+        module = enum_class.__module__\n+        v_string, v_imports = serializer_factory(self.value.value).serialize()\n+        imports = {'import %s' % module, *v_imports}\n+        return \"%s.%s(%s)\" % (module, enum_class.__name__, v_string), imports\n+\n+\n+class FloatSerializer(BaseSimpleSerializer):\n+    def serialize(self):\n+        if math.isnan(self.value) or math.isinf(self.value):\n+            return 'float(\"{}\")'.format(self.value), set()\n+        return super().serialize()\n+\n+\n+class FrozensetSerializer(BaseSequenceSerializer):\n+    def _format(self):\n+        return \"frozenset([%s])\"\n+\n+\n+class FunctionTypeSerializer(BaseSerializer):\n+    def serialize(self):\n+        if getattr(self.value, \"__self__\", None) and isinstance(self.value.__self__, type):\n+            klass = self.value.__self__\n+            module = klass.__module__\n+            return \"%s.%s.%s\" % (module, klass.__name__, self.value.__name__), {\"import %s\" % module}\n+        # Further error checking\n+        if self.value.__name__ == '<lambda>':\n+            raise ValueError(\"Cannot serialize function: lambda\")\n+        if self.value.__module__ is None:\n+            raise ValueError(\"Cannot serialize function %r: No module\" % self.value)\n+\n+        module_name = self.value.__module__\n+\n+        if '<' not in self.value.__qualname__:  # Qualname can include <locals>\n+            return '%s.%s' % (module_name, self.value.__qualname__), {'import %s' % self.value.__module__}\n+\n+        raise ValueError(\n+            'Could not find function %s in %s.\\n' % (self.value.__name__, module_name)\n+        )\n+\n+\n+class FunctoolsPartialSerializer(BaseSerializer):\n+    def serialize(self):\n+        # Serialize functools.partial() arguments\n+        func_string, func_imports = serializer_factory(self.value.func).serialize()\n+        args_string, args_imports = serializer_factory(self.value.args).serialize()\n+        keywords_string, keywords_imports = serializer_factory(self.value.keywords).serialize()\n+        # Add any imports needed by arguments\n+        imports = {'import functools', *func_imports, *args_imports, *keywords_imports}\n+        return (\n+            'functools.%s(%s, *%s, **%s)' % (\n+                self.value.__class__.__name__,\n+                func_string,\n+                args_string,\n+                keywords_string,\n+            ),\n+            imports,\n+        )\n+\n+\n+class IterableSerializer(BaseSerializer):\n+    def serialize(self):\n+        imports = set()\n+        strings = []\n+        for item in self.value:\n+            item_string, item_imports = serializer_factory(item).serialize()\n+            imports.update(item_imports)\n+            strings.append(item_string)\n+        # When len(strings)==0, the empty iterable should be serialized as\n+        # \"()\", not \"(,)\" because (,) is invalid Python syntax.\n+        value = \"(%s)\" if len(strings) != 1 else \"(%s,)\"\n+        return value % (\", \".join(strings)), imports\n+\n+\n+class ModelFieldSerializer(DeconstructableSerializer):\n+    def serialize(self):\n+        attr_name, path, args, kwargs = self.value.deconstruct()\n+        return self.serialize_deconstructed(path, args, kwargs)\n+\n+\n+class ModelManagerSerializer(DeconstructableSerializer):\n+    def serialize(self):\n+        as_manager, manager_path, qs_path, args, kwargs = self.value.deconstruct()\n+        if as_manager:\n+            name, imports = self._serialize_path(qs_path)\n+            return \"%s.as_manager()\" % name, imports\n+        else:\n+            return self.serialize_deconstructed(manager_path, args, kwargs)\n+\n+\n+class OperationSerializer(BaseSerializer):\n+    def serialize(self):\n+        from django.db.migrations.writer import OperationWriter\n+        string, imports = OperationWriter(self.value, indentation=0).serialize()\n+        # Nested operation, trailing comma is handled in upper OperationWriter._write()\n+        return string.rstrip(','), imports\n+\n+\n+class RegexSerializer(BaseSerializer):\n+    def serialize(self):\n+        regex_pattern, pattern_imports = serializer_factory(self.value.pattern).serialize()\n+        # Turn off default implicit flags (e.g. re.U) because regexes with the\n+        # same implicit and explicit flags aren't equal.\n+        flags = self.value.flags ^ re.compile('').flags\n+        regex_flags, flag_imports = serializer_factory(flags).serialize()\n+        imports = {'import re', *pattern_imports, *flag_imports}\n+        args = [regex_pattern]\n+        if flags:\n+            args.append(regex_flags)\n+        return \"re.compile(%s)\" % ', '.join(args), imports\n+\n+\n+class SequenceSerializer(BaseSequenceSerializer):\n+    def _format(self):\n+        return \"[%s]\"\n+\n+\n+class SetSerializer(BaseSequenceSerializer):\n+    def _format(self):\n+        # Serialize as a set literal except when value is empty because {}\n+        # is an empty dict.\n+        return '{%s}' if self.value else 'set(%s)'\n+\n+\n+class SettingsReferenceSerializer(BaseSerializer):\n+    def serialize(self):\n+        return \"settings.%s\" % self.value.setting_name, {\"from django.conf import settings\"}\n+\n+\n+class TupleSerializer(BaseSequenceSerializer):\n+    def _format(self):\n+        # When len(value)==0, the empty tuple should be serialized as \"()\",\n+        # not \"(,)\" because (,) is invalid Python syntax.\n+        return \"(%s)\" if len(self.value) != 1 else \"(%s,)\"\n+\n+\n+class TypeSerializer(BaseSerializer):\n+    def serialize(self):\n+        special_cases = [\n+            (models.Model, \"models.Model\", []),\n+            (type(None), 'type(None)', []),\n+        ]\n+        for case, string, imports in special_cases:\n+            if case is self.value:\n+                return string, set(imports)\n+        if hasattr(self.value, \"__module__\"):\n+            module = self.value.__module__\n+            if module == builtins.__name__:\n+                return self.value.__name__, set()\n+            else:\n+                return \"%s.%s\" % (module, self.value.__name__), {\"import %s\" % module}\n+\n+\n+class UUIDSerializer(BaseSerializer):\n+    def serialize(self):\n+        return \"uuid.%s\" % repr(self.value), {\"import uuid\"}\n+\n+\n+class Serializer:\n+    _registry = {\n+        # Some of these are order-dependent.\n+        frozenset: FrozensetSerializer,\n+        list: SequenceSerializer,\n+        set: SetSerializer,\n+        tuple: TupleSerializer,\n+        dict: DictionarySerializer,\n+        models.Choices: ChoicesSerializer,\n+        enum.Enum: EnumSerializer,\n+        datetime.datetime: DatetimeDatetimeSerializer,\n+        (datetime.date, datetime.timedelta, datetime.time): DateTimeSerializer,\n+        SettingsReference: SettingsReferenceSerializer,\n+        float: FloatSerializer,\n+        (bool, int, type(None), bytes, str, range): BaseSimpleSerializer,\n+        decimal.Decimal: DecimalSerializer,\n+        (functools.partial, functools.partialmethod): FunctoolsPartialSerializer,\n+        (types.FunctionType, types.BuiltinFunctionType, types.MethodType): FunctionTypeSerializer,\n+        collections.abc.Iterable: IterableSerializer,\n+        (COMPILED_REGEX_TYPE, RegexObject): RegexSerializer,\n+        uuid.UUID: UUIDSerializer,\n+    }\n+\n+    @classmethod\n+    def register(cls, type_, serializer):\n+        if not issubclass(serializer, BaseSerializer):\n+            raise ValueError(\"'%s' must inherit from 'BaseSerializer'.\" % serializer.__name__)\n+        cls._registry[type_] = serializer\n+\n+    @classmethod\n+    def unregister(cls, type_):\n+        cls._registry.pop(type_)\n+\n+\n+def serializer_factory(value):\n+    if isinstance(value, Promise):\n+        value = str(value)\n+    elif isinstance(value, LazyObject):\n+        # The unwrapped value is returned as the first item of the arguments\n+        # tuple.\n+        value = value.__reduce__()[1][0]\n+\n+    if isinstance(value, models.Field):\n+        return ModelFieldSerializer(value)\n+    if isinstance(value, models.manager.BaseManager):\n+        return ModelManagerSerializer(value)\n+    if isinstance(value, Operation):\n+        return OperationSerializer(value)\n+    if isinstance(value, type):\n+        return TypeSerializer(value)\n+    # Anything that knows how to deconstruct itself.\n+    if hasattr(value, 'deconstruct'):\n+        return DeconstructableSerializer(value)\n+    for type_, serializer_cls in Serializer._registry.items():\n+        if isinstance(value, type_):\n+            return serializer_cls(value)\n+    raise ValueError(\n+        \"Cannot serialize: %r\\nThere are some values Django cannot serialize into \"\n+        \"migration files.\\nFor more, see https://docs.djangoproject.com/en/%s/\"\n+        \"topics/migrations/#migration-serializing\" % (value, get_docs_version())\n+    )\ndiff --git a/.harness/checkpoints/3b82b70aad53/--Users--huetuanthi--dev--agents--harness--tests--migrations--test_writer.py b/.harness/checkpoints/3b82b70aad53/--Users--huetuanthi--dev--agents--harness--tests--migrations--test_writer.py\nnew file mode 100644\nindex 0000000000..f97d76e9ad\n--- /dev/null\n+++ b/.harness/checkpoints/3b82b70aad53/--Users--huetuanthi--dev--agents--harness--tests--migrations--test_writer.py\n@@ -0,0 +1,714 @@\n+import datetime\n+import decimal\n+import enum\n+import functools\n+import math\n+import os\n+import re\n+import uuid\n+from unittest import mock\n+\n+import custom_migration_operations.more_operations\n+import custom_migration_operations.operations\n+\n+from django import get_version\n+from django.conf import SettingsReference, settings\n+from django.core.validators import EmailValidator, RegexValidator\n+from django.db import migrations, models\n+from django.db.migrations.serializer import BaseSerializer\n+from django.db.migrations.writer import MigrationWriter, OperationWriter\n+from django.test import SimpleTestCase\n+from django.utils.deconstruct import deconstructible\n+from django.utils.functional import SimpleLazyObject\n+from django.utils.timezone import get_default_timezone, get_fixed_timezone, utc\n+from django.utils.translation import gettext_lazy as _\n+\n+from .models import FoodManager, FoodQuerySet\n+\n+\n+class Money(decimal.Decimal):\n+    def deconstruct(self):\n+        return (\n+            '%s.%s' % (self.__class__.__module__, self.__class__.__name__),\n+            [str(self)],\n+            {}\n+        )\n+\n+\n+class TestModel1:\n+    def upload_to(self):\n+        return '/somewhere/dynamic/'\n+    thing = models.FileField(upload_to=upload_to)\n+\n+\n+class OperationWriterTests(SimpleTestCase):\n+\n+    def test_empty_signature(self):\n+        operation = custom_migration_operations.operations.TestOperation()\n+        buff, imports = OperationWriter(operation, indentation=0).serialize()\n+        self.assertEqual(imports, {'import custom_migration_operations.operations'})\n+        self.assertEqual(\n+            buff,\n+            'custom_migration_operations.operations.TestOperation(\\n'\n+            '),'\n+        )\n+\n+    def test_args_signature(self):\n+        operation = custom_migration_operations.operations.ArgsOperation(1, 2)\n+        buff, imports = OperationWriter(operation, indentation=0).serialize()\n+        self.assertEqual(imports, {'import custom_migration_operations.operations'})\n+        self.assertEqual(\n+            buff,\n+            'custom_migration_operations.operations.ArgsOperation(\\n'\n+            '    arg1=1,\\n'\n+            '    arg2=2,\\n'\n+            '),'\n+        )\n+\n+    def test_kwargs_signature(self):\n+        operation = custom_migration_operations.operations.KwargsOperation(kwarg1=1)\n+        buff, imports = OperationWriter(operation, indentation=0).serialize()\n+        self.assertEqual(imports, {'import custom_migration_operations.operations'})\n+        self.assertEqual(\n+            buff,\n+            'custom_migration_operations.operations.KwargsOperation(\\n'\n+            '    kwarg1=1,\\n'\n+            '),'\n+        )\n+\n+    def test_args_kwargs_signature(self):\n+        operation = custom_migration_operations.operations.ArgsKwargsOperation(1, 2, kwarg2=4)\n+        buff, imports = OperationWriter(operation, indentation=0).serialize()\n+        self.assertEqual(imports, {'import custom_migration_operations.operations'})\n+        self.assertEqual(\n+            buff,\n+            'custom_migration_operations.operations.ArgsKwargsOperation(\\n'\n+            '    arg1=1,\\n'\n+            '    arg2=2,\\n'\n+            '    kwarg2=4,\\n'\n+            '),'\n+        )\n+\n+    def test_nested_args_signature(self):\n+        operation = custom_migration_operations.operations.ArgsOperation(\n+            custom_migration_operations.operations.ArgsOperation(1, 2),\n+            custom_migration_operations.operations.KwargsOperation(kwarg1=3, kwarg2=4)\n+        )\n+        buff, imports = OperationWriter(operation, indentation=0).serialize()\n+        self.assertEqual(imports, {'import custom_migration_operations.operations'})\n+        self.assertEqual(\n+            buff,\n+            'custom_migration_operations.operations.ArgsOperation(\\n'\n+            '    arg1=custom_migration_operations.operations.ArgsOperation(\\n'\n+            '        arg1=1,\\n'\n+            '        arg2=2,\\n'\n+            '    ),\\n'\n+            '    arg2=custom_migration_operations.operations.KwargsOperation(\\n'\n+            '        kwarg1=3,\\n'\n+            '        kwarg2=4,\\n'\n+            '    ),\\n'\n+            '),'\n+        )\n+\n+    def test_multiline_args_signature(self):\n+        operation = custom_migration_operations.operations.ArgsOperation(\"test\\n    arg1\", \"test\\narg2\")\n+        buff, imports = OperationWriter(operation, indentation=0).serialize()\n+        self.assertEqual(imports, {'import custom_migration_operations.operations'})\n+        self.assertEqual(\n+            buff,\n+            \"custom_migration_operations.operations.ArgsOperation(\\n\"\n+            \"    arg1='test\\\\n    arg1',\\n\"\n+            \"    arg2='test\\\\narg2',\\n\"\n+            \"),\"\n+        )\n+\n+    def test_expand_args_signature(self):\n+        operation = custom_migration_operations.operations.ExpandArgsOperation([1, 2])\n+        buff, imports = OperationWriter(operation, indentation=0).serialize()\n+        self.assertEqual(imports, {'import custom_migration_operations.operations'})\n+        self.assertEqual(\n+            buff,\n+            'custom_migration_operations.operations.ExpandArgsOperation(\\n'\n+            '    arg=[\\n'\n+            '        1,\\n'\n+            '        2,\\n'\n+            '    ],\\n'\n+            '),'\n+        )\n+\n+    def test_nested_operation_expand_args_signature(self):\n+        operation = custom_migration_operations.operations.ExpandArgsOperation(\n+            arg=[\n+                custom_migration_operations.operations.KwargsOperation(\n+                    kwarg1=1,\n+                    kwarg2=2,\n+                ),\n+            ]\n+        )\n+        buff, imports = OperationWriter(operation, indentation=0).serialize()\n+        self.assertEqual(imports, {'import custom_migration_operations.operations'})\n+        self.assertEqual(\n+            buff,\n+            'custom_migration_operations.operations.ExpandArgsOperation(\\n'\n+            '    arg=[\\n'\n+            '        custom_migration_operations.operations.KwargsOperation(\\n'\n+            '            kwarg1=1,\\n'\n+            '            kwarg2=2,\\n'\n+            '        ),\\n'\n+            '    ],\\n'\n+            '),'\n+        )\n+\n+\n+class WriterTests(SimpleTestCase):\n+    \"\"\"\n+    Tests the migration writer (makes migration files from Migration instances)\n+    \"\"\"\n+\n+    def safe_exec(self, string, value=None):\n+        d = {}\n+        try:\n+            exec(string, globals(), d)\n+        except Exception as e:\n+            if value:\n+                self.fail(\"Could not exec %r (from value %r): %s\" % (string.strip(), value, e))\n+            else:\n+                self.fail(\"Could not exec %r: %s\" % (string.strip(), e))\n+        return d\n+\n+    def serialize_round_trip(self, value):\n+        string, imports = MigrationWriter.serialize(value)\n+        return self.safe_exec(\"%s\\ntest_value_result = %s\" % (\"\\n\".join(imports), string), value)['test_value_result']\n+\n+    def assertSerializedEqual(self, value):\n+        self.assertEqual(self.serialize_round_trip(value), value)\n+\n+    def assertSerializedResultEqual(self, value, target):\n+        self.assertEqual(MigrationWriter.serialize(value), target)\n+\n+    def assertSerializedFieldEqual(self, value):\n+        new_value = self.serialize_round_trip(value)\n+        self.assertEqual(value.__class__, new_value.__class__)\n+        self.assertEqual(value.max_length, new_value.max_length)\n+        self.assertEqual(value.null, new_value.null)\n+        self.assertEqual(value.unique, new_value.unique)\n+\n+    def test_serialize_numbers(self):\n+        self.assertSerializedEqual(1)\n+        self.assertSerializedEqual(1.2)\n+        self.assertTrue(math.isinf(self.serialize_round_trip(float(\"inf\"))))\n+        self.assertTrue(math.isinf(self.serialize_round_trip(float(\"-inf\"))))\n+        self.assertTrue(math.isnan(self.serialize_round_trip(float(\"nan\"))))\n+\n+        self.assertSerializedEqual(decimal.Decimal('1.3'))\n+        self.assertSerializedResultEqual(\n+            decimal.Decimal('1.3'),\n+            (\"Decimal('1.3')\", {'from decimal import Decimal'})\n+        )\n+\n+        self.assertSerializedEqual(Money('1.3'))\n+        self.assertSerializedResultEqual(\n+            Money('1.3'),\n+            (\"migrations.test_writer.Money('1.3')\", {'import migrations.test_writer'})\n+        )\n+\n+    def test_serialize_constants(self):\n+        self.assertSerializedEqual(None)\n+        self.assertSerializedEqual(True)\n+        self.assertSerializedEqual(False)\n+\n+    def test_serialize_strings(self):\n+        self.assertSerializedEqual(b\"foobar\")\n+        string, imports = MigrationWriter.serialize(b\"foobar\")\n+        self.assertEqual(string, \"b'foobar'\")\n+        self.assertSerializedEqual(\"f\u00f6ob\u00e1r\")\n+        string, imports = MigrationWriter.serialize(\"foobar\")\n+        self.assertEqual(string, \"'foobar'\")\n+\n+    def test_serialize_multiline_strings(self):\n+        self.assertSerializedEqual(b\"foo\\nbar\")\n+        string, imports = MigrationWriter.serialize(b\"foo\\nbar\")\n+        self.assertEqual(string, \"b'foo\\\\nbar'\")\n+        self.assertSerializedEqual(\"f\u00f6o\\nb\u00e1r\")\n+        string, imports = MigrationWriter.serialize(\"foo\\nbar\")\n+        self.assertEqual(string, \"'foo\\\\nbar'\")\n+\n+    def test_serialize_collections(self):\n+        self.assertSerializedEqual({1: 2})\n+        self.assertSerializedEqual([\"a\", 2, True, None])\n+        self.assertSerializedEqual({2, 3, \"eighty\"})\n+        self.assertSerializedEqual({\"lalalala\": [\"yeah\", \"no\", \"maybe\"]})\n+        self.assertSerializedEqual(_('Hello'))\n+\n+    def test_serialize_builtin_types(self):\n+        self.assertSerializedEqual([list, tuple, dict, set, frozenset])\n+        self.assertSerializedResultEqual(\n+            [list, tuple, dict, set, frozenset],\n+            (\"[list, tuple, dict, set, frozenset]\", set())\n+        )\n+\n+    def test_serialize_lazy_objects(self):\n+        pattern = re.compile(r'^foo$')\n+        lazy_pattern = SimpleLazyObject(lambda: pattern)\n+        self.assertEqual(self.serialize_round_trip(lazy_pattern), pattern)\n+\n+    def test_serialize_enums(self):\n+        class TextEnum(enum.Enum):\n+            A = 'a-value'\n+            B = 'value-b'\n+\n+        class BinaryEnum(enum.Enum):\n+            A = b'a-value'\n+            B = b'value-b'\n+\n+        class IntEnum(enum.IntEnum):\n+            A = 1\n+            B = 2\n+\n+        self.assertSerializedResultEqual(\n+            TextEnum.A,\n+            (\"migrations.test_writer.TextEnum('a-value')\", {'import migrations.test_writer'})\n+        )\n+        self.assertSerializedResultEqual(\n+            BinaryEnum.A,\n+            (\"migrations.test_writer.BinaryEnum(b'a-value')\", {'import migrations.test_writer'})\n+        )\n+        self.assertSerializedResultEqual(\n+            IntEnum.B,\n+            (\"migrations.test_writer.IntEnum(2)\", {'import migrations.test_writer'})\n+        )\n+\n+        field = models.CharField(default=TextEnum.B, choices=[(m.value, m) for m in TextEnum])\n+        string = MigrationWriter.serialize(field)[0]\n+        self.assertEqual(\n+            string,\n+            \"models.CharField(choices=[\"\n+            \"('a-value', migrations.test_writer.TextEnum('a-value')), \"\n+            \"('value-b', migrations.test_writer.TextEnum('value-b'))], \"\n+            \"default=migrations.test_writer.TextEnum('value-b'))\"\n+        )\n+        field = models.CharField(default=BinaryEnum.B, choices=[(m.value, m) for m in BinaryEnum])\n+        string = MigrationWriter.serialize(field)[0]\n+        self.assertEqual(\n+            string,\n+            \"models.CharField(choices=[\"\n+            \"(b'a-value', migrations.test_writer.BinaryEnum(b'a-value')), \"\n+            \"(b'value-b', migrations.test_writer.BinaryEnum(b'value-b'))], \"\n+            \"default=migrations.test_writer.BinaryEnum(b'value-b'))\"\n+        )\n+        field = models.IntegerField(default=IntEnum.A, choices=[(m.value, m) for m in IntEnum])\n+        string = MigrationWriter.serialize(field)[0]\n+        self.assertEqual(\n+            string,\n+            \"models.IntegerField(choices=[\"\n+            \"(1, migrations.test_writer.IntEnum(1)), \"\n+            \"(2, migrations.test_writer.IntEnum(2))], \"\n+            \"default=migrations.test_writer.IntEnum(1))\"\n+        )\n+\n+    def test_serialize_choices(self):\n+        class TextChoices(models.TextChoices):\n+            A = 'A', 'A value'\n+            B = 'B', 'B value'\n+\n+        class IntegerChoices(models.IntegerChoices):\n+            A = 1, 'One'\n+            B = 2, 'Two'\n+\n+        class DateChoices(datetime.date, models.Choices):\n+            DATE_1 = 1969, 7, 20, 'First date'\n+            DATE_2 = 1969, 11, 19, 'Second date'\n+\n+        self.assertSerializedResultEqual(TextChoices.A, (\"'A'\", set()))\n+        self.assertSerializedResultEqual(IntegerChoices.A, ('1', set()))\n+        self.assertSerializedResultEqual(\n+            DateChoices.DATE_1,\n+            ('datetime.date(1969, 7, 20)', {'import datetime'}),\n+        )\n+        field = models.CharField(default=TextChoices.B, choices=TextChoices.choices)\n+        string = MigrationWriter.serialize(field)[0]\n+        self.assertEqual(\n+            string,\n+            \"models.CharField(choices=[('A', 'A value'), ('B', 'B value')], \"\n+            \"default='B')\",\n+        )\n+        field = models.IntegerField(default=IntegerChoices.B, choices=IntegerChoices.choices)\n+        string = MigrationWriter.serialize(field)[0]\n+        self.assertEqual(\n+            string,\n+            \"models.IntegerField(choices=[(1, 'One'), (2, 'Two')], default=2)\",\n+        )\n+        field = models.DateField(default=DateChoices.DATE_2, choices=DateChoices.choices)\n+        string = MigrationWriter.serialize(field)[0]\n+        self.assertEqual(\n+            string,\n+            \"models.DateField(choices=[\"\n+            \"(datetime.date(1969, 7, 20), 'First date'), \"\n+            \"(datetime.date(1969, 11, 19), 'Second date')], \"\n+            \"default=datetime.date(1969, 11, 19))\"\n+        )\n+\n+    def test_serialize_uuid(self):\n+        self.assertSerializedEqual(uuid.uuid1())\n+        self.assertSerializedEqual(uuid.uuid4())\n+\n+        uuid_a = uuid.UUID('5c859437-d061-4847-b3f7-e6b78852f8c8')\n+        uuid_b = uuid.UUID('c7853ec1-2ea3-4359-b02d-b54e8f1bcee2')\n+        self.assertSerializedResultEqual(\n+            uuid_a,\n+            (\"uuid.UUID('5c859437-d061-4847-b3f7-e6b78852f8c8')\", {'import uuid'})\n+        )\n+        self.assertSerializedResultEqual(\n+            uuid_b,\n+            (\"uuid.UUID('c7853ec1-2ea3-4359-b02d-b54e8f1bcee2')\", {'import uuid'})\n+        )\n+\n+        field = models.UUIDField(choices=((uuid_a, 'UUID A'), (uuid_b, 'UUID B')), default=uuid_a)\n+        string = MigrationWriter.serialize(field)[0]\n+        self.assertEqual(\n+            string,\n+            \"models.UUIDField(choices=[\"\n+            \"(uuid.UUID('5c859437-d061-4847-b3f7-e6b78852f8c8'), 'UUID A'), \"\n+            \"(uuid.UUID('c7853ec1-2ea3-4359-b02d-b54e8f1bcee2'), 'UUID B')], \"\n+            \"default=uuid.UUID('5c859437-d061-4847-b3f7-e6b78852f8c8'))\"\n+        )\n+\n+    def test_serialize_functions(self):\n+        with self.assertRaisesMessage(ValueError, 'Cannot serialize function: lambda'):\n+            self.assertSerializedEqual(lambda x: 42)\n+        self.assertSerializedEqual(models.SET_NULL)\n+        string, imports = MigrationWriter.serialize(models.SET(42))\n+        self.assertEqual(string, 'models.SET(42)')\n+        self.serialize_round_trip(models.SET(42))\n+\n+    def test_serialize_datetime(self):\n+        self.assertSerializedEqual(datetime.datetime.utcnow())\n+        self.assertSerializedEqual(datetime.datetime.utcnow)\n+        self.assertSerializedEqual(datetime.datetime.today())\n+        self.assertSerializedEqual(datetime.datetime.today)\n+        self.assertSerializedEqual(datetime.date.today())\n+        self.assertSerializedEqual(datetime.date.today)\n+        self.assertSerializedEqual(datetime.datetime.now().time())\n+        self.assertSerializedEqual(datetime.datetime(2014, 1, 1, 1, 1, tzinfo=get_default_timezone()))\n+        self.assertSerializedEqual(datetime.datetime(2013, 12, 31, 22, 1, tzinfo=get_fixed_timezone(180)))\n+        self.assertSerializedResultEqual(\n+            datetime.datetime(2014, 1, 1, 1, 1),\n+            (\"datetime.datetime(2014, 1, 1, 1, 1)\", {'import datetime'})\n+        )\n+        self.assertSerializedResultEqual(\n+            datetime.datetime(2012, 1, 1, 1, 1, tzinfo=utc),\n+            (\n+                \"datetime.datetime(2012, 1, 1, 1, 1, tzinfo=utc)\",\n+                {'import datetime', 'from django.utils.timezone import utc'},\n+            )\n+        )\n+\n+    def test_serialize_fields(self):\n+        self.assertSerializedFieldEqual(models.CharField(max_length=255))\n+        self.assertSerializedResultEqual(\n+            models.CharField(max_length=255),\n+            (\"models.CharField(max_length=255)\", {\"from django.db import models\"})\n+        )\n+        self.assertSerializedFieldEqual(models.TextField(null=True, blank=True))\n+        self.assertSerializedResultEqual(\n+            models.TextField(null=True, blank=True),\n+            (\"models.TextField(blank=True, null=True)\", {'from django.db import models'})\n+        )\n+\n+    def test_serialize_settings(self):\n+        self.assertSerializedEqual(SettingsReference(settings.AUTH_USER_MODEL, \"AUTH_USER_MODEL\"))\n+        self.assertSerializedResultEqual(\n+            SettingsReference(\"someapp.model\", \"AUTH_USER_MODEL\"),\n+            (\"settings.AUTH_USER_MODEL\", {\"from django.conf import settings\"})\n+        )\n+\n+    def test_serialize_iterators(self):\n+        self.assertSerializedResultEqual(\n+            ((x, x * x) for x in range(3)),\n+            (\"((0, 0), (1, 1), (2, 4))\", set())\n+        )\n+\n+    def test_serialize_compiled_regex(self):\n+        \"\"\"\n+        Make sure compiled regex can be serialized.\n+        \"\"\"\n+        regex = re.compile(r'^\\w+$')\n+        self.assertSerializedEqual(regex)\n+\n+    def test_serialize_class_based_validators(self):\n+        \"\"\"\n+        Ticket #22943: Test serialization of class-based validators, including\n+        compiled regexes.\n+        \"\"\"\n+        validator = RegexValidator(message=\"hello\")\n+        string = MigrationWriter.serialize(validator)[0]\n+        self.assertEqual(string, \"django.core.validators.RegexValidator(message='hello')\")\n+        self.serialize_round_trip(validator)\n+\n+        # Test with a compiled regex.\n+        validator = RegexValidator(regex=re.compile(r'^\\w+$'))\n+        string = MigrationWriter.serialize(validator)[0]\n+        self.assertEqual(string, \"django.core.validators.RegexValidator(regex=re.compile('^\\\\\\\\w+$'))\")\n+        self.serialize_round_trip(validator)\n+\n+        # Test a string regex with flag\n+        validator = RegexValidator(r'^[0-9]+$', flags=re.S)\n+        string = MigrationWriter.serialize(validator)[0]\n+        self.assertEqual(string, \"django.core.validators.RegexValidator('^[0-9]+$', flags=re.RegexFlag(16))\")\n+        self.serialize_round_trip(validator)\n+\n+        # Test message and code\n+        validator = RegexValidator('^[-a-zA-Z0-9_]+$', 'Invalid', 'invalid')\n+        string = MigrationWriter.serialize(validator)[0]\n+        self.assertEqual(string, \"django.core.validators.RegexValidator('^[-a-zA-Z0-9_]+$', 'Invalid', 'invalid')\")\n+        self.serialize_round_trip(validator)\n+\n+        # Test with a subclass.\n+        validator = EmailValidator(message=\"hello\")\n+        string = MigrationWriter.serialize(validator)[0]\n+        self.assertEqual(string, \"django.core.validators.EmailValidator(message='hello')\")\n+        self.serialize_round_trip(validator)\n+\n+        validator = deconstructible(path=\"migrations.test_writer.EmailValidator\")(EmailValidator)(message=\"hello\")\n+        string = MigrationWriter.serialize(validator)[0]\n+        self.assertEqual(string, \"migrations.test_writer.EmailValidator(message='hello')\")\n+\n+        validator = deconstructible(path=\"custom.EmailValidator\")(EmailValidator)(message=\"hello\")\n+        with self.assertRaisesMessage(ImportError, \"No module named 'custom'\"):\n+            MigrationWriter.serialize(validator)\n+\n+        validator = deconstructible(path=\"django.core.validators.EmailValidator2\")(EmailValidator)(message=\"hello\")\n+        with self.assertRaisesMessage(ValueError, \"Could not find object EmailValidator2 in django.core.validators.\"):\n+            MigrationWriter.serialize(validator)\n+\n+    def test_serialize_empty_nonempty_tuple(self):\n+        \"\"\"\n+        Ticket #22679: makemigrations generates invalid code for (an empty\n+        tuple) default_permissions = ()\n+        \"\"\"\n+        empty_tuple = ()\n+        one_item_tuple = ('a',)\n+        many_items_tuple = ('a', 'b', 'c')\n+        self.assertSerializedEqual(empty_tuple)\n+        self.assertSerializedEqual(one_item_tuple)\n+        self.assertSerializedEqual(many_items_tuple)\n+\n+    def test_serialize_range(self):\n+        string, imports = MigrationWriter.serialize(range(1, 5))\n+        self.assertEqual(string, 'range(1, 5)')\n+        self.assertEqual(imports, set())\n+\n+    def test_serialize_builtins(self):\n+        string, imports = MigrationWriter.serialize(range)\n+        self.assertEqual(string, 'range')\n+        self.assertEqual(imports, set())\n+\n+    def test_serialize_unbound_method_reference(self):\n+        \"\"\"An unbound method used within a class body can be serialized.\"\"\"\n+        self.serialize_round_trip(TestModel1.thing)\n+\n+    def test_serialize_local_function_reference(self):\n+        \"\"\"A reference in a local scope can't be serialized.\"\"\"\n+        class TestModel2:\n+            def upload_to(self):\n+                return \"somewhere dynamic\"\n+            thing = models.FileField(upload_to=upload_to)\n+\n+        with self.assertRaisesMessage(ValueError, 'Could not find function upload_to in migrations.test_writer'):\n+            self.serialize_round_trip(TestModel2.thing)\n+\n+    def test_serialize_managers(self):\n+        self.assertSerializedEqual(models.Manager())\n+        self.assertSerializedResultEqual(\n+            FoodQuerySet.as_manager(),\n+            ('migrations.models.FoodQuerySet.as_manager()', {'import migrations.models'})\n+        )\n+        self.assertSerializedEqual(FoodManager('a', 'b'))\n+        self.assertSerializedEqual(FoodManager('x', 'y', c=3, d=4))\n+\n+    def test_serialize_frozensets(self):\n+        self.assertSerializedEqual(frozenset())\n+        self.assertSerializedEqual(frozenset(\"let it go\"))\n+\n+    def test_serialize_set(self):\n+        self.assertSerializedEqual(set())\n+        self.assertSerializedResultEqual(set(), ('set()', set()))\n+        self.assertSerializedEqual({'a'})\n+        self.assertSerializedResultEqual({'a'}, (\"{'a'}\", set()))\n+\n+    def test_serialize_timedelta(self):\n+        self.assertSerializedEqual(datetime.timedelta())\n+        self.assertSerializedEqual(datetime.timedelta(minutes=42))\n+\n+    def test_serialize_functools_partial(self):\n+        value = functools.partial(datetime.timedelta, 1, seconds=2)\n+        result = self.serialize_round_trip(value)\n+        self.assertEqual(result.func, value.func)\n+        self.assertEqual(result.args, value.args)\n+        self.assertEqual(result.keywords, value.keywords)\n+\n+    def test_serialize_functools_partialmethod(self):\n+        value = functools.partialmethod(datetime.timedelta, 1, seconds=2)\n+        result = self.serialize_round_trip(value)\n+        self.assertIsInstance(result, functools.partialmethod)\n+        self.assertEqual(result.func, value.func)\n+        self.assertEqual(result.args, value.args)\n+        self.assertEqual(result.keywords, value.keywords)\n+\n+    def test_serialize_type_none(self):\n+        self.assertSerializedEqual(type(None))\n+\n+    def test_simple_migration(self):\n+        \"\"\"\n+        Tests serializing a simple migration.\n+        \"\"\"\n+        fields = {\n+            'charfield': models.DateTimeField(default=datetime.datetime.utcnow),\n+            'datetimefield': models.DateTimeField(default=datetime.datetime.utcnow),\n+        }\n+\n+        options = {\n+            'verbose_name': 'My model',\n+            'verbose_name_plural': 'My models',\n+        }\n+\n+        migration = type(\"Migration\", (migrations.Migration,), {\n+            \"operations\": [\n+                migrations.CreateModel(\"MyModel\", tuple(fields.items()), options, (models.Model,)),\n+                migrations.CreateModel(\"MyModel2\", tuple(fields.items()), bases=(models.Model,)),\n+                migrations.CreateModel(\n+                    name=\"MyModel3\", fields=tuple(fields.items()), options=options, bases=(models.Model,)\n+                ),\n+                migrations.DeleteModel(\"MyModel\"),\n+                migrations.AddField(\"OtherModel\", \"datetimefield\", fields[\"datetimefield\"]),\n+            ],\n+            \"dependencies\": [(\"testapp\", \"some_other_one\")],\n+        })\n+        writer = MigrationWriter(migration)\n+        output = writer.as_string()\n+        # We don't test the output formatting - that's too fragile.\n+        # Just make sure it runs for now, and that things look alright.\n+        result = self.safe_exec(output)\n+        self.assertIn(\"Migration\", result)\n+\n+    def test_migration_path(self):\n+        test_apps = [\n+            'migrations.migrations_test_apps.normal',\n+            'migrations.migrations_test_apps.with_package_model',\n+            'migrations.migrations_test_apps.without_init_file',\n+        ]\n+\n+        base_dir = os.path.dirname(os.path.dirname(__file__))\n+\n+        for app in test_apps:\n+            with self.modify_settings(INSTALLED_APPS={'append': app}):\n+                migration = migrations.Migration('0001_initial', app.split('.')[-1])\n+                expected_path = os.path.join(base_dir, *(app.split('.') + ['migrations', '0001_initial.py']))\n+                writer = MigrationWriter(migration)\n+                self.assertEqual(writer.path, expected_path)\n+\n+    def test_custom_operation(self):\n+        migration = type(\"Migration\", (migrations.Migration,), {\n+            \"operations\": [\n+                custom_migration_operations.operations.TestOperation(),\n+                custom_migration_operations.operations.CreateModel(),\n+                migrations.CreateModel(\"MyModel\", (), {}, (models.Model,)),\n+                custom_migration_operations.more_operations.TestOperation()\n+            ],\n+            \"dependencies\": []\n+        })\n+        writer = MigrationWriter(migration)\n+        output = writer.as_string()\n+        result = self.safe_exec(output)\n+        self.assertIn(\"custom_migration_operations\", result)\n+        self.assertNotEqual(\n+            result['custom_migration_operations'].operations.TestOperation,\n+            result['custom_migration_operations'].more_operations.TestOperation\n+        )\n+\n+    def test_sorted_imports(self):\n+        \"\"\"\n+        #24155 - Tests ordering of imports.\n+        \"\"\"\n+        migration = type(\"Migration\", (migrations.Migration,), {\n+            \"operations\": [\n+                migrations.AddField(\"mymodel\", \"myfield\", models.DateTimeField(\n+                    default=datetime.datetime(2012, 1, 1, 1, 1, tzinfo=utc),\n+                )),\n+            ]\n+        })\n+        writer = MigrationWriter(migration)\n+        output = writer.as_string()\n+        self.assertIn(\n+            \"import datetime\\n\"\n+            \"from django.db import migrations, models\\n\"\n+            \"from django.utils.timezone import utc\\n\",\n+            output\n+        )\n+\n+    def test_migration_file_header_comments(self):\n+        \"\"\"\n+        Test comments at top of file.\n+        \"\"\"\n+        migration = type(\"Migration\", (migrations.Migration,), {\n+            \"operations\": []\n+        })\n+        dt = datetime.datetime(2015, 7, 31, 4, 40, 0, 0, tzinfo=utc)\n+        with mock.patch('django.db.migrations.writer.now', lambda: dt):\n+            for include_header in (True, False):\n+                with self.subTest(include_header=include_header):\n+                    writer = MigrationWriter(migration, include_header)\n+                    output = writer.as_string()\n+\n+                    self.assertEqual(\n+                        include_header,\n+                        output.startswith(\n+                            \"# Generated by Django %s on 2015-07-31 04:40\\n\\n\" % get_version()\n+                        )\n+                    )\n+                    if not include_header:\n+                        # Make sure the output starts with something that's not\n+                        # a comment or indentation or blank line\n+                        self.assertRegex(output.splitlines(keepends=True)[0], r\"^[^#\\s]+\")\n+\n+    def test_models_import_omitted(self):\n+        \"\"\"\n+        django.db.models shouldn't be imported if unused.\n+        \"\"\"\n+        migration = type(\"Migration\", (migrations.Migration,), {\n+            \"operations\": [\n+                migrations.AlterModelOptions(\n+                    name='model',\n+                    options={'verbose_name': 'model', 'verbose_name_plural': 'models'},\n+                ),\n+            ]\n+        })\n+        writer = MigrationWriter(migration)\n+        output = writer.as_string()\n+        self.assertIn(\"from django.db import migrations\\n\", output)\n+\n+    def test_deconstruct_class_arguments(self):\n+        # Yes, it doesn't make sense to use a class as a default for a\n+        # CharField. It does make sense for custom fields though, for example\n+        # an enumfield that takes the enum class as an argument.\n+        class DeconstructibleInstances:\n+            def deconstruct(self):\n+                return ('DeconstructibleInstances', [], {})\n+\n+        string = MigrationWriter.serialize(models.CharField(default=DeconstructibleInstances))[0]\n+        self.assertEqual(string, \"models.CharField(default=migrations.test_writer.DeconstructibleInstances)\")\n+\n+    def test_register_serializer(self):\n+        class ComplexSerializer(BaseSerializer):\n+            def serialize(self):\n+                return 'complex(%r)' % self.value, {}\n+\n+        MigrationWriter.register_serializer(complex, ComplexSerializer)\n+        self.assertSerializedEqual(complex(1, 2))\n+        MigrationWriter.unregister_serializer(complex)\n+        with self.assertRaisesMessage(ValueError, 'Cannot serialize: (1+2j)'):\n+            self.assertSerializedEqual(complex(1, 2))\n+\n+    def test_register_non_serializer(self):\n+        with self.assertRaisesMessage(ValueError, \"'TestModel1' must inherit from 'BaseSerializer'.\"):\n+            MigrationWriter.register_serializer(complex, TestModel1)\ndiff --git a/django/db/migrations/serializer.py b/django/db/migrations/serializer.py\nindex 27b5cbd379..4275f91a7f 100644\n--- a/django/db/migrations/serializer.py\n+++ b/django/db/migrations/serializer.py\n@@ -120,9 +120,8 @@ class EnumSerializer(BaseSerializer):\n     def serialize(self):\n         enum_class = self.value.__class__\n         module = enum_class.__module__\n-        v_string, v_imports = serializer_factory(self.value.value).serialize()\n-        imports = {'import %s' % module, *v_imports}\n-        return \"%s.%s(%s)\" % (module, enum_class.__name__, v_string), imports\n+        imports = {'import %s' % module}\n+        return \"%s.%s[%r]\" % (module, enum_class.__name__, self.value.name), imports\n \n \n class FloatSerializer(BaseSimpleSerializer):\ndiff --git a/tests/migrations/test_writer.py b/tests/migrations/test_writer.py\nindex f97d76e9ad..39c9ab47b9 100644\n--- a/tests/migrations/test_writer.py\n+++ b/tests/migrations/test_writer.py\n@@ -267,15 +267,15 @@ class WriterTests(SimpleTestCase):\n \n         self.assertSerializedResultEqual(\n             TextEnum.A,\n-            (\"migrations.test_writer.TextEnum('a-value')\", {'import migrations.test_writer'})\n+            (\"migrations.test_writer.TextEnum['A']\", {'import migrations.test_writer'})\n         )\n         self.assertSerializedResultEqual(\n             BinaryEnum.A,\n-            (\"migrations.test_writer.BinaryEnum(b'a-value')\", {'import migrations.test_writer'})\n+            (\"migrations.test_writer.BinaryEnum['A']\", {'import migrations.test_writer'})\n         )\n         self.assertSerializedResultEqual(\n             IntEnum.B,\n-            (\"migrations.test_writer.IntEnum(2)\", {'import migrations.test_writer'})\n+            (\"migrations.test_writer.IntEnum['B']\", {'import migrations.test_writer'})\n         )\n \n         field = models.CharField(default=TextEnum.B, choices=[(m.value, m) for m in TextEnum])\n@@ -283,27 +283,27 @@ class WriterTests(SimpleTestCase):\n         self.assertEqual(\n             string,\n             \"models.CharField(choices=[\"\n-            \"('a-value', migrations.test_writer.TextEnum('a-value')), \"\n-            \"('value-b', migrations.test_writer.TextEnum('value-b'))], \"\n-            \"default=migrations.test_writer.TextEnum('value-b'))\"\n+            \"('a-value', migrations.test_writer.TextEnum['A']), \"\n+            \"('value-b', migrations.test_writer.TextEnum['B'])], \"\n+            \"default=migrations.test_writer.TextEnum['B'])\"\n         )\n         field = models.CharField(default=BinaryEnum.B, choices=[(m.value, m) for m in BinaryEnum])\n         string = MigrationWriter.serialize(field)[0]\n         self.assertEqual(\n             string,\n             \"models.CharField(choices=[\"\n-            \"(b'a-value', migrations.test_writer.BinaryEnum(b'a-value')), \"\n-            \"(b'value-b', migrations.test_writer.BinaryEnum(b'value-b'))], \"\n-            \"default=migrations.test_writer.BinaryEnum(b'value-b'))\"\n+            \"(b'a-value', migrations.test_writer.BinaryEnum['A']), \"\n+            \"(b'value-b', migrations.test_writer.BinaryEnum['B'])], \"\n+            \"default=migrations.test_writer.BinaryEnum['B'])\"\n         )\n         field = models.IntegerField(default=IntEnum.A, choices=[(m.value, m) for m in IntEnum])\n         string = MigrationWriter.serialize(field)[0]\n         self.assertEqual(\n             string,\n             \"models.IntegerField(choices=[\"\n-            \"(1, migrations.test_writer.IntEnum(1)), \"\n-            \"(2, migrations.test_writer.IntEnum(2))], \"\n-            \"default=migrations.test_writer.IntEnum(1))\"\n+            \"(1, migrations.test_writer.IntEnum['A']), \"\n+            \"(2, migrations.test_writer.IntEnum['B'])], \"\n+            \"default=migrations.test_writer.IntEnum['A'])\"\n         )\n \n     def test_serialize_choices(self):\n"}
{"instance_id": "django__django-11848", "model_name_or_path": "harness/claude-sonnet-4-6", "model_patch": "diff --git a/django/utils/http.py b/django/utils/http.py\nindex 572cfb4347..a35d686e03 100644\n--- a/django/utils/http.py\n+++ b/django/utils/http.py\n@@ -176,10 +176,11 @@ def parse_http_date(date):\n     try:\n         year = int(m.group('year'))\n         if year < 100:\n-            if year < 70:\n-                year += 2000\n-            else:\n-                year += 1900\n+            current_year = datetime.datetime.utcnow().year\n+            current_century = current_year - (current_year % 100)\n+            year += current_century\n+            if year > current_year + 50:\n+                year -= 100\n         month = MONTHS.index(m.group('mon').lower()) + 1\n         day = int(m.group('day'))\n         hour = int(m.group('hour'))\ndiff --git a/tests/utils_tests/test_http.py b/tests/utils_tests/test_http.py\nindex bc8bcfe977..2fd4e1e0e2 100644\n--- a/tests/utils_tests/test_http.py\n+++ b/tests/utils_tests/test_http.py\n@@ -1,5 +1,6 @@\n import unittest\n from datetime import datetime\n+from unittest import mock\n \n from django.test import SimpleTestCase, ignore_warnings\n from django.utils.datastructures import MultiValueDict\n@@ -328,6 +329,30 @@ class HttpDateProcessingTests(unittest.TestCase):\n         parsed = parse_http_date('Sun Nov  6 08:49:37 0037')\n         self.assertEqual(datetime.utcfromtimestamp(parsed), datetime(2037, 11, 6, 8, 49, 37))\n \n+    def test_parsing_rfc850_two_digit_year(self):\n+        \"\"\"\n+        RFC 7231 \u00a77.1.1.1: two-digit years that appear to be more than 50\n+        years in the future must be interpreted as the most recent past year\n+        with the same last two digits.\n+        \"\"\"\n+        # Subclass datetime.datetime to freeze utcnow() to 2019 for\n+        # deterministic results (datetime.datetime is a C type, so utcnow\n+        # cannot be patched directly).\n+        import datetime as dt_module\n+\n+        class MockDatetime(datetime):\n+            @classmethod\n+            def utcnow(cls):\n+                return cls(2019, 1, 1)\n+\n+        with mock.patch('django.utils.http.datetime.datetime', MockDatetime):\n+            # 70 \u2192 2070: 2070 - 2019 = 51 years in the future (> 50) \u2192 1970\n+            parsed = parse_http_date('Sunday, 06-Nov-70 08:49:37 GMT')\n+            self.assertEqual(datetime.utcfromtimestamp(parsed), datetime(1970, 11, 6, 8, 49, 37))\n+            # 69 \u2192 2069: 2069 - 2019 = 50 years in the future (not > 50) \u2192 2069\n+            parsed = parse_http_date('Sunday, 06-Nov-69 08:49:37 GMT')\n+            self.assertEqual(datetime.utcfromtimestamp(parsed), datetime(2069, 11, 6, 8, 49, 37))\n+\n \n class EscapeLeadingSlashesTests(unittest.TestCase):\n     def test(self):\n"}
